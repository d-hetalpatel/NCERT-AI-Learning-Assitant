{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit gdown pandas pypdf sentence-transformers scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1p-tiA-Gti9",
        "outputId": "63d6681e-0b5e-4033-e64d-a17dde8023ee"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.54.0-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (5.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-6.7.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=5.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.2.6)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.1)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.46)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (26.0)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.6)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (9.1.4)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown) (4.13.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown) (3.24.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from gdown) (4.67.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (5.0.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.10.0+cu128)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (4.26.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (0.28.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.5.4)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (0.24.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2026.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
            "Requirement already satisfied: cuda-bindings==12.9.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.9.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.6.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: cuda-pathfinder~=1.1 in /usr/local/lib/python3.12/dist-packages (from cuda-bindings==12.9.4->torch>=1.11.0->sentence-transformers) (1.3.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (2.8.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.30.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: typer>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (0.24.1)\n",
            "Requirement already satisfied: rich>=12.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (13.9.4)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (0.0.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (0.1.2)\n",
            "Downloading streamlit-1.54.0-py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-6.7.3-py3-none-any.whl (331 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m331.3/331.3 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m131.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 pypdf-6.7.3 streamlit-1.54.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original\n",
        "\n"
      ],
      "metadata": {
        "id": "DHGZ9oe_3DPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "##Original version\n",
        "import os\n",
        "from pathlib import Path\n",
        "import zipfile\n",
        "import re\n",
        "\n",
        "import streamlit as st\n",
        "import gdown\n",
        "import pandas as pd\n",
        "from pypdf import PdfReader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# -----------------------------\n",
        "# CONFIG\n",
        "# -----------------------------\n",
        "FILE_ID = \"1iKxF0Db1YjySxbFVCfQhOJ6CXYVwdxbC\"\n",
        "ZIP_PATH = \"ncert.zip\"\n",
        "EXTRACT_DIR = \"ncert_extracted\"\n",
        "\n",
        "SUBJECTS = [\"Polity\", \"Economics\", \"Sociology\", \"Psychology\", \"Business Studies\"]\n",
        "\n",
        "# -----------------------------\n",
        "# STREAMLIT SETUP\n",
        "# -----------------------------\n",
        "st.set_page_config(page_title=\"NCERT Learning Assistant\", layout=\"wide\")\n",
        "st.title(\"ðŸ“˜ NCERT Learning Assistant\")\n",
        "st.write(\"Select topics to get chapter, book, and video recommendations for Class 11â€“12 students\")\n",
        "\n",
        "# -----------------------------\n",
        "# DOWNLOAD & EXTRACT NCERT PDFs\n",
        "# -----------------------------\n",
        "def download_and_extract():\n",
        "    if not os.path.exists(ZIP_PATH):\n",
        "        gdown.download(f\"https://drive.google.com/uc?id={FILE_ID}\", ZIP_PATH, quiet=False)\n",
        "    os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
        "    with zipfile.ZipFile(ZIP_PATH, \"r\") as z:\n",
        "        z.extractall(EXTRACT_DIR)\n",
        "    # extract nested zips\n",
        "    for zfile in Path(EXTRACT_DIR).rglob(\"*.zip\"):\n",
        "        try:\n",
        "            target = zfile.parent / zfile.stem\n",
        "            target.mkdir(exist_ok=True)\n",
        "            with zipfile.ZipFile(zfile, \"r\") as inner:\n",
        "                inner.extractall(target)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "download_and_extract()\n",
        "\n",
        "# -----------------------------\n",
        "# PDF Utilities\n",
        "# -----------------------------\n",
        "def read_pdf(path):\n",
        "    try:\n",
        "        reader = PdfReader(path)\n",
        "        return \" \".join(p.extract_text() or \"\" for p in reader.pages)\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"(activity|let us|exercise|project|editor|reprint|copyright|isbn).*\", \" \", text, flags=re.I)\n",
        "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "def load_all_texts():\n",
        "    texts = []\n",
        "    paths = []\n",
        "    for pdf in Path(EXTRACT_DIR).rglob(\"*.pdf\"):\n",
        "        t = clean_text(read_pdf(str(pdf)))\n",
        "        if len(t.split()) > 50:\n",
        "            texts.append(t)\n",
        "            paths.append(pdf)\n",
        "    return texts, paths\n",
        "\n",
        "# -----------------------------\n",
        "# EXTRACT TOPICS\n",
        "# -----------------------------\n",
        "def extract_topics_from_text(text):\n",
        "    lines = text.split(\"\\n\")\n",
        "    topics = []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if 3 <= len(line.split()) <= 8 and line[0].isupper():\n",
        "            line = re.sub(r\"^(chapter\\s*\\d+:?|[\\d.]+\\s*)\", \"\", line, flags=re.I)\n",
        "            topics.append(line.strip())\n",
        "    return list(set(topics))\n",
        "\n",
        "@st.cache_data\n",
        "def get_all_topics():\n",
        "    texts, _ = load_all_texts()\n",
        "    all_topics = []\n",
        "    for text in texts:\n",
        "        all_topics.extend(extract_topics_from_text(text))\n",
        "    return sorted(list(set(all_topics)))\n",
        "\n",
        "all_topics = get_all_topics()\n",
        "\n",
        "# -----------------------------\n",
        "# Sentence Transformer\n",
        "# -----------------------------\n",
        "@st.cache_resource\n",
        "def load_embedder():\n",
        "    return SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "embedder = load_embedder()\n",
        "\n",
        "# -----------------------------\n",
        "# Books & Videos\n",
        "# -----------------------------\n",
        "books = {\n",
        "    \"Sociology\": {\n",
        "        \"Beginner\": [\"NCERT Sociology Class 11\", \"Understanding Society by Haralambos (Intro)\"],\n",
        "        \"Intermediate\": [\"Introduction to Sociology by Bottomore\", \"Sociology: Themes and Perspectives by Haralambos\"],\n",
        "        \"Advanced\": [\"Advanced Sociology by Giddens\", \"Sociology: Concepts and Theories by Macionis\"]\n",
        "    },\n",
        "    \"Polity\": {\n",
        "        \"Beginner\": [\"NCERT Political Science Class 11\", \"Indian Constitution Made Easy\"],\n",
        "        \"Intermediate\": [\"Indian Polity by M. Laxmikanth\", \"Governance in India by D.D. Basu\"],\n",
        "        \"Advanced\": [\"Introduction to the Constitution of India by D.D. Basu\", \"Indian Constitutional Law Advanced\"]\n",
        "    },\n",
        "    \"Psychology\": {\n",
        "        \"Beginner\": [\"NCERT Psychology Class 11\", \"Psychology Basics Intro\"],\n",
        "        \"Intermediate\": [\"Psychology: An Exploration by Saundra K. Ciccarelli\", \"Understanding Psychology by Feldman\"],\n",
        "        \"Advanced\": [\"Advanced Psychology by Baron & Misra\", \"Handbook of Psychology by Weiner\"]\n",
        "    },\n",
        "    \"Economics\": {\n",
        "        \"Beginner\": [\"NCERT Economics Class 11\", \"Principles of Economics Intro by Mankiw\"],\n",
        "        \"Intermediate\": [\"Principles of Economics by N. Gregory Mankiw\", \"Economic Theory by Samuelson\"],\n",
        "        \"Advanced\": [\"Advanced Economic Theory by H.L. Ahuja\", \"Micro and Macro Economics Advanced\"]\n",
        "    },\n",
        "    \"Business Studies\": {\n",
        "        \"Beginner\": [\"NCERT Business Studies Class 11\", \"Introduction to Management Basics\"],\n",
        "        \"Intermediate\": [\"Business Studies by Poonam Gandhi\", \"Business Management Concepts\"],\n",
        "        \"Advanced\": [\"Advanced Business Management by Robbins\", \"Strategic Management Advanced\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "videos = {\n",
        "    subject: {\n",
        "        level: [f\"https://www.youtube.com/watch?v={subject}_{level}_{i+1}\" for i in range(3)]\n",
        "        for level in [\"Beginner\", \"Intermediate\", \"Advanced\"]\n",
        "    } for subject in SUBJECTS\n",
        "}\n",
        "\n",
        "# -----------------------------\n",
        "# Streamlit UI\n",
        "# -----------------------------\n",
        "st.sidebar.header(\"ðŸŽ“ Select Options\")\n",
        "selected_subject = st.sidebar.selectbox(\"Select Subject\", SUBJECTS)\n",
        "selected_level = st.sidebar.selectbox(\"Select Level\", [\"Beginner\", \"Intermediate\", \"Advanced\"])\n",
        "selected_topics = st.sidebar.multiselect(\"Select Topic(s)\", all_topics)\n",
        "\n",
        "# -----------------------------\n",
        "# Recommend chapters using embeddings\n",
        "# -----------------------------\n",
        "@st.cache_data\n",
        "def get_chapter_recommendations(topics, top_n=5):\n",
        "    if not topics:\n",
        "        return pd.DataFrame({\"Message\":[\"Please select topics\"]})\n",
        "\n",
        "    texts, paths = load_all_texts()\n",
        "    text_embeddings = embedder.encode(texts, convert_to_tensor=True)\n",
        "    topic_embedding = embedder.encode([\" \".join(topics)], convert_to_tensor=True)\n",
        "\n",
        "    sims = cosine_similarity(topic_embedding.cpu().numpy(), text_embeddings.cpu().numpy()).flatten()\n",
        "\n",
        "    top_idx = sims.argsort()[-top_n:][::-1]\n",
        "    results = []\n",
        "    for i in top_idx:\n",
        "        results.append({\n",
        "            \"Chapter PDF\": str(paths[i].name),\n",
        "            \"Similarity Score\": sims[i]\n",
        "        })\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# -----------------------------\n",
        "# Recommend books & videos\n",
        "# -----------------------------\n",
        "def recommend_materials(subject, level, topics):\n",
        "    if not topics:\n",
        "        return pd.DataFrame({\"Message\":[\"Please select at least one topic\"]})\n",
        "    recommended_books = books.get(subject, {}).get(level, [])\n",
        "    recommended_videos = videos.get(subject, {}).get(level, [])\n",
        "    return pd.DataFrame({\n",
        "        \"Topics\": [\", \".join(topics)],\n",
        "        \"Recommended Books\": [\", \".join(recommended_books)],\n",
        "        \"Recommended Videos\": [\", \".join(recommended_videos)]\n",
        "    })\n",
        "\n",
        "# -----------------------------\n",
        "# Display recommendations\n",
        "# -----------------------------\n",
        "st.subheader(\"ðŸ“š Chapter Recommendations\")\n",
        "if st.button(\"Get Chapter Recommendations\"):\n",
        "    chapter_recs = get_chapter_recommendations(selected_topics)\n",
        "    st.dataframe(chapter_recs)\n",
        "\n",
        "st.subheader(\"ðŸ“– Book & Video Recommendations\")\n",
        "if st.button(\"Get Books & Videos\"):\n",
        "    materials = recommend_materials(selected_subject, selected_level, selected_topics)\n",
        "    st.dataframe(materials)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jEaMLtiy23Dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "changed---robust recursive extractor with gdown + requests fallback, ZIP validation, and @st.cache_resource so it only runs once per session."
      ],
      "metadata": {
        "id": "-rQf4BBT3G6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "%%writefile app.py\n",
        "import os\n",
        "from pathlib import Path\n",
        "import zipfile\n",
        "import re\n",
        "\n",
        "import streamlit as st\n",
        "import gdown\n",
        "import requests\n",
        "import pandas as pd\n",
        "from pypdf import PdfReader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# -----------------------------\n",
        "# CONFIG\n",
        "# -----------------------------\n",
        "FILE_ID = \"1iKxF0Db1YjySxbFVCfQhOJ6CXYVwdxbC\"\n",
        "ZIP_PATH = \"ncert.zip\"\n",
        "EXTRACT_DIR = \"ncert_extracted\"\n",
        "\n",
        "SUBJECTS = [\"Polity\", \"Economics\", \"Sociology\", \"Psychology\", \"Business Studies\"]\n",
        "\n",
        "# -----------------------------\n",
        "# STREAMLIT SETUP\n",
        "# -----------------------------\n",
        "st.set_page_config(page_title=\"NCERT Learning Assistant\", layout=\"wide\")\n",
        "st.title(\"ðŸ“˜ NCERT Learning Assistant\")\n",
        "st.write(\"Select topics to get chapter, book, and video recommendations for Class 11â€“12 students\")\n",
        "\n",
        "# -----------------------------\n",
        "# DOWNLOAD & EXTRACT NCERT PDFs\n",
        "# -----------------------------\n",
        "\n",
        "def extract_all_zips(folder: str) -> None:\n",
        "    \"\"\"Recursively extract all nested ZIPs until none remain unextracted.\"\"\"\n",
        "    found_new = True\n",
        "    while found_new:\n",
        "        found_new = False\n",
        "        for root, _, files in os.walk(folder):\n",
        "            for file in files:\n",
        "                if not file.lower().endswith(\".zip\"):\n",
        "                    continue\n",
        "                zip_path   = os.path.join(root, file)\n",
        "                extract_to = os.path.join(root, os.path.splitext(file)[0])\n",
        "                if os.path.exists(extract_to):\n",
        "                    continue                      # already extracted, skip\n",
        "                try:\n",
        "                    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "                        zf.extractall(extract_to)\n",
        "                    found_new = True             # new folder found, scan again\n",
        "                except zipfile.BadZipFile:\n",
        "                    st.warning(f\"âš ï¸ Skipping corrupt ZIP: {file}\")\n",
        "                except Exception as e:\n",
        "                    st.warning(f\"âš ï¸ Could not extract {file}: {e}\")\n",
        "\n",
        "\n",
        "@st.cache_resource\n",
        "def download_and_extract():\n",
        "    # â”€â”€ 1. DOWNLOAD â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    if not os.path.exists(ZIP_PATH):\n",
        "        with st.spinner(\"â¬‡ï¸ Downloading NCERT datasetâ€¦\"):\n",
        "            # Primary: gdown\n",
        "            downloaded = False\n",
        "            try:\n",
        "                gdown.download(id=FILE_ID, output=ZIP_PATH, quiet=False, fuzzy=True)\n",
        "                downloaded = os.path.exists(ZIP_PATH) and os.path.getsize(ZIP_PATH) > 10_000\n",
        "            except Exception as e:\n",
        "                st.warning(f\"gdown failed: {e}. Trying fallbackâ€¦\")\n",
        "\n",
        "            # Fallback: requests + Drive cookie-confirmation\n",
        "            if not downloaded:\n",
        "                try:\n",
        "                    session  = requests.Session()\n",
        "                    URL      = \"https://drive.google.com/uc?export=download\"\n",
        "                    response = session.get(URL, params={\"id\": FILE_ID}, stream=True)\n",
        "                    token    = next(\n",
        "                        (v for k, v in response.cookies.items()\n",
        "                         if k.startswith(\"download_warning\")), None\n",
        "                    )\n",
        "                    if token:\n",
        "                        response = session.get(URL, params={\"id\": FILE_ID, \"confirm\": token}, stream=True)\n",
        "                    else:\n",
        "                        response = session.get(\n",
        "                            f\"https://drive.google.com/uc?id={FILE_ID}&export=download&confirm=t\",\n",
        "                            stream=True,\n",
        "                        )\n",
        "                    with open(ZIP_PATH, \"wb\") as f:\n",
        "                        for chunk in response.iter_content(chunk_size=32_768):\n",
        "                            if chunk:\n",
        "                                f.write(chunk)\n",
        "                    if not os.path.exists(ZIP_PATH) or os.path.getsize(ZIP_PATH) < 10_000:\n",
        "                        raise ValueError(\"Downloaded file too small â€” likely an error page.\")\n",
        "                except Exception as e2:\n",
        "                    st.error(f\"âŒ Both download methods failed: {e2}\")\n",
        "                    st.stop()\n",
        "\n",
        "    # â”€â”€ 2. VALIDATE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    if not zipfile.is_zipfile(ZIP_PATH):\n",
        "        os.remove(ZIP_PATH)\n",
        "        st.error(\"âŒ Downloaded file is not a valid ZIP. Check FILE_ID and sharing settings.\")\n",
        "        st.stop()\n",
        "\n",
        "    # â”€â”€ 3. EXTRACT OUTER ZIP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    if not os.path.exists(EXTRACT_DIR):\n",
        "        with st.spinner(\"ðŸ“¦ Extracting outer ZIPâ€¦\"):\n",
        "            with zipfile.ZipFile(ZIP_PATH, \"r\") as zf:\n",
        "                zf.extractall(EXTRACT_DIR)\n",
        "\n",
        "    # â”€â”€ 4. EXTRACT ALL NESTED ZIPS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    with st.spinner(\"ðŸ“‚ Extracting subject/chapter ZIPsâ€¦\"):\n",
        "        extract_all_zips(EXTRACT_DIR)\n",
        "\n",
        "    # â”€â”€ 5. COLLECT PDFs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    pdf_files = [\n",
        "        os.path.join(root, f)\n",
        "        for root, _, files in os.walk(EXTRACT_DIR)\n",
        "        for f in files if f.lower().endswith(\".pdf\")\n",
        "    ]\n",
        "    st.info(f\"ðŸ“„ PDFs found: {len(pdf_files)}\")\n",
        "    return EXTRACT_DIR, pdf_files\n",
        "\n",
        "\n",
        "data_path, pdf_files = download_and_extract()\n",
        "\n",
        "# -----------------------------\n",
        "# PDF Utilities\n",
        "# -----------------------------\n",
        "def read_pdf(path):\n",
        "    try:\n",
        "        reader = PdfReader(path)\n",
        "        return \" \".join(p.extract_text() or \"\" for p in reader.pages)\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"(activity|let us|exercise|project|editor|reprint|copyright|isbn).*\", \" \", text, flags=re.I)\n",
        "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "def load_all_texts():\n",
        "    texts = []\n",
        "    paths = []\n",
        "    for pdf in Path(EXTRACT_DIR).rglob(\"*.pdf\"):\n",
        "        t = clean_text(read_pdf(str(pdf)))\n",
        "        if len(t.split()) > 50:\n",
        "            texts.append(t)\n",
        "            paths.append(pdf)\n",
        "    return texts, paths\n",
        "\n",
        "# -----------------------------\n",
        "# EXTRACT TOPICS\n",
        "# -----------------------------\n",
        "def extract_topics_from_text(text):\n",
        "    lines = text.split(\"\\n\")\n",
        "    topics = []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if 3 <= len(line.split()) <= 8 and line[0].isupper():\n",
        "            line = re.sub(r\"^(chapter\\s*\\d+:?|[\\d.]+\\s*)\", \"\", line, flags=re.I)\n",
        "            topics.append(line.strip())\n",
        "    return list(set(topics))\n",
        "\n",
        "@st.cache_data\n",
        "def get_all_topics():\n",
        "    texts, _ = load_all_texts()\n",
        "    all_topics = []\n",
        "    for text in texts:\n",
        "        all_topics.extend(extract_topics_from_text(text))\n",
        "    return sorted(list(set(all_topics)))\n",
        "\n",
        "all_topics = get_all_topics()\n",
        "\n",
        "# -----------------------------\n",
        "# Sentence Transformer\n",
        "# -----------------------------\n",
        "@st.cache_resource\n",
        "def load_embedder():\n",
        "    return SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "embedder = load_embedder()\n",
        "\n",
        "# -----------------------------\n",
        "# Books & Videos\n",
        "# -----------------------------\n",
        "books = {\n",
        "    \"Sociology\": {\n",
        "        \"Beginner\": [\"NCERT Sociology Class 11\", \"Understanding Society by Haralambos (Intro)\"],\n",
        "        \"Intermediate\": [\"Introduction to Sociology by Bottomore\", \"Sociology: Themes and Perspectives by Haralambos\"],\n",
        "        \"Advanced\": [\"Advanced Sociology by Giddens\", \"Sociology: Concepts and Theories by Macionis\"]\n",
        "    },\n",
        "    \"Polity\": {\n",
        "        \"Beginner\": [\"NCERT Political Science Class 11\", \"Indian Constitution Made Easy\"],\n",
        "        \"Intermediate\": [\"Indian Polity by M. Laxmikanth\", \"Governance in India by D.D. Basu\"],\n",
        "        \"Advanced\": [\"Introduction to the Constitution of India by D.D. Basu\", \"Indian Constitutional Law Advanced\"]\n",
        "    },\n",
        "    \"Psychology\": {\n",
        "        \"Beginner\": [\"NCERT Psychology Class 11\", \"Psychology Basics Intro\"],\n",
        "        \"Intermediate\": [\"Psychology: An Exploration by Saundra K. Ciccarelli\", \"Understanding Psychology by Feldman\"],\n",
        "        \"Advanced\": [\"Advanced Psychology by Baron & Misra\", \"Handbook of Psychology by Weiner\"]\n",
        "    },\n",
        "    \"Economics\": {\n",
        "        \"Beginner\": [\"NCERT Economics Class 11\", \"Principles of Economics Intro by Mankiw\"],\n",
        "        \"Intermediate\": [\"Principles of Economics by N. Gregory Mankiw\", \"Economic Theory by Samuelson\"],\n",
        "        \"Advanced\": [\"Advanced Economic Theory by H.L. Ahuja\", \"Micro and Macro Economics Advanced\"]\n",
        "    },\n",
        "    \"Business Studies\": {\n",
        "        \"Beginner\": [\"NCERT Business Studies Class 11\", \"Introduction to Management Basics\"],\n",
        "        \"Intermediate\": [\"Business Studies by Poonam Gandhi\", \"Business Management Concepts\"],\n",
        "        \"Advanced\": [\"Advanced Business Management by Robbins\", \"Strategic Management Advanced\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "videos = {\n",
        "    subject: {\n",
        "        level: [f\"https://www.youtube.com/watch?v={subject}_{level}_{i+1}\" for i in range(3)]\n",
        "        for level in [\"Beginner\", \"Intermediate\", \"Advanced\"]\n",
        "    } for subject in SUBJECTS\n",
        "}\n",
        "\n",
        "# -----------------------------\n",
        "# Streamlit UI\n",
        "# -----------------------------\n",
        "st.sidebar.header(\"ðŸŽ“ Select Options\")\n",
        "selected_subject = st.sidebar.selectbox(\"Select Subject\", SUBJECTS)\n",
        "selected_level = st.sidebar.selectbox(\"Select Level\", [\"Beginner\", \"Intermediate\", \"Advanced\"])\n",
        "selected_topics = st.sidebar.multiselect(\"Select Topic(s)\", all_topics)\n",
        "\n",
        "# -----------------------------\n",
        "# Recommend chapters using embeddings\n",
        "# -----------------------------\n",
        "@st.cache_data\n",
        "def get_chapter_recommendations(topics, top_n=5):\n",
        "    if not topics:\n",
        "        return pd.DataFrame({\"Message\":[\"Please select topics\"]})\n",
        "\n",
        "    texts, paths = load_all_texts()\n",
        "    text_embeddings = embedder.encode(texts, convert_to_tensor=True)\n",
        "    topic_embedding = embedder.encode([\" \".join(topics)], convert_to_tensor=True)\n",
        "\n",
        "    sims = cosine_similarity(topic_embedding.cpu().numpy(), text_embeddings.cpu().numpy()).flatten()\n",
        "\n",
        "    top_idx = sims.argsort()[-top_n:][::-1]\n",
        "    results = []\n",
        "    for i in top_idx:\n",
        "        results.append({\n",
        "            \"Chapter PDF\": str(paths[i].name),\n",
        "            \"Similarity Score\": sims[i]\n",
        "        })\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# -----------------------------\n",
        "# Recommend books & videos\n",
        "# -----------------------------\n",
        "def recommend_materials(subject, level, topics):\n",
        "    if not topics:\n",
        "        return pd.DataFrame({\"Message\":[\"Please select at least one topic\"]})\n",
        "    recommended_books = books.get(subject, {}).get(level, [])\n",
        "    recommended_videos = videos.get(subject, {}).get(level, [])\n",
        "    return pd.DataFrame({\n",
        "        \"Topics\": [\", \".join(topics)],\n",
        "        \"Recommended Books\": [\", \".join(recommended_books)],\n",
        "        \"Recommended Videos\": [\", \".join(recommended_videos)]\n",
        "    })\n",
        "\n",
        "# -----------------------------\n",
        "# Display recommendations\n",
        "# -----------------------------\n",
        "st.subheader(\"ðŸ“š Chapter Recommendations\")\n",
        "if st.button(\"Get Chapter Recommendations\"):\n",
        "    chapter_recs = get_chapter_recommendations(selected_topics)\n",
        "    st.dataframe(chapter_recs)\n",
        "\n",
        "st.subheader(\"ðŸ“– Book & Video Recommendations\")\n",
        "if st.button(\"Get Books & Videos\"):\n",
        "    materials = recommend_materials(selected_subject, selected_level, selected_topics)\n",
        "    st.dataframe(materials)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "ZdVFUets3L14",
        "outputId": "93a4f9ea-979a-4d38-d2dc-ef3ae68dec81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Caching â€” @st.cache_data on PDF reading, topic extraction, and recommendations; @st.cache_resource on the embedding model. Nothing re-runs unnecessarily."
      ],
      "metadata": {
        "id": "3fzPhOfi6-Vg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "%%writefile app.py\n",
        "import os\n",
        "from pathlib import Path\n",
        "import zipfile\n",
        "import re\n",
        "\n",
        "import streamlit as st\n",
        "import gdown\n",
        "import requests\n",
        "import pandas as pd\n",
        "from pypdf import PdfReader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# -----------------------------\n",
        "# CONFIG\n",
        "# -----------------------------\n",
        "FILE_ID = \"1iKxF0Db1YjySxbFVCfQhOJ6CXYVwdxbC\"\n",
        "ZIP_PATH = \"ncert.zip\"\n",
        "EXTRACT_DIR = \"ncert_extracted\"\n",
        "\n",
        "SUBJECTS = [\"Polity\", \"Economics\", \"Sociology\", \"Psychology\", \"Business Studies\"]\n",
        "\n",
        "# -----------------------------\n",
        "# STREAMLIT SETUP\n",
        "# -----------------------------\n",
        "st.set_page_config(page_title=\"NCERT Learning Assistant\", layout=\"wide\")\n",
        "st.title(\"ðŸ“˜ NCERT Learning Assistant\")\n",
        "st.write(\"Select topics to get chapter, book, and video recommendations for Class 11â€“12 students\")\n",
        "\n",
        "# -----------------------------\n",
        "# DOWNLOAD & EXTRACT NCERT PDFs\n",
        "# -----------------------------\n",
        "\n",
        "def extract_all_zips(folder: str) -> None:\n",
        "    \"\"\"Recursively extract all nested ZIPs until none remain unextracted.\"\"\"\n",
        "    found_new = True\n",
        "    while found_new:\n",
        "        found_new = False\n",
        "        for root, _, files in os.walk(folder):\n",
        "            for file in files:\n",
        "                if not file.lower().endswith(\".zip\"):\n",
        "                    continue\n",
        "                zip_path   = os.path.join(root, file)\n",
        "                extract_to = os.path.join(root, os.path.splitext(file)[0])\n",
        "                if os.path.exists(extract_to):\n",
        "                    continue                      # already extracted, skip\n",
        "                try:\n",
        "                    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "                        zf.extractall(extract_to)\n",
        "                    found_new = True             # new folder found, scan again\n",
        "                except zipfile.BadZipFile:\n",
        "                    st.warning(f\"âš ï¸ Skipping corrupt ZIP: {file}\")\n",
        "                except Exception as e:\n",
        "                    st.warning(f\"âš ï¸ Could not extract {file}: {e}\")\n",
        "\n",
        "\n",
        "@st.cache_data\n",
        "def download_and_extract():\n",
        "    # â”€â”€ 1. DOWNLOAD â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    if not os.path.exists(ZIP_PATH):\n",
        "        with st.spinner(\"â¬‡ï¸ Downloading NCERT datasetâ€¦\"):\n",
        "            # Primary: gdown\n",
        "            downloaded = False\n",
        "            try:\n",
        "                gdown.download(id=FILE_ID, output=ZIP_PATH, quiet=False, fuzzy=True)\n",
        "                downloaded = os.path.exists(ZIP_PATH) and os.path.getsize(ZIP_PATH) > 10_000\n",
        "            except Exception as e:\n",
        "                st.warning(f\"gdown failed: {e}. Trying fallbackâ€¦\")\n",
        "\n",
        "            # Fallback: requests + Drive cookie-confirmation\n",
        "            if not downloaded:\n",
        "                try:\n",
        "                    session  = requests.Session()\n",
        "                    URL      = \"https://drive.google.com/uc?export=download\"\n",
        "                    response = session.get(URL, params={\"id\": FILE_ID}, stream=True)\n",
        "                    token    = next(\n",
        "                        (v for k, v in response.cookies.items()\n",
        "                         if k.startswith(\"download_warning\")), None\n",
        "                    )\n",
        "                    if token:\n",
        "                        response = session.get(URL, params={\"id\": FILE_ID, \"confirm\": token}, stream=True)\n",
        "                    else:\n",
        "                        response = session.get(\n",
        "                            f\"https://drive.google.com/uc?id={FILE_ID}&export=download&confirm=t\",\n",
        "                            stream=True,\n",
        "                        )\n",
        "                    with open(ZIP_PATH, \"wb\") as f:\n",
        "                        for chunk in response.iter_content(chunk_size=32_768):\n",
        "                            if chunk:\n",
        "                                f.write(chunk)\n",
        "                    if not os.path.exists(ZIP_PATH) or os.path.getsize(ZIP_PATH) < 10_000:\n",
        "                        raise ValueError(\"Downloaded file too small â€” likely an error page.\")\n",
        "                except Exception as e2:\n",
        "                    st.error(f\"âŒ Both download methods failed: {e2}\")\n",
        "                    st.stop()\n",
        "\n",
        "    # â”€â”€ 2. VALIDATE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    if not zipfile.is_zipfile(ZIP_PATH):\n",
        "        os.remove(ZIP_PATH)\n",
        "        st.error(\"âŒ Downloaded file is not a valid ZIP. Check FILE_ID and sharing settings.\")\n",
        "        st.stop()\n",
        "\n",
        "    # â”€â”€ 3. EXTRACT OUTER ZIP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    if not os.path.exists(EXTRACT_DIR):\n",
        "        with st.spinner(\"ðŸ“¦ Extracting outer ZIPâ€¦\"):\n",
        "            with zipfile.ZipFile(ZIP_PATH, \"r\") as zf:\n",
        "                zf.extractall(EXTRACT_DIR)\n",
        "\n",
        "    # â”€â”€ 4. EXTRACT ALL NESTED ZIPS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    with st.spinner(\"ðŸ“‚ Extracting subject/chapter ZIPsâ€¦\"):\n",
        "        extract_all_zips(EXTRACT_DIR)\n",
        "\n",
        "    # â”€â”€ 5. COLLECT PDFs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    pdf_files = [\n",
        "        os.path.join(root, f)\n",
        "        for root, _, files in os.walk(EXTRACT_DIR)\n",
        "        for f in files if f.lower().endswith(\".pdf\")\n",
        "    ]\n",
        "    st.info(f\"ðŸ“„ PDFs found: {len(pdf_files)}\")\n",
        "    return EXTRACT_DIR, pdf_files\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# PDF Utilities\n",
        "# -----------------------------\n",
        "def read_pdf(path):\n",
        "    try:\n",
        "        reader = PdfReader(path)\n",
        "        return \" \".join(p.extract_text() or \"\" for p in reader.pages)\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"(activity|let us|exercise|project|editor|reprint|copyright|isbn).*\", \" \", text, flags=re.I)\n",
        "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "@st.cache_data(show_spinner=\"ðŸ“– Reading PDFsâ€¦\")\n",
        "def load_all_texts():\n",
        "    texts = []\n",
        "    paths = []\n",
        "    for pdf in Path(EXTRACT_DIR).rglob(\"*.pdf\"):\n",
        "        t = clean_text(read_pdf(str(pdf)))\n",
        "        if len(t.split()) > 50:\n",
        "            texts.append(t)\n",
        "            paths.append(str(pdf.name))   # str, not Path â€” serializable by st.cache_data\n",
        "    return texts, paths\n",
        "\n",
        "# -----------------------------\n",
        "# EXTRACT TOPICS\n",
        "# -----------------------------\n",
        "def extract_topics_from_text(text):\n",
        "    # PDF text has no newlines â€” split on sentence boundaries instead\n",
        "    segments = re.split(r\"[.\\n]\", text)\n",
        "    topics = []\n",
        "    for seg in segments:\n",
        "        seg = seg.strip()\n",
        "        if not seg or not seg[0].isupper():          # guard empty / lowercase\n",
        "            continue\n",
        "        words = seg.split()\n",
        "        if 3 <= len(words) <= 8:\n",
        "            cleaned = re.sub(r\"^(chapter\\s*\\d+:?|[\\d.]+\\s*)\", \"\", seg, flags=re.I).strip()\n",
        "            if cleaned:\n",
        "                topics.append(cleaned)\n",
        "    return list(set(topics))\n",
        "\n",
        "@st.cache_data(show_spinner=\"ðŸ” Extracting topicsâ€¦\")\n",
        "def get_all_topics():\n",
        "    texts, _ = load_all_texts()\n",
        "    all_topics = []\n",
        "    for text in texts:\n",
        "        all_topics.extend(extract_topics_from_text(text))\n",
        "    return sorted(list(set(all_topics)))\n",
        "\n",
        "# 1. Ensure data is there\n",
        "data_path, pdf_files = download_and_extract()\n",
        "\n",
        "# 2. Only get topics if we have files\n",
        "if pdf_files:\n",
        "    all_topics = get_all_topics()\n",
        "else:\n",
        "    all_topics = []\n",
        "\n",
        "# -----------------------------\n",
        "# Sentence Transformer\n",
        "# -----------------------------\n",
        "@st.cache_resource\n",
        "def load_embedder():\n",
        "    return SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "embedder = load_embedder()\n",
        "\n",
        "# -----------------------------\n",
        "# Books & Videos\n",
        "# -----------------------------\n",
        "books = {\n",
        "    \"Sociology\": {\n",
        "        \"Beginner\": [\"NCERT Sociology Class 11\", \"Understanding Society by Haralambos (Intro)\"],\n",
        "        \"Intermediate\": [\"Introduction to Sociology by Bottomore\", \"Sociology: Themes and Perspectives by Haralambos\"],\n",
        "        \"Advanced\": [\"Advanced Sociology by Giddens\", \"Sociology: Concepts and Theories by Macionis\"]\n",
        "    },\n",
        "    \"Polity\": {\n",
        "        \"Beginner\": [\"NCERT Political Science Class 11\", \"Indian Constitution Made Easy\"],\n",
        "        \"Intermediate\": [\"Indian Polity by M. Laxmikanth\", \"Governance in India by D.D. Basu\"],\n",
        "        \"Advanced\": [\"Introduction to the Constitution of India by D.D. Basu\", \"Indian Constitutional Law Advanced\"]\n",
        "    },\n",
        "    \"Psychology\": {\n",
        "        \"Beginner\": [\"NCERT Psychology Class 11\", \"Psychology Basics Intro\"],\n",
        "        \"Intermediate\": [\"Psychology: An Exploration by Saundra K. Ciccarelli\", \"Understanding Psychology by Feldman\"],\n",
        "        \"Advanced\": [\"Advanced Psychology by Baron & Misra\", \"Handbook of Psychology by Weiner\"]\n",
        "    },\n",
        "    \"Economics\": {\n",
        "        \"Beginner\": [\"NCERT Economics Class 11\", \"Principles of Economics Intro by Mankiw\"],\n",
        "        \"Intermediate\": [\"Principles of Economics by N. Gregory Mankiw\", \"Economic Theory by Samuelson\"],\n",
        "        \"Advanced\": [\"Advanced Economic Theory by H.L. Ahuja\", \"Micro and Macro Economics Advanced\"]\n",
        "    },\n",
        "    \"Business Studies\": {\n",
        "        \"Beginner\": [\"NCERT Business Studies Class 11\", \"Introduction to Management Basics\"],\n",
        "        \"Intermediate\": [\"Business Studies by Poonam Gandhi\", \"Business Management Concepts\"],\n",
        "        \"Advanced\": [\"Advanced Business Management by Robbins\", \"Strategic Management Advanced\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "videos = {\n",
        "    subject: {\n",
        "        level: [f\"https://www.youtube.com/watch?v={subject}_{level}_{i+1}\" for i in range(3)]\n",
        "        for level in [\"Beginner\", \"Intermediate\", \"Advanced\"]\n",
        "    } for subject in SUBJECTS\n",
        "}\n",
        "\n",
        "# -----------------------------\n",
        "# Streamlit UI\n",
        "# -----------------------------\n",
        "st.sidebar.header(\"ðŸŽ“ Select Options\")\n",
        "selected_subject = st.sidebar.selectbox(\"Select Subject\", SUBJECTS)\n",
        "selected_level = st.sidebar.selectbox(\"Select Level\", [\"Beginner\", \"Intermediate\", \"Advanced\"])\n",
        "if not all_topics:\n",
        "    st.sidebar.warning(\"No topics found yet. Please wait for extraction or check your PDF folder.\")\n",
        "    selected_topics = st.sidebar.multiselect(\"Select Topic(s)\", [\"Loading...\"], disabled=True)\n",
        "else:\n",
        "    selected_topics = st.sidebar.multiselect(\"Select Topic(s)\", all_topics)\n",
        "\n",
        "# -----------------------------\n",
        "# Recommend chapters using embeddings\n",
        "# -----------------------------\n",
        "@st.cache_data(show_spinner=\"ðŸ”Ž Finding relevant chaptersâ€¦\")\n",
        "def get_chapter_recommendations(topics, top_n=5):  # topics must be a tuple for cache hashing\n",
        "    if not topics:\n",
        "        return pd.DataFrame({\"Message\":[\"Please select topics\"]})\n",
        "\n",
        "    texts, paths = load_all_texts()\n",
        "    text_embeddings = embedder.encode(texts, convert_to_numpy=True, show_progress_bar=False)\n",
        "    topic_embedding = embedder.encode([\" \".join(topics)], convert_to_numpy=True)\n",
        "\n",
        "    sims = cosine_similarity(topic_embedding, text_embeddings).flatten()\n",
        "\n",
        "    top_idx = sims.argsort()[-top_n:][::-1]\n",
        "    results = []\n",
        "    for i in top_idx:\n",
        "        results.append({\n",
        "            \"Chapter PDF\": paths[i],\n",
        "            \"Similarity Score\": round(float(sims[i]), 4)\n",
        "        })\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# -----------------------------\n",
        "# Recommend books & videos\n",
        "# -----------------------------\n",
        "@st.cache_data\n",
        "def recommend_materials(subject, level, topics):  # topics must be a tuple for cache hashing\n",
        "    if not topics:\n",
        "        return pd.DataFrame({\"Message\":[\"Please select at least one topic\"]})\n",
        "    recommended_books = books.get(subject, {}).get(level, [])\n",
        "    recommended_videos = videos.get(subject, {}).get(level, [])\n",
        "    return pd.DataFrame({\n",
        "        \"Topics\": [\", \".join(topics)],\n",
        "        \"Recommended Books\": [\", \".join(recommended_books)],\n",
        "        \"Recommended Videos\": [\", \".join(recommended_videos)]\n",
        "    })\n",
        "\n",
        "# -----------------------------\n",
        "# Display recommendations\n",
        "# -----------------------------\n",
        "st.subheader(\"ðŸ“š Chapter Recommendations\")\n",
        "if st.button(\"Get Chapter Recommendations\"):\n",
        "    chapter_recs = get_chapter_recommendations(tuple(selected_topics))\n",
        "    st.dataframe(chapter_recs)\n",
        "\n",
        "st.subheader(\"ðŸ“– Book & Video Recommendations\")\n",
        "if st.button(\"Get Books & Videos\"):\n",
        "    materials = recommend_materials(selected_subject, selected_level, tuple(selected_topics))\n",
        "    st.dataframe(materials)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwtT1gnI5msT",
        "outputId": "b39c3448-b026-430c-de3b-8a161f2d2d22",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Topics â€” noisy/empty lines filtered out"
      ],
      "metadata": {
        "id": "XB9coG57FNXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "%%writefile app.py\n",
        "import os\n",
        "from pathlib import Path\n",
        "import zipfile\n",
        "import re\n",
        "\n",
        "import streamlit as st\n",
        "import gdown\n",
        "import requests\n",
        "import pandas as pd\n",
        "from pypdf import PdfReader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# -----------------------------\n",
        "# CONFIG\n",
        "# -----------------------------\n",
        "FILE_ID = \"1iKxF0Db1YjySxbFVCfQhOJ6CXYVwdxbC\"\n",
        "ZIP_PATH = \"ncert.zip\"\n",
        "EXTRACT_DIR = \"ncert_extracted\"\n",
        "\n",
        "SUBJECTS = [\"Polity\", \"Economics\", \"Sociology\", \"Psychology\", \"Business Studies\"]\n",
        "\n",
        "# -----------------------------\n",
        "# STREAMLIT SETUP\n",
        "# -----------------------------\n",
        "st.set_page_config(page_title=\"NCERT Learning Assistant\", layout=\"wide\")\n",
        "st.title(\"ðŸ“˜ NCERT Learning Assistant\")\n",
        "st.write(\"Select topics to get chapter, book, and video recommendations for Class 11â€“12 students\")\n",
        "\n",
        "# -----------------------------\n",
        "# DOWNLOAD & EXTRACT NCERT PDFs\n",
        "# -----------------------------\n",
        "\n",
        "def extract_all_zips(folder: str) -> None:\n",
        "    \"\"\"Recursively extract all nested ZIPs until none remain unextracted.\"\"\"\n",
        "    found_new = True\n",
        "    while found_new:\n",
        "        found_new = False\n",
        "        for root, _, files in os.walk(folder):\n",
        "            for file in files:\n",
        "                if not file.lower().endswith(\".zip\"):\n",
        "                    continue\n",
        "                zip_path   = os.path.join(root, file)\n",
        "                extract_to = os.path.join(root, os.path.splitext(file)[0])\n",
        "                if os.path.exists(extract_to):\n",
        "                    continue                      # already extracted, skip\n",
        "                try:\n",
        "                    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "                        zf.extractall(extract_to)\n",
        "                    found_new = True             # new folder found, scan again\n",
        "                except zipfile.BadZipFile:\n",
        "                    st.warning(f\"âš ï¸ Skipping corrupt ZIP: {file}\")\n",
        "                except Exception as e:\n",
        "                    st.warning(f\"âš ï¸ Could not extract {file}: {e}\")\n",
        "\n",
        "\n",
        "@st.cache_data\n",
        "def download_and_extract():\n",
        "    # â”€â”€ 1. DOWNLOAD â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    if not os.path.exists(ZIP_PATH):\n",
        "        with st.spinner(\"â¬‡ï¸ Downloading NCERT datasetâ€¦\"):\n",
        "            # Primary: gdown\n",
        "            downloaded = False\n",
        "            try:\n",
        "                gdown.download(id=FILE_ID, output=ZIP_PATH, quiet=False, fuzzy=True)\n",
        "                downloaded = os.path.exists(ZIP_PATH) and os.path.getsize(ZIP_PATH) > 10_000\n",
        "            except Exception as e:\n",
        "                st.warning(f\"gdown failed: {e}. Trying fallbackâ€¦\")\n",
        "\n",
        "            # Fallback: requests + Drive cookie-confirmation\n",
        "            if not downloaded:\n",
        "                try:\n",
        "                    session  = requests.Session()\n",
        "                    URL      = \"https://drive.google.com/uc?export=download\"\n",
        "                    response = session.get(URL, params={\"id\": FILE_ID}, stream=True)\n",
        "                    token    = next(\n",
        "                        (v for k, v in response.cookies.items()\n",
        "                         if k.startswith(\"download_warning\")), None\n",
        "                    )\n",
        "                    if token:\n",
        "                        response = session.get(URL, params={\"id\": FILE_ID, \"confirm\": token}, stream=True)\n",
        "                    else:\n",
        "                        response = session.get(\n",
        "                            f\"https://drive.google.com/uc?id={FILE_ID}&export=download&confirm=t\",\n",
        "                            stream=True,\n",
        "                        )\n",
        "                    with open(ZIP_PATH, \"wb\") as f:\n",
        "                        for chunk in response.iter_content(chunk_size=32_768):\n",
        "                            if chunk:\n",
        "                                f.write(chunk)\n",
        "                    if not os.path.exists(ZIP_PATH) or os.path.getsize(ZIP_PATH) < 10_000:\n",
        "                        raise ValueError(\"Downloaded file too small â€” likely an error page.\")\n",
        "                except Exception as e2:\n",
        "                    st.error(f\"âŒ Both download methods failed: {e2}\")\n",
        "                    st.stop()\n",
        "\n",
        "    # â”€â”€ 2. VALIDATE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    if not zipfile.is_zipfile(ZIP_PATH):\n",
        "        os.remove(ZIP_PATH)\n",
        "        st.error(\"âŒ Downloaded file is not a valid ZIP. Check FILE_ID and sharing settings.\")\n",
        "        st.stop()\n",
        "\n",
        "    # â”€â”€ 3. EXTRACT OUTER ZIP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    if not os.path.exists(EXTRACT_DIR):\n",
        "        with st.spinner(\"ðŸ“¦ Extracting outer ZIPâ€¦\"):\n",
        "            with zipfile.ZipFile(ZIP_PATH, \"r\") as zf:\n",
        "                zf.extractall(EXTRACT_DIR)\n",
        "\n",
        "    # â”€â”€ 4. EXTRACT ALL NESTED ZIPS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    with st.spinner(\"ðŸ“‚ Extracting subject/chapter ZIPsâ€¦\"):\n",
        "        extract_all_zips(EXTRACT_DIR)\n",
        "\n",
        "    # â”€â”€ 5. COLLECT PDFs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    pdf_files = [\n",
        "        os.path.join(root, f)\n",
        "        for root, _, files in os.walk(EXTRACT_DIR)\n",
        "        for f in files if f.lower().endswith(\".pdf\")\n",
        "    ]\n",
        "    st.info(f\"ðŸ“„ PDFs found: {len(pdf_files)}\")\n",
        "    return EXTRACT_DIR, pdf_files\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# PDF Utilities\n",
        "# -----------------------------\n",
        "def read_pdf(path):\n",
        "    try:\n",
        "        reader = PdfReader(path)\n",
        "        return \" \".join(p.extract_text() or \"\" for p in reader.pages)\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"(activity|let us|exercise|project|editor|reprint|copyright|isbn).*\", \" \", text, flags=re.I)\n",
        "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "@st.cache_data(show_spinner=\"ðŸ“– Reading PDFsâ€¦\")\n",
        "def load_all_texts():\n",
        "    texts = []\n",
        "    paths = []\n",
        "    for pdf in Path(EXTRACT_DIR).rglob(\"*.pdf\"):\n",
        "        t = clean_text(read_pdf(str(pdf)))\n",
        "        if len(t.split()) > 50:\n",
        "            texts.append(t)\n",
        "            paths.append(str(pdf.name))   # str, not Path â€” serializable by st.cache_data\n",
        "    return texts, paths\n",
        "\n",
        "# -----------------------------\n",
        "# EXTRACT TOPICS\n",
        "# -----------------------------\n",
        "def extract_topics_from_text(text):\n",
        "    # PDF text has no newlines â€” split on sentence boundaries instead\n",
        "    segments = re.split(r\"[.\\n]\", text)\n",
        "    topics = []\n",
        "    for seg in segments:\n",
        "        seg = seg.strip()\n",
        "        if not seg or not seg[0].isupper():          # guard empty / lowercase\n",
        "            continue\n",
        "        words = seg.split()\n",
        "        if 3 <= len(words) <= 8:\n",
        "            cleaned = re.sub(r\"^(chapter\\s*\\d+:?|[\\d.]+\\s*)\", \"\", seg, flags=re.I).strip()\n",
        "            if cleaned:\n",
        "                topics.append(cleaned)\n",
        "    return list(set(topics))\n",
        "\n",
        "@st.cache_data(show_spinner=\"ðŸ” Extracting topicsâ€¦\")\n",
        "def get_all_topics():\n",
        "    texts, _ = load_all_texts()\n",
        "    all_topics = []\n",
        "    for text in texts:\n",
        "        all_topics.extend(extract_topics_from_text(text))\n",
        "    return sorted(list(set(all_topics)))\n",
        "\n",
        "# 1. Ensure data is there\n",
        "data_path, pdf_files = download_and_extract()\n",
        "\n",
        "# 2. Only get topics if we have files\n",
        "if pdf_files:\n",
        "    all_topics = get_all_topics()\n",
        "else:\n",
        "    all_topics = []\n",
        "\n",
        "# -----------------------------\n",
        "# Sentence Transformer\n",
        "# -----------------------------\n",
        "@st.cache_resource\n",
        "def load_embedder():\n",
        "    return SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "embedder = load_embedder()\n",
        "\n",
        "# -----------------------------\n",
        "# Books & Videos\n",
        "# -----------------------------\n",
        "books = {\n",
        "    \"Sociology\": {\n",
        "        \"Beginner\": [\"NCERT Sociology Class 11\", \"Understanding Society by Haralambos (Intro)\"],\n",
        "        \"Intermediate\": [\"Introduction to Sociology by Bottomore\", \"Sociology: Themes and Perspectives by Haralambos\"],\n",
        "        \"Advanced\": [\"Advanced Sociology by Giddens\", \"Sociology: Concepts and Theories by Macionis\"]\n",
        "    },\n",
        "    \"Polity\": {\n",
        "        \"Beginner\": [\"NCERT Political Science Class 11\", \"Indian Constitution Made Easy\"],\n",
        "        \"Intermediate\": [\"Indian Polity by M. Laxmikanth\", \"Governance in India by D.D. Basu\"],\n",
        "        \"Advanced\": [\"Introduction to the Constitution of India by D.D. Basu\", \"Indian Constitutional Law Advanced\"]\n",
        "    },\n",
        "    \"Psychology\": {\n",
        "        \"Beginner\": [\"NCERT Psychology Class 11\", \"Psychology Basics Intro\"],\n",
        "        \"Intermediate\": [\"Psychology: An Exploration by Saundra K. Ciccarelli\", \"Understanding Psychology by Feldman\"],\n",
        "        \"Advanced\": [\"Advanced Psychology by Baron & Misra\", \"Handbook of Psychology by Weiner\"]\n",
        "    },\n",
        "    \"Economics\": {\n",
        "        \"Beginner\": [\"NCERT Economics Class 11\", \"Principles of Economics Intro by Mankiw\"],\n",
        "        \"Intermediate\": [\"Principles of Economics by N. Gregory Mankiw\", \"Economic Theory by Samuelson\"],\n",
        "        \"Advanced\": [\"Advanced Economic Theory by H.L. Ahuja\", \"Micro and Macro Economics Advanced\"]\n",
        "    },\n",
        "    \"Business Studies\": {\n",
        "        \"Beginner\": [\"NCERT Business Studies Class 11\", \"Introduction to Management Basics\"],\n",
        "        \"Intermediate\": [\"Business Studies by Poonam Gandhi\", \"Business Management Concepts\"],\n",
        "        \"Advanced\": [\"Advanced Business Management by Robbins\", \"Strategic Management Advanced\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "videos = {\n",
        "    subject: {\n",
        "        level: [f\"https://www.youtube.com/watch?v={subject}_{level}_{i+1}\" for i in range(3)]\n",
        "        for level in [\"Beginner\", \"Intermediate\", \"Advanced\"]\n",
        "    } for subject in SUBJECTS\n",
        "}\n",
        "\n",
        "# -----------------------------\n",
        "# Streamlit UI\n",
        "# -----------------------------\n",
        "st.sidebar.header(\"ðŸŽ“ Select Options\")\n",
        "selected_subject = st.sidebar.selectbox(\"Select Subject\", SUBJECTS)\n",
        "selected_level = st.sidebar.selectbox(\"Select Level\", [\"Beginner\", \"Intermediate\", \"Advanced\"])\n",
        "if not all_topics:\n",
        "    st.sidebar.warning(\"No topics found yet. Please wait for extraction or check your PDF folder.\")\n",
        "    selected_topics = st.sidebar.multiselect(\"Select Topic(s)\", [\"Loading...\"], disabled=True)\n",
        "else:\n",
        "    selected_topics = st.sidebar.multiselect(\"Select Topic(s)\", all_topics)\n",
        "\n",
        "# -----------------------------\n",
        "# Recommend chapters using embeddings\n",
        "# -----------------------------\n",
        "@st.cache_data(show_spinner=\"ðŸ”Ž Finding relevant chaptersâ€¦\")\n",
        "def get_chapter_recommendations(topics, top_n=5):  # topics must be a tuple for cache hashing\n",
        "    if not topics:\n",
        "        return pd.DataFrame({\"Message\":[\"Please select topics\"]})\n",
        "\n",
        "    texts, paths = load_all_texts()\n",
        "    text_embeddings = embedder.encode(texts, convert_to_numpy=True, show_progress_bar=False)\n",
        "    topic_embedding = embedder.encode([\" \".join(topics)], convert_to_numpy=True)\n",
        "\n",
        "    sims = cosine_similarity(topic_embedding, text_embeddings).flatten()\n",
        "\n",
        "    top_idx = sims.argsort()[-top_n:][::-1]\n",
        "    results = []\n",
        "    for i in top_idx:\n",
        "        results.append({\n",
        "            \"Chapter PDF\": paths[i],\n",
        "            \"Similarity Score\": round(float(sims[i]), 4)\n",
        "        })\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# -----------------------------\n",
        "# Recommend books & videos\n",
        "# -----------------------------\n",
        "@st.cache_data\n",
        "def recommend_materials(subject, level, topics):  # topics must be a tuple for cache hashing\n",
        "    if not topics:\n",
        "        return pd.DataFrame({\"Message\":[\"Please select at least one topic\"]})\n",
        "    recommended_books = books.get(subject, {}).get(level, [])\n",
        "    recommended_videos = videos.get(subject, {}).get(level, [])\n",
        "    return pd.DataFrame({\n",
        "        \"Topics\": [\", \".join(topics)],\n",
        "        \"Recommended Books\": [\", \".join(recommended_books)],\n",
        "        \"Recommended Videos\": [\", \".join(recommended_videos)]\n",
        "    })\n",
        "\n",
        "# -----------------------------\n",
        "# Display recommendations\n",
        "# -----------------------------\n",
        "st.subheader(\"ðŸ“š Chapter Recommendations\")\n",
        "if st.button(\"Get Chapter Recommendations\"):\n",
        "    chapter_recs = get_chapter_recommendations(tuple(selected_topics))\n",
        "    st.dataframe(chapter_recs)\n",
        "\n",
        "st.subheader(\"ðŸ“– Book & Video Recommendations\")\n",
        "if st.button(\"Get Books & Videos\"):\n",
        "    materials = recommend_materials(selected_subject, selected_level, tuple(selected_topics))\n",
        "    st.dataframe(materials)"
      ],
      "metadata": {
        "id": "s5pR6LQr66jd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "42139d77-c80f-47e3-a4cd-72759b2c27be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The issue is that NCERT PDFs have very structured text â€” topics are typically chapter headings, section titles, and key terms that need a smarter extraction strategy. Let me look at what the current extractor actually produces vs. what it should.\n",
        "The core problems with the current extract_topics_from_text:\n",
        "\n",
        "Splits on . â€” breaks mid-sentence phrases like \"The Indian Constitution, adopted in 1949, established...\" into garbage fragments\n",
        "3â€“8 word filter â€” too broad, picks up random sentences not just headings\n",
        "No subject-awareness â€” treats all PDFs the same, mixes Economics fragments with Polity ones\n",
        "\n",
        "Here's a much smarter extractor:"
      ],
      "metadata": {
        "id": "2aNrwX_WIdRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "%%writefile app.py\n",
        "import os\n",
        "from pathlib import Path\n",
        "import zipfile\n",
        "import re\n",
        "\n",
        "import streamlit as st\n",
        "import gdown\n",
        "import requests\n",
        "import pandas as pd\n",
        "from pypdf import PdfReader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# -----------------------------\n",
        "# CONFIG\n",
        "# -----------------------------\n",
        "FILE_ID = \"1iKxF0Db1YjySxbFVCfQhOJ6CXYVwdxbC\"\n",
        "ZIP_PATH = \"ncert.zip\"\n",
        "EXTRACT_DIR = \"ncert_extracted\"\n",
        "\n",
        "SUBJECTS = [\"Polity\", \"Economics\", \"Sociology\", \"Psychology\", \"Business Studies\"]\n",
        "\n",
        "# -----------------------------\n",
        "# STREAMLIT SETUP\n",
        "# -----------------------------\n",
        "st.set_page_config(page_title=\"NCERT Learning Assistant\", layout=\"wide\")\n",
        "st.title(\"ðŸ“˜ NCERT Learning Assistant\")\n",
        "st.write(\"Select topics to get chapter, book, and video recommendations for Class 11â€“12 students\")\n",
        "\n",
        "# -----------------------------\n",
        "# DOWNLOAD & EXTRACT NCERT PDFs\n",
        "# -----------------------------\n",
        "\n",
        "def extract_all_zips(folder: str) -> None:\n",
        "    \"\"\"Recursively extract all nested ZIPs until none remain unextracted.\"\"\"\n",
        "    found_new = True\n",
        "    while found_new:\n",
        "        found_new = False\n",
        "        for root, _, files in os.walk(folder):\n",
        "            for file in files:\n",
        "                if not file.lower().endswith(\".zip\"):\n",
        "                    continue\n",
        "                zip_path   = os.path.join(root, file)\n",
        "                extract_to = os.path.join(root, os.path.splitext(file)[0])\n",
        "                if os.path.exists(extract_to):\n",
        "                    continue                      # already extracted, skip\n",
        "                try:\n",
        "                    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "                        zf.extractall(extract_to)\n",
        "                    found_new = True             # new folder found, scan again\n",
        "                except zipfile.BadZipFile:\n",
        "                    st.warning(f\"âš ï¸ Skipping corrupt ZIP: {file}\")\n",
        "                except Exception as e:\n",
        "                    st.warning(f\"âš ï¸ Could not extract {file}: {e}\")\n",
        "\n",
        "\n",
        "@st.cache_data\n",
        "def download_and_extract():\n",
        "    # â”€â”€ 1. DOWNLOAD â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    if not os.path.exists(ZIP_PATH):\n",
        "        with st.spinner(\"â¬‡ï¸ Downloading NCERT datasetâ€¦\"):\n",
        "            # Primary: gdown\n",
        "            downloaded = False\n",
        "            try:\n",
        "                gdown.download(id=FILE_ID, output=ZIP_PATH, quiet=False, fuzzy=True)\n",
        "                downloaded = os.path.exists(ZIP_PATH) and os.path.getsize(ZIP_PATH) > 10_000\n",
        "            except Exception as e:\n",
        "                st.warning(f\"gdown failed: {e}. Trying fallbackâ€¦\")\n",
        "\n",
        "            # Fallback: requests + Drive cookie-confirmation\n",
        "            if not downloaded:\n",
        "                try:\n",
        "                    session  = requests.Session()\n",
        "                    URL      = \"https://drive.google.com/uc?export=download\"\n",
        "                    response = session.get(URL, params={\"id\": FILE_ID}, stream=True)\n",
        "                    token    = next(\n",
        "                        (v for k, v in response.cookies.items()\n",
        "                         if k.startswith(\"download_warning\")), None\n",
        "                    )\n",
        "                    if token:\n",
        "                        response = session.get(URL, params={\"id\": FILE_ID, \"confirm\": token}, stream=True)\n",
        "                    else:\n",
        "                        response = session.get(\n",
        "                            f\"https://drive.google.com/uc?id={FILE_ID}&export=download&confirm=t\",\n",
        "                            stream=True,\n",
        "                        )\n",
        "                    with open(ZIP_PATH, \"wb\") as f:\n",
        "                        for chunk in response.iter_content(chunk_size=32_768):\n",
        "                            if chunk:\n",
        "                                f.write(chunk)\n",
        "                    if not os.path.exists(ZIP_PATH) or os.path.getsize(ZIP_PATH) < 10_000:\n",
        "                        raise ValueError(\"Downloaded file too small â€” likely an error page.\")\n",
        "                except Exception as e2:\n",
        "                    st.error(f\"âŒ Both download methods failed: {e2}\")\n",
        "                    st.stop()\n",
        "\n",
        "    # â”€â”€ 2. VALIDATE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    if not zipfile.is_zipfile(ZIP_PATH):\n",
        "        os.remove(ZIP_PATH)\n",
        "        st.error(\"âŒ Downloaded file is not a valid ZIP. Check FILE_ID and sharing settings.\")\n",
        "        st.stop()\n",
        "\n",
        "    # â”€â”€ 3. EXTRACT OUTER ZIP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    if not os.path.exists(EXTRACT_DIR):\n",
        "        with st.spinner(\"ðŸ“¦ Extracting outer ZIPâ€¦\"):\n",
        "            with zipfile.ZipFile(ZIP_PATH, \"r\") as zf:\n",
        "                zf.extractall(EXTRACT_DIR)\n",
        "\n",
        "    # â”€â”€ 4. EXTRACT ALL NESTED ZIPS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    with st.spinner(\"ðŸ“‚ Extracting subject/chapter ZIPsâ€¦\"):\n",
        "        extract_all_zips(EXTRACT_DIR)\n",
        "\n",
        "    # â”€â”€ 5. COLLECT PDFs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    pdf_files = [\n",
        "        os.path.join(root, f)\n",
        "        for root, _, files in os.walk(EXTRACT_DIR)\n",
        "        for f in files if f.lower().endswith(\".pdf\")\n",
        "    ]\n",
        "    st.info(f\"ðŸ“„ PDFs found: {len(pdf_files)}\")\n",
        "    return EXTRACT_DIR, pdf_files\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# PDF Utilities\n",
        "# -----------------------------\n",
        "def read_pdf(path):\n",
        "    try:\n",
        "        reader = PdfReader(path)\n",
        "        return \" \".join(p.extract_text() or \"\" for p in reader.pages)\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"(activity|let us|exercise|project|editor|reprint|copyright|isbn).*\", \" \", text, flags=re.I)\n",
        "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "@st.cache_data(show_spinner=\"ðŸ“– Reading PDFsâ€¦\")\n",
        "def load_all_texts():\n",
        "    texts = []\n",
        "    paths = []\n",
        "    for pdf in Path(EXTRACT_DIR).rglob(\"*.pdf\"):\n",
        "        t = clean_text(read_pdf(str(pdf)))\n",
        "        if len(t.split()) > 50:\n",
        "            texts.append(t)\n",
        "            paths.append(str(pdf.name))   # str, not Path â€” serializable by st.cache_data\n",
        "    return texts, paths\n",
        "\n",
        "# -----------------------------\n",
        "# EXTRACT TOPICS\n",
        "# -----------------------------\n",
        "# Noise words that indicate a fragment, not a real topic\n",
        "_NOISE = re.compile(\n",
        "    r\"^(the|a|an|this|that|these|those|it|its|in|on|at|by|for|of|to|and|or|but|\"\n",
        "    r\"such|also|thus|hence|therefore|however|moreover|furthermore|although|\"\n",
        "    r\"figure|table|box|note|source|see|ref|pg|pp|ibid|op|cit|ncert|reprint)\\b\",\n",
        "    re.I\n",
        ")\n",
        "_JUNK_CHARS = re.compile(r\"[\\(\\)\\[\\]\\{\\}@#$%^&*+=|\\\\/<>~`\\'\\\"]{2,}\")\n",
        "\n",
        "def extract_topics_from_text(text):\n",
        "    \"\"\"\n",
        "    Extract meaningful topic phrases from PDF text.\n",
        "    Strategy:\n",
        "      - Split on newlines AND common sentence-ending punctuation\n",
        "      - Keep only Title Case or ALL-CAPS short phrases (likely headings)\n",
        "      - Filter noise words, junk characters, and out-of-range lengths\n",
        "    \"\"\"\n",
        "    # Re-insert newlines at likely heading boundaries (all-caps words or Title Case runs)\n",
        "    segments = re.split(r\"[\\n]+|(?<=[a-z])\\.\\s+(?=[A-Z])\", text)\n",
        "    topics = []\n",
        "    for seg in segments:\n",
        "        seg = seg.strip()\n",
        "        # Basic guards\n",
        "        if not seg or len(seg) < 5:\n",
        "            continue\n",
        "        if _JUNK_CHARS.search(seg):\n",
        "            continue\n",
        "        # Strip leading numbering like \"1.\", \"2.3\", \"Chapter 4\"\n",
        "        cleaned = re.sub(r\"^(chapter\\s*\\d+[:\\.]?|unit\\s*\\d+[:\\.]?|[\\d]+[\\d\\.]*\\s*)\", \"\", seg, flags=re.I).strip()\n",
        "        if not cleaned or not cleaned[0].isupper():\n",
        "            continue\n",
        "        words = cleaned.split()\n",
        "        if not (2 <= len(words) <= 7):\n",
        "            continue\n",
        "        # Must look like a heading: Title Case or short ALL-CAPS\n",
        "        is_title_case = sum(1 for w in words if w[0].isupper()) >= len(words) * 0.6\n",
        "        is_all_caps   = cleaned.isupper() and 2 <= len(words) <= 4\n",
        "        if not (is_title_case or is_all_caps):\n",
        "            continue\n",
        "        # Filter noise starters\n",
        "        if _NOISE.match(cleaned):\n",
        "            continue\n",
        "        # Filter if it ends with a comma or common sentence connectors\n",
        "        if cleaned.endswith((\",\", \";\", \":\", \"and\", \"or\", \"the\")):\n",
        "            continue\n",
        "        topics.append(cleaned)\n",
        "    return list(set(topics))\n",
        "\n",
        "@st.cache_data(show_spinner=\"ðŸ” Extracting topicsâ€¦\")\n",
        "def get_all_topics():\n",
        "    texts, _ = load_all_texts()\n",
        "    all_topics = []\n",
        "    for text in texts:\n",
        "        all_topics.extend(extract_topics_from_text(text))\n",
        "    return sorted(list(set(all_topics)))[:500]\n",
        "\n",
        "# 1. Ensure data is there\n",
        "data_path, pdf_files = download_and_extract()\n",
        "\n",
        "# 2. Only get topics if we have files\n",
        "if pdf_files:\n",
        "    all_topics = get_all_topics()\n",
        "else:\n",
        "    all_topics = []\n",
        "\n",
        "# -----------------------------\n",
        "# Sentence Transformer\n",
        "# -----------------------------\n",
        "@st.cache_resource\n",
        "def load_embedder():\n",
        "    return SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "embedder = load_embedder()\n",
        "\n",
        "# -----------------------------\n",
        "# Books & Videos\n",
        "# -----------------------------\n",
        "books = {\n",
        "    \"Sociology\": {\n",
        "        \"Beginner\": [\"NCERT Sociology Class 11\", \"Understanding Society by Haralambos (Intro)\"],\n",
        "        \"Intermediate\": [\"Introduction to Sociology by Bottomore\", \"Sociology: Themes and Perspectives by Haralambos\"],\n",
        "        \"Advanced\": [\"Advanced Sociology by Giddens\", \"Sociology: Concepts and Theories by Macionis\"]\n",
        "    },\n",
        "    \"Polity\": {\n",
        "        \"Beginner\": [\"NCERT Political Science Class 11\", \"Indian Constitution Made Easy\"],\n",
        "        \"Intermediate\": [\"Indian Polity by M. Laxmikanth\", \"Governance in India by D.D. Basu\"],\n",
        "        \"Advanced\": [\"Introduction to the Constitution of India by D.D. Basu\", \"Indian Constitutional Law Advanced\"]\n",
        "    },\n",
        "    \"Psychology\": {\n",
        "        \"Beginner\": [\"NCERT Psychology Class 11\", \"Psychology Basics Intro\"],\n",
        "        \"Intermediate\": [\"Psychology: An Exploration by Saundra K. Ciccarelli\", \"Understanding Psychology by Feldman\"],\n",
        "        \"Advanced\": [\"Advanced Psychology by Baron & Misra\", \"Handbook of Psychology by Weiner\"]\n",
        "    },\n",
        "    \"Economics\": {\n",
        "        \"Beginner\": [\"NCERT Economics Class 11\", \"Principles of Economics Intro by Mankiw\"],\n",
        "        \"Intermediate\": [\"Principles of Economics by N. Gregory Mankiw\", \"Economic Theory by Samuelson\"],\n",
        "        \"Advanced\": [\"Advanced Economic Theory by H.L. Ahuja\", \"Micro and Macro Economics Advanced\"]\n",
        "    },\n",
        "    \"Business Studies\": {\n",
        "        \"Beginner\": [\"NCERT Business Studies Class 11\", \"Introduction to Management Basics\"],\n",
        "        \"Intermediate\": [\"Business Studies by Poonam Gandhi\", \"Business Management Concepts\"],\n",
        "        \"Advanced\": [\"Advanced Business Management by Robbins\", \"Strategic Management Advanced\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "videos = {\n",
        "    subject: {\n",
        "        level: [f\"https://www.youtube.com/watch?v={subject}_{level}_{i+1}\" for i in range(3)]\n",
        "        for level in [\"Beginner\", \"Intermediate\", \"Advanced\"]\n",
        "    } for subject in SUBJECTS\n",
        "}\n",
        "\n",
        "# -----------------------------\n",
        "# Streamlit UI\n",
        "# -----------------------------\n",
        "st.sidebar.header(\"ðŸŽ“ Select Options\")\n",
        "selected_subject = st.sidebar.selectbox(\"Select Subject\", SUBJECTS)\n",
        "selected_level = st.sidebar.selectbox(\"Select Level\", [\"Beginner\", \"Intermediate\", \"Advanced\"])\n",
        "if not all_topics:\n",
        "    st.sidebar.warning(\"No topics found yet. Please wait for extraction or check your PDF folder.\")\n",
        "    selected_topics = st.sidebar.multiselect(\"Select Topic(s)\", [\"Loading...\"], disabled=True)\n",
        "else:\n",
        "    selected_topics = st.sidebar.multiselect(\"Select Topic(s)\", all_topics)\n",
        "\n",
        "# -----------------------------\n",
        "# Recommend chapters using embeddings\n",
        "# -----------------------------\n",
        "@st.cache_data(show_spinner=\"ðŸ”Ž Finding relevant chaptersâ€¦\")\n",
        "def get_chapter_recommendations(topics, top_n=5):  # topics must be a tuple for cache hashing\n",
        "    if not topics:\n",
        "        return pd.DataFrame({\"Message\":[\"Please select topics\"]})\n",
        "\n",
        "    texts, paths = load_all_texts()\n",
        "    text_embeddings = embedder.encode(texts, convert_to_numpy=True, show_progress_bar=False)\n",
        "    topic_embedding = embedder.encode([\" \".join(topics)], convert_to_numpy=True)\n",
        "\n",
        "    sims = cosine_similarity(topic_embedding, text_embeddings).flatten()\n",
        "\n",
        "    top_idx = sims.argsort()[-top_n:][::-1]\n",
        "    results = []\n",
        "    for i in top_idx:\n",
        "        results.append({\n",
        "            \"Chapter PDF\": paths[i],\n",
        "            \"Similarity Score\": round(float(sims[i]), 4)\n",
        "        })\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# -----------------------------\n",
        "# Recommend books & videos\n",
        "# -----------------------------\n",
        "@st.cache_data\n",
        "def recommend_materials(subject, level, topics):  # topics must be a tuple for cache hashing\n",
        "    if not topics:\n",
        "        return pd.DataFrame({\"Message\":[\"Please select at least one topic\"]})\n",
        "    recommended_books = books.get(subject, {}).get(level, [])\n",
        "    recommended_videos = videos.get(subject, {}).get(level, [])\n",
        "    return pd.DataFrame({\n",
        "        \"Topics\": [\", \".join(topics)],\n",
        "        \"Recommended Books\": [\", \".join(recommended_books)],\n",
        "        \"Recommended Videos\": [\", \".join(recommended_videos)]\n",
        "    })\n",
        "\n",
        "# -----------------------------\n",
        "# Display recommendations\n",
        "# -----------------------------\n",
        "st.subheader(\"ðŸ“š Chapter Recommendations\")\n",
        "if st.button(\"Get Chapter Recommendations\"):\n",
        "    chapter_recs = get_chapter_recommendations(tuple(selected_topics))\n",
        "    st.dataframe(chapter_recs)\n",
        "\n",
        "st.subheader(\"ðŸ“– Book & Video Recommendations\")\n",
        "if st.button(\"Get Books & Videos\"):\n",
        "    materials = recommend_materials(selected_subject, selected_level, tuple(selected_topics))\n",
        "    st.dataframe(materials)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "0NzRruG1If_5",
        "outputId": "276bef1b-19bc-4c15-d0ac-aebab2b393bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ignore below for now\n"
      ],
      "metadata": {
        "id": "zRc3QWbE54TP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNQNciC_EqqZ",
        "outputId": "ff9d1ae7-1a04-4fcd-8699-54a44e543488",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "%%writefile app.py\n",
        "import os\n",
        "import zipfile\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "import streamlit as st\n",
        "import gdown\n",
        "import pandas as pd\n",
        "from pypdf import PdfReader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# -----------------------------\n",
        "# 1. CONFIG & DATA (Defined First)\n",
        "# -----------------------------\n",
        "FILE_ID = \"1iKxF0Db1YjySxbFVCfQhOJ6CXYVwdxbC\"\n",
        "ZIP_PATH = \"ncert.zip\"\n",
        "EXTRACT_DIR = \"ncert_extracted\"\n",
        "SUBJECTS = [\"Polity\", \"Economics\", \"Sociology\", \"Psychology\", \"Business Studies\"]\n",
        "\n",
        "# Recommendation Data\n",
        "books = {\n",
        "    \"Sociology\": {\n",
        "        \"Beginner\": [\"NCERT Sociology Class 11\", \"Understanding Society by Haralambos (Intro)\"],\n",
        "        \"Intermediate\": [\"Introduction to Sociology by Bottomore\", \"Sociology: Themes and Perspectives by Haralambos\"],\n",
        "        \"Advanced\": [\"Advanced Sociology by Giddens\", \"Sociology: Concepts and Theories by Macionis\"]\n",
        "    },\n",
        "    \"Polity\": {\n",
        "        \"Beginner\": [\"NCERT Political Science Class 11\", \"Indian Constitution Made Easy\"],\n",
        "        \"Intermediate\": [\"Indian Polity by M. Laxmikanth\", \"Governance in India by D.D. Basu\"],\n",
        "        \"Advanced\": [\"Introduction to the Constitution of India by D.D. Basu\", \"Indian Constitutional Law Advanced\"]\n",
        "    },\n",
        "    \"Psychology\": {\n",
        "        \"Beginner\": [\"NCERT Psychology Class 11\", \"Psychology Basics Intro\"],\n",
        "        \"Intermediate\": [\"Psychology: An Exploration by Saundra K. Ciccarelli\", \"Understanding Psychology by Feldman\"],\n",
        "        \"Advanced\": [\"Advanced Psychology by Baron & Misra\", \"Handbook of Psychology by Weiner\"]\n",
        "    },\n",
        "    \"Economics\": {\n",
        "        \"Beginner\": [\"NCERT Economics Class 11\", \"Principles of Economics Intro by Mankiw\"],\n",
        "        \"Intermediate\": [\"Principles of Economics by N. Gregory Mankiw\", \"Economic Theory by Samuelson\"],\n",
        "        \"Advanced\": [\"Advanced Economic Theory by H.L. Ahuja\", \"Micro and Macro Economics Advanced\"]\n",
        "    },\n",
        "    \"Business Studies\": {\n",
        "        \"Beginner\": [\"NCERT Business Studies Class 11\", \"Introduction to Management Basics\"],\n",
        "        \"Intermediate\": [\"Business Studies by Poonam Gandhi\", \"Business Management Concepts\"],\n",
        "        \"Advanced\": [\"Advanced Business Management by Robbins\", \"Strategic Management Advanced\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "# -----------------------------\n",
        "# 2. DOWNLOAD & EXTRACTION\n",
        "# -----------------------------\n",
        "st.set_page_config(page_title=\"NCERT Learning Assistant\", layout=\"wide\")\n",
        "st.title(\"ðŸ“˜ NCERT Learning Assistant\")\n",
        "\n",
        "def extract_all_zips(folder):\n",
        "    found_new = True\n",
        "    while found_new:\n",
        "        found_new = False\n",
        "        for root, _, files in os.walk(folder):\n",
        "            for file in files:\n",
        "                if file.lower().endswith(\".zip\"):\n",
        "                    zip_path = os.path.join(root, file)\n",
        "                    extract_to = os.path.join(root, os.path.splitext(file)[0])\n",
        "                    if not os.path.exists(extract_to):\n",
        "                        try:\n",
        "                            with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "                                zf.extractall(extract_to)\n",
        "                            found_new = True\n",
        "                        except: pass\n",
        "\n",
        "@st.cache_resource\n",
        "def setup_data():\n",
        "    if not os.path.exists(ZIP_PATH):\n",
        "        with st.spinner(\"â¬‡ï¸ Downloading NCERT dataset...\"):\n",
        "            gdown.download(id=FILE_ID, output=ZIP_PATH, quiet=False)\n",
        "\n",
        "    if not os.path.exists(EXTRACT_DIR):\n",
        "        with st.spinner(\"ðŸ“¦ Extracting...\"):\n",
        "            with zipfile.ZipFile(ZIP_PATH, \"r\") as zf:\n",
        "                zf.extractall(EXTRACT_DIR)\n",
        "            extract_all_zips(EXTRACT_DIR)\n",
        "\n",
        "    return [str(p) for p in Path(EXTRACT_DIR).rglob(\"*.pdf\")]\n",
        "\n",
        "pdf_paths = setup_data()\n",
        "\n",
        "# -----------------------------\n",
        "# 3. TEXT PROCESSING\n",
        "# -----------------------------\n",
        "@st.cache_data\n",
        "def process_pdfs(paths):\n",
        "    texts, valid_paths, topics = [], [], []\n",
        "    for path in paths:\n",
        "        try:\n",
        "            reader = PdfReader(path)\n",
        "            # Read first few pages for topic extraction and content\n",
        "            content = \" \".join([page.extract_text() or \"\" for page in reader.pages[:15]])\n",
        "            if len(content.split()) > 100:\n",
        "                texts.append(content)\n",
        "                valid_paths.append(path)\n",
        "                # Simple topic heuristic: capitalized lines with 3-6 words\n",
        "                lines = content.split('\\n')\n",
        "                for line in lines[:50]:\n",
        "                    clean_line = line.strip()\n",
        "                    if 3 <= len(clean_line.split()) <= 6 and clean_line[0].isupper():\n",
        "                        topics.append(clean_line)\n",
        "        except: continue\n",
        "    return texts, valid_paths, sorted(list(set(topics)))\n",
        "\n",
        "all_texts, valid_pdf_paths, extracted_topics = process_pdfs(pdf_paths)\n",
        "\n",
        "# -----------------------------\n",
        "# 4. EMBEDDINGS & SEARCH\n",
        "# -----------------------------\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    return SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "model = load_model()\n",
        "\n",
        "@st.cache_data\n",
        "def get_vectors(_texts):\n",
        "    return model.encode(_texts, convert_to_tensor=True).cpu().numpy()\n",
        "\n",
        "# -----------------------------\n",
        "# 5. UI & OUTPUT\n",
        "# -----------------------------\n",
        "if not all_texts:\n",
        "    st.error(\"No PDFs found or readable. Please check your data source.\")\n",
        "else:\n",
        "    vectors = get_vectors(all_texts)\n",
        "\n",
        "    st.sidebar.header(\"ðŸŽ“ Learning Path\")\n",
        "    selected_subject = st.sidebar.selectbox(\"Subject\", SUBJECTS)\n",
        "    selected_level = st.sidebar.selectbox(\"Level\", [\"Beginner\", \"Intermediate\", \"Advanced\"])\n",
        "\n",
        "    # Use extracted topics if found, else fallback\n",
        "    topic_options = extracted_topics if len(extracted_topics) > 5 else [\"Constitution\", \"Demand and Supply\", \"Social Groups\"]\n",
        "    selected_topics = st.sidebar.multiselect(\"Select Topics of Interest\", topic_options)\n",
        "\n",
        "    if st.button(\"ðŸš€ Generate My Learning Plan\"):\n",
        "        if not selected_topics:\n",
        "            st.warning(\"Please select at least one topic in the sidebar.\")\n",
        "        else:\n",
        "            # Chapter Matching\n",
        "            st.header(\"ðŸ“– Recommended NCERT Chapters\")\n",
        "            query_vec = model.encode([\" \".join(selected_topics)])\n",
        "            scores = cosine_similarity(query_vec, vectors).flatten()\n",
        "            top_hits = scores.argsort()[-3:][::-1]\n",
        "\n",
        "            for idx in top_hits:\n",
        "                p = Path(valid_pdf_paths[idx])\n",
        "                st.success(f\"**{p.name}** â€” Match Score: {scores[idx]:.2%}\")\n",
        "                st.caption(f\"Location: {p.parent.name}\")\n",
        "\n",
        "            st.divider()\n",
        "\n",
        "            # Resources Matching\n",
        "            col1, col2 = st.columns(2)\n",
        "            with col1:\n",
        "                st.subheader(\"ðŸ“š External Books\")\n",
        "                book_list = books.get(selected_subject, {}).get(selected_level, [])\n",
        "                for b in book_list:\n",
        "                    st.write(f\"âœ… {b}\")\n",
        "\n",
        "            with col2:\n",
        "                st.subheader(\"ðŸŽ¥ Video Lectures\")\n",
        "                st.info(f\"Searching for {selected_level} content on {selected_subject}...\")\n",
        "                # Dynamic search link\n",
        "                search_query = \"+\".join([selected_subject] + selected_topics[:2])\n",
        "                st.write(f\"[Click here to search YouTube for these topics](https://www.youtube.com/results?search_query={search_query})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "npC0sTkKJCEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "%%writefile app.py\n",
        "import os\n",
        "import re\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "import gdown\n",
        "import pandas as pd\n",
        "import requests\n",
        "import streamlit as st\n",
        "from pypdf import PdfReader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# -----------------------------\n",
        "# CONFIG\n",
        "# -----------------------------\n",
        "FILE_ID     = \"1iKxF0Db1YjySxbFVCfQhOJ6CXYVwdxbC\"\n",
        "ZIP_PATH    = \"ncert.zip\"\n",
        "EXTRACT_DIR = \"ncert_extracted\"\n",
        "\n",
        "SUBJECTS = [\"Polity\", \"Economics\", \"Sociology\", \"Psychology\", \"Business Studies\"]\n",
        "LEVELS   = [\"Beginner\", \"Intermediate\", \"Advanced\"]\n",
        "\n",
        "# ==========================================================\n",
        "# STREAMLIT PAGE SETUP\n",
        "# ==========================================================\n",
        "st.set_page_config(page_title=\"NCERT Learning Assistant\", layout=\"wide\")\n",
        "\n",
        "st.markdown(\"\"\"\n",
        "    <style>\n",
        "        .block-container { padding-top: 2rem; }\n",
        "        .stButton > button {\n",
        "            width: 100%;\n",
        "            background-color: #1a73e8;\n",
        "            color: white;\n",
        "            border: none;\n",
        "            border-radius: 8px;\n",
        "            padding: 0.5rem 1rem;\n",
        "            font-weight: 600;\n",
        "        }\n",
        "        .stButton > button:hover { background-color: #1558b0; }\n",
        "        .stDataFrame { border-radius: 8px; }\n",
        "        h1 { color: #1a73e8; }\n",
        "        h2, h3 { color: #333; }\n",
        "    </style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "st.title(\"ðŸ“˜ NCERT Learning Assistant\")\n",
        "st.caption(\"Chapter, book, and video recommendations for Class 11â€“12 students.\")\n",
        "\n",
        "# ==========================================================\n",
        "# STEP 1 â€” RECURSIVE ZIP EXTRACTOR\n",
        "# ==========================================================\n",
        "def extract_all_zips(folder: str) -> None:\n",
        "    \"\"\"\n",
        "    Recursively find and extract every .zip inside `folder`.\n",
        "    Keeps looping until no new ZIPs are discovered â€” handles\n",
        "    arbitrary nesting depth automatically.\n",
        "    Warns on corrupt ZIPs and continues; skips already-extracted folders.\n",
        "    \"\"\"\n",
        "    found_new = True\n",
        "    while found_new:\n",
        "        found_new = False\n",
        "        for root, _, files in os.walk(folder):\n",
        "            for file in files:\n",
        "                if not file.lower().endswith(\".zip\"):\n",
        "                    continue\n",
        "                zip_path   = os.path.join(root, file)\n",
        "                extract_to = os.path.join(root, os.path.splitext(file)[0])\n",
        "                if os.path.exists(extract_to):\n",
        "                    continue\n",
        "                try:\n",
        "                    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "                        zf.extractall(extract_to)\n",
        "                    found_new = True\n",
        "                except zipfile.BadZipFile:\n",
        "                    st.warning(f\"âš ï¸ Skipping corrupt ZIP: {file}\")\n",
        "                except Exception as e:\n",
        "                    st.warning(f\"âš ï¸ Could not extract {file}: {e}\")\n",
        "\n",
        "\n",
        "# ==========================================================\n",
        "# STEP 2 â€” DOWNLOAD + EXTRACT  (runs once per session)\n",
        "# ==========================================================\n",
        "@st.cache_resource\n",
        "def download_and_extract(file_id: str = FILE_ID) -> tuple[str, list[str]]:\n",
        "    \"\"\"\n",
        "    1. Download outer ZIP from Google Drive (gdown, then requests fallback).\n",
        "    2. Validate it is a real ZIP.\n",
        "    3. Extract outer ZIP into EXTRACT_DIR.\n",
        "    4. Recursively extract all nested subject/chapter ZIPs.\n",
        "    5. Return (extract_dir, list_of_all_pdf_paths).\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Download ---\n",
        "    if not os.path.exists(ZIP_PATH):\n",
        "        with st.spinner(\"â¬‡ï¸ Downloading NCERT datasetâ€¦\"):\n",
        "            downloaded = False\n",
        "            try:\n",
        "                gdown.download(id=file_id, output=ZIP_PATH, quiet=False, fuzzy=True)\n",
        "                downloaded = os.path.exists(ZIP_PATH) and os.path.getsize(ZIP_PATH) > 10_000\n",
        "            except Exception as e:\n",
        "                st.warning(f\"gdown failed: {e}. Trying requests fallbackâ€¦\")\n",
        "\n",
        "            if not downloaded:\n",
        "                try:\n",
        "                    session  = requests.Session()\n",
        "                    base_url = \"https://drive.google.com/uc?export=download\"\n",
        "                    resp     = session.get(base_url, params={\"id\": file_id}, stream=True)\n",
        "                    token    = next(\n",
        "                        (v for k, v in resp.cookies.items()\n",
        "                         if k.startswith(\"download_warning\")), None\n",
        "                    )\n",
        "                    if token:\n",
        "                        resp = session.get(base_url, params={\"id\": file_id, \"confirm\": token}, stream=True)\n",
        "                    else:\n",
        "                        resp = session.get(\n",
        "                            f\"https://drive.google.com/uc?id={file_id}&export=download&confirm=t\",\n",
        "                            stream=True,\n",
        "                        )\n",
        "                    with open(ZIP_PATH, \"wb\") as f:\n",
        "                        for chunk in resp.iter_content(chunk_size=32_768):\n",
        "                            if chunk:\n",
        "                                f.write(chunk)\n",
        "                    if not os.path.exists(ZIP_PATH) or os.path.getsize(ZIP_PATH) < 10_000:\n",
        "                        raise ValueError(\"File too small â€” likely an error page, not the ZIP.\")\n",
        "                except Exception as e2:\n",
        "                    st.error(f\"âŒ Both download methods failed: {e2}\")\n",
        "                    st.stop()\n",
        "\n",
        "    # --- Validate ---\n",
        "    if not zipfile.is_zipfile(ZIP_PATH):\n",
        "        os.remove(ZIP_PATH)\n",
        "        st.error(\"âŒ Downloaded file is not a valid ZIP. Check FILE_ID and sharing permissions.\")\n",
        "        st.stop()\n",
        "\n",
        "    # --- Extract outer ZIP ---\n",
        "    if not os.path.exists(EXTRACT_DIR):\n",
        "        with st.spinner(\"ðŸ“¦ Extracting outer ZIPâ€¦\"):\n",
        "            with zipfile.ZipFile(ZIP_PATH, \"r\") as zf:\n",
        "                zf.extractall(EXTRACT_DIR)\n",
        "\n",
        "    # --- Extract nested subject/chapter ZIPs ---\n",
        "    with st.spinner(\"ðŸ“‚ Extracting subject/chapter ZIPsâ€¦\"):\n",
        "        extract_all_zips(EXTRACT_DIR)\n",
        "\n",
        "    # --- Collect all PDFs ---\n",
        "    pdf_files = [\n",
        "        os.path.join(root, f)\n",
        "        for root, _, files in os.walk(EXTRACT_DIR)\n",
        "        for f in files\n",
        "        if f.lower().endswith(\".pdf\")\n",
        "    ]\n",
        "\n",
        "    st.success(f\"âœ… Extraction complete â€” {len(pdf_files)} PDFs ready.\")\n",
        "    return EXTRACT_DIR, pdf_files\n",
        "\n",
        "\n",
        "data_path, pdf_files = download_and_extract()\n",
        "\n",
        "# ==========================================================\n",
        "# PDF UTILITIES\n",
        "# ==========================================================\n",
        "def read_pdf(path: str) -> str:\n",
        "    try:\n",
        "        reader = PdfReader(path)\n",
        "        return \" \".join(p.extract_text() or \"\" for p in reader.pages)\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    text = re.sub(\n",
        "        r\"(activity|let us|exercise|project|editor|reprint|copyright|isbn).*\",\n",
        "        \" \", text, flags=re.I,\n",
        "    )\n",
        "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "@st.cache_data(show_spinner=\"ðŸ“– Reading PDFsâ€¦\")\n",
        "def load_all_texts() -> tuple[list[str], list[Path]]:\n",
        "    texts, paths = [], []\n",
        "    for pdf in Path(EXTRACT_DIR).rglob(\"*.pdf\"):\n",
        "        t = clean_text(read_pdf(str(pdf)))\n",
        "        if len(t.split()) > 50:\n",
        "            texts.append(t)\n",
        "            paths.append(pdf)\n",
        "    return texts, paths\n",
        "\n",
        "# ==========================================================\n",
        "# TOPIC EXTRACTION\n",
        "# ==========================================================\n",
        "def extract_topics_from_text(text: str) -> list[str]:\n",
        "    topics = []\n",
        "    for line in text.split(\"\\n\"):\n",
        "        line = line.strip()\n",
        "        if 3 <= len(line.split()) <= 8 and line and line[0].isupper():\n",
        "            cleaned = re.sub(r\"^(chapter\\s*\\d+:?|[\\d.]+\\s*)\", \"\", line, flags=re.I).strip()\n",
        "            if cleaned:\n",
        "                topics.append(cleaned)\n",
        "    return list(set(topics))\n",
        "\n",
        "@st.cache_data(show_spinner=\"ðŸ” Extracting topicsâ€¦\")\n",
        "def get_all_topics() -> list[str]:\n",
        "    texts, _ = load_all_texts()\n",
        "    all_topics: set[str] = set()\n",
        "    for text in texts:\n",
        "        all_topics.update(extract_topics_from_text(text))\n",
        "    return sorted(all_topics)[:500]\n",
        "\n",
        "all_topics = get_all_topics()\n",
        "\n",
        "# ==========================================================\n",
        "# SENTENCE TRANSFORMER\n",
        "# ==========================================================\n",
        "@st.cache_resource(show_spinner=\"ðŸ¤– Loading embedding modelâ€¦\")\n",
        "def load_embedder():\n",
        "    return SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "embedder = load_embedder()\n",
        "\n",
        "# ==========================================================\n",
        "# BOOKS & VIDEOS DATA\n",
        "# ==========================================================\n",
        "books = {\n",
        "    \"Sociology\": {\n",
        "        \"Beginner\":     [\"NCERT Sociology Class 11\", \"Understanding Society by Haralambos (Intro)\"],\n",
        "        \"Intermediate\": [\"Introduction to Sociology by Bottomore\", \"Sociology: Themes and Perspectives by Haralambos\"],\n",
        "        \"Advanced\":     [\"Advanced Sociology by Giddens\", \"Sociology: Concepts and Theories by Macionis\"],\n",
        "    },\n",
        "    \"Polity\": {\n",
        "        \"Beginner\":     [\"NCERT Political Science Class 11\", \"Indian Constitution Made Easy\"],\n",
        "        \"Intermediate\": [\"Indian Polity by M. Laxmikanth\", \"Governance in India by D.D. Basu\"],\n",
        "        \"Advanced\":     [\"Introduction to the Constitution of India by D.D. Basu\", \"Indian Constitutional Law Advanced\"],\n",
        "    },\n",
        "    \"Psychology\": {\n",
        "        \"Beginner\":     [\"NCERT Psychology Class 11\", \"Psychology Basics Intro\"],\n",
        "        \"Intermediate\": [\"Psychology: An Exploration by Ciccarelli\", \"Understanding Psychology by Feldman\"],\n",
        "        \"Advanced\":     [\"Advanced Psychology by Baron & Misra\", \"Handbook of Psychology by Weiner\"],\n",
        "    },\n",
        "    \"Economics\": {\n",
        "        \"Beginner\":     [\"NCERT Economics Class 11\", \"Principles of Economics Intro by Mankiw\"],\n",
        "        \"Intermediate\": [\"Principles of Economics by N. Gregory Mankiw\", \"Economic Theory by Samuelson\"],\n",
        "        \"Advanced\":     [\"Advanced Economic Theory by H.L. Ahuja\", \"Micro and Macro Economics Advanced\"],\n",
        "    },\n",
        "    \"Business Studies\": {\n",
        "        \"Beginner\":     [\"NCERT Business Studies Class 11\", \"Introduction to Management Basics\"],\n",
        "        \"Intermediate\": [\"Business Studies by Poonam Gandhi\", \"Business Management Concepts\"],\n",
        "        \"Advanced\":     [\"Advanced Business Management by Robbins\", \"Strategic Management Advanced\"],\n",
        "    },\n",
        "}\n",
        "\n",
        "def _yt_search(subject: str, level: str) -> str:\n",
        "    query = f\"NCERT+{subject}+{level}+Class+11+12\".replace(\" \", \"+\")\n",
        "    return f\"https://www.youtube.com/results?search_query={query}\"\n",
        "\n",
        "videos = {\n",
        "    subject: {level: _yt_search(subject, level) for level in LEVELS}\n",
        "    for subject in SUBJECTS\n",
        "}\n",
        "\n",
        "# ==========================================================\n",
        "# CHAPTER RECOMMENDATIONS\n",
        "# ==========================================================\n",
        "@st.cache_data(show_spinner=\"ðŸ”Ž Computing recommendationsâ€¦\")\n",
        "def get_chapter_recommendations(topics: tuple, top_n: int = 5) -> pd.DataFrame:\n",
        "    if not topics:\n",
        "        return pd.DataFrame({\"Message\": [\"Please select at least one topic.\"]})\n",
        "\n",
        "    texts, paths = load_all_texts()\n",
        "    if not texts:\n",
        "        return pd.DataFrame({\"Message\": [\"No PDF text found â€” check extraction.\"]})\n",
        "\n",
        "    text_embs = embedder.encode(texts, convert_to_numpy=True, show_progress_bar=False)\n",
        "    topic_emb = embedder.encode([\" \".join(topics)], convert_to_numpy=True)\n",
        "    sims      = cosine_similarity(topic_emb, text_embs).flatten()\n",
        "    top_idx   = sims.argsort()[-top_n:][::-1]\n",
        "\n",
        "    return pd.DataFrame([\n",
        "        {\"Chapter PDF\": paths[i].name, \"Similarity Score\": round(float(sims[i]), 4)}\n",
        "        for i in top_idx\n",
        "    ])\n",
        "\n",
        "# ==========================================================\n",
        "# BOOK & VIDEO RECOMMENDATIONS\n",
        "# ==========================================================\n",
        "def recommend_materials(subject: str, level: str, topics: list[str]) -> pd.DataFrame:\n",
        "    if not topics:\n",
        "        return pd.DataFrame({\"Message\": [\"Please select at least one topic.\"]})\n",
        "    return pd.DataFrame({\n",
        "        \"Subject\":             [subject],\n",
        "        \"Level\":               [level],\n",
        "        \"Topics\":              [\", \".join(topics)],\n",
        "        \"Recommended Books\":   [\" | \".join(books.get(subject, {}).get(level, []))],\n",
        "        \"YouTube Search Link\": [videos.get(subject, {}).get(level, \"\")],\n",
        "    })\n",
        "\n",
        "# ==========================================================\n",
        "# SIDEBAR\n",
        "# ==========================================================\n",
        "st.sidebar.header(\"ðŸŽ“ Select Options\")\n",
        "\n",
        "col_m1, col_m2, col_m3 = st.sidebar.columns(3)\n",
        "col_m1.metric(\"PDFs\", len(pdf_files))\n",
        "col_m2.metric(\"Topics\", len(all_topics))\n",
        "col_m3.metric(\"Subjects\", len(SUBJECTS))\n",
        "\n",
        "st.sidebar.divider()\n",
        "\n",
        "selected_subject = st.sidebar.selectbox(\"Subject\", SUBJECTS)\n",
        "selected_level   = st.sidebar.selectbox(\"Level\",   LEVELS)\n",
        "selected_topics  = st.sidebar.multiselect(\n",
        "    \"Topic(s)\",\n",
        "    all_topics,\n",
        "    help=\"Select one or more topics to get personalized recommendations.\"\n",
        ")\n",
        "\n",
        "# ==========================================================\n",
        "# MAIN LAYOUT\n",
        "# ==========================================================\n",
        "col1, col2 = st.columns(2, gap=\"large\")\n",
        "\n",
        "with col1:\n",
        "    st.subheader(\"ðŸ“š Chapter Recommendations\")\n",
        "    st.caption(\"Most relevant chapters from the NCERT PDFs based on your topics.\")\n",
        "    if st.button(\"Get Chapter Recommendations\", key=\"ch_btn\"):\n",
        "        if not selected_topics:\n",
        "            st.warning(\"Please select at least one topic from the sidebar.\")\n",
        "        else:\n",
        "            df = get_chapter_recommendations(tuple(selected_topics))\n",
        "            st.dataframe(df, use_container_width=True, hide_index=True)\n",
        "\n",
        "with col2:\n",
        "    st.subheader(\"ðŸ“– Books & Videos\")\n",
        "    st.caption(\"Curated books and YouTube resources for your subject and level.\")\n",
        "    if st.button(\"Get Books & Videos\", key=\"bv_btn\"):\n",
        "        if not selected_topics:\n",
        "            st.warning(\"Please select at least one topic from the sidebar.\")\n",
        "        else:\n",
        "            df = recommend_materials(selected_subject, selected_level, selected_topics)\n",
        "            st.dataframe(df, use_container_width=True, hide_index=True)\n",
        "            yt_link = videos.get(selected_subject, {}).get(selected_level, \"\")\n",
        "            if yt_link:\n",
        "                st.markdown(f\"ðŸ”— [Search YouTube â†’ {selected_subject} ({selected_level})]({yt_link})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "cKLxRxSbyWuf",
        "outputId": "ed357958-9982-43b3-f374-b94ece7fb548"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "database added"
      ],
      "metadata": {
        "id": "xey4ZChI29ef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "%%writefile app.py\n",
        "import os\n",
        "import re\n",
        "import uuid\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "import streamlit as st\n",
        "import gdown\n",
        "import requests\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "from pypdf import PdfReader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# CONFIG\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "FILE_ID     = \"1iKxF0Db1YjySxbFVCfQhOJ6CXYVwdxbC\"\n",
        "ZIP_PATH    = \"ncert.zip\"\n",
        "EXTRACT_DIR = \"ncert_extracted\"\n",
        "DB_PATH     = \"/content/drive/MyDrive/ZenithIndia/AI Learning Assistant/ncert.db\"\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# PAGE CONFIG\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "st.set_page_config(page_title=\"NCERT Learning Assistant\", layout=\"wide\")\n",
        "st.title(\"ðŸ“˜ NCERT Learning Assistant\")\n",
        "st.write(\"Select topics to get chapter, book, and research paper recommendations for Class 11â€“12 students\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# MOUNT GOOGLE DRIVE\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    if not os.path.exists(\"/content/drive/MyDrive\"):\n",
        "        drive.mount(\"/content/drive\")\n",
        "except ImportError:\n",
        "    DB_PATH = \"ncert.db\"   # fallback for local runs\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# DATABASE â€” connect only, no table creation\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "@st.cache_resource\n",
        "def get_db():\n",
        "    if not os.path.exists(DB_PATH):\n",
        "        st.error(f\"âŒ Database not found at:\\n`{DB_PATH}`\\n\\nPlease run setup_db.ipynb first, or update DB_PATH at the top of app.py.\")\n",
        "        st.stop()\n",
        "    try:\n",
        "        conn = sqlite3.connect(DB_PATH, check_same_thread=False)\n",
        "        conn.row_factory = sqlite3.Row\n",
        "        # Quick sanity check â€” if file is corrupt/not a DB this will fail fast\n",
        "        conn.execute(\"SELECT 1\")\n",
        "        return conn\n",
        "    except sqlite3.DatabaseError as e:\n",
        "        st.error(f\"âŒ File exists but is not a valid SQLite database:\\n`{DB_PATH}`\\n\\nError: {e}\\n\\nPlease re-run setup_db.ipynb to recreate it.\")\n",
        "        st.stop()\n",
        "\n",
        "\n",
        "def get_subjects() -> list:\n",
        "    \"\"\"Load all subjects from DB â€” adding a new subject to DB instantly appears in app.\"\"\"\n",
        "    cur = get_db().cursor()\n",
        "    cur.execute(\"SELECT subject_name FROM subjects ORDER BY subject_name\")\n",
        "    return [row[\"subject_name\"] for row in cur.fetchall()]\n",
        "\n",
        "\n",
        "def get_levels(subject_name: str = None) -> list:\n",
        "    \"\"\"\n",
        "    Load distinct levels available for a subject from recommendations table.\n",
        "    Order: Beginner â†’ Intermediate â†’ Advanced.\n",
        "    Adding a new level to DB instantly appears in app.\n",
        "    \"\"\"\n",
        "    cur = get_db().cursor()\n",
        "    level_order = \"CASE level WHEN 'Beginner' THEN 1 WHEN 'Intermediate' THEN 2 WHEN 'Advanced' THEN 3 ELSE 4 END\"\n",
        "    if subject_name:\n",
        "        subject_id = subject_name.lower().replace(\" \", \"_\")\n",
        "        cur.execute(f\"\"\"\n",
        "            SELECT DISTINCT level FROM recommendations\n",
        "            WHERE subject_id = ?\n",
        "            ORDER BY {level_order}\n",
        "        \"\"\", (subject_id,))\n",
        "    else:\n",
        "        cur.execute(f\"\"\"\n",
        "            SELECT DISTINCT level FROM recommendations\n",
        "            ORDER BY {level_order}\n",
        "        \"\"\")\n",
        "    return [row[\"level\"] for row in cur.fetchall()]\n",
        "\n",
        "\n",
        "def get_recommendations(subject_name: str, level: str) -> list:\n",
        "    subject_id = subject_name.lower().replace(\" \", \"_\")\n",
        "    cur = get_db().cursor()\n",
        "    cur.execute(\"\"\"\n",
        "        SELECT title, author, level_type, why, link, journal, ncert_chapter_link\n",
        "        FROM recommendations\n",
        "        WHERE subject_id = ? AND level = ?\n",
        "        ORDER BY rec_id\n",
        "    \"\"\", (subject_id, level))\n",
        "    return [dict(row) for row in cur.fetchall()]\n",
        "\n",
        "\n",
        "def save_topics_to_db(topics: list, source_pdf: str) -> None:\n",
        "    conn = get_db()\n",
        "    conn.executemany(\n",
        "        \"INSERT OR IGNORE INTO topics (topic_text, source_pdf) VALUES (?, ?)\",\n",
        "        [(t, source_pdf) for t in topics]\n",
        "    )\n",
        "    conn.commit()\n",
        "\n",
        "\n",
        "def load_topics_from_db() -> list:\n",
        "    cur = get_db().cursor()\n",
        "    cur.execute(\"SELECT DISTINCT topic_text FROM topics ORDER BY topic_text LIMIT 500\")\n",
        "    return [row[\"topic_text\"] for row in cur.fetchall()]\n",
        "\n",
        "\n",
        "def load_topics_from_chapters(subject_id: str = None) -> list:\n",
        "    \"\"\"\n",
        "    Load clean topic list from the chapters table.\n",
        "    These are real NCERT chapter names â€” no noise.\n",
        "    Optionally filter by subject.\n",
        "    \"\"\"\n",
        "    cur = get_db().cursor()\n",
        "    if subject_id:\n",
        "        cur.execute(\"\"\"\n",
        "            SELECT DISTINCT chapter_name FROM chapters\n",
        "            WHERE is_special = 0 AND subject_id = ?\n",
        "            ORDER BY chapter_name\n",
        "        \"\"\", (subject_id,))\n",
        "    else:\n",
        "        cur.execute(\"\"\"\n",
        "            SELECT DISTINCT chapter_name FROM chapters\n",
        "            WHERE is_special = 0\n",
        "            ORDER BY chapter_name\n",
        "        \"\"\")\n",
        "    return [row[\"chapter_name\"] for row in cur.fetchall()]\n",
        "\n",
        "\n",
        "'''def log_user_search(session_id: str, subject: str, level: str, topics: list) -> None:\n",
        "    conn = get_db()\n",
        "    conn.execute(\"\"\"\n",
        "        INSERT INTO user_history (session_id, subject_id, level, selected_topics)\n",
        "        VALUES (?, ?, ?, ?)\n",
        "    \"\"\", (session_id, subject.lower().replace(\" \", \"_\"), level, str(topics)))\n",
        "    conn.commit()'''\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# DOWNLOAD & EXTRACT NCERT PDFs\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def extract_all_zips(folder: str) -> None:\n",
        "    found_new = True\n",
        "    while found_new:\n",
        "        found_new = False\n",
        "        for root, _, files in os.walk(folder):\n",
        "            for file in files:\n",
        "                if not file.lower().endswith(\".zip\"):\n",
        "                    continue\n",
        "                zip_path   = os.path.join(root, file)\n",
        "                extract_to = os.path.join(root, os.path.splitext(file)[0])\n",
        "                if os.path.exists(extract_to):\n",
        "                    continue\n",
        "                try:\n",
        "                    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "                        zf.extractall(extract_to)\n",
        "                    found_new = True\n",
        "                except zipfile.BadZipFile:\n",
        "                    st.warning(f\"âš ï¸ Skipping corrupt ZIP: {file}\")\n",
        "                except Exception as e:\n",
        "                    st.warning(f\"âš ï¸ Could not extract {file}: {e}\")\n",
        "\n",
        "\n",
        "@st.cache_data\n",
        "def download_and_extract():\n",
        "    if not os.path.exists(ZIP_PATH):\n",
        "        with st.spinner(\"â¬‡ï¸ Downloading NCERT datasetâ€¦\"):\n",
        "            downloaded = False\n",
        "            try:\n",
        "                gdown.download(id=FILE_ID, output=ZIP_PATH, quiet=False, fuzzy=True)\n",
        "                downloaded = os.path.exists(ZIP_PATH) and os.path.getsize(ZIP_PATH) > 10_000\n",
        "            except Exception as e:\n",
        "                st.warning(f\"gdown failed: {e}. Trying fallbackâ€¦\")\n",
        "\n",
        "            if not downloaded:\n",
        "                try:\n",
        "                    session  = requests.Session()\n",
        "                    URL      = \"https://drive.google.com/uc?export=download\"\n",
        "                    response = session.get(URL, params={\"id\": FILE_ID}, stream=True)\n",
        "                    token    = next(\n",
        "                        (v for k, v in response.cookies.items()\n",
        "                         if k.startswith(\"download_warning\")), None\n",
        "                    )\n",
        "                    if token:\n",
        "                        response = session.get(URL, params={\"id\": FILE_ID, \"confirm\": token}, stream=True)\n",
        "                    else:\n",
        "                        response = session.get(\n",
        "                            f\"https://drive.google.com/uc?id={FILE_ID}&export=download&confirm=t\",\n",
        "                            stream=True,\n",
        "                        )\n",
        "                    with open(ZIP_PATH, \"wb\") as f:\n",
        "                        for chunk in response.iter_content(chunk_size=32_768):\n",
        "                            if chunk:\n",
        "                                f.write(chunk)\n",
        "                    if not os.path.exists(ZIP_PATH) or os.path.getsize(ZIP_PATH) < 10_000:\n",
        "                        raise ValueError(\"Downloaded file too small.\")\n",
        "                except Exception as e2:\n",
        "                    st.error(f\"âŒ Both download methods failed: {e2}\")\n",
        "                    st.stop()\n",
        "\n",
        "    if not zipfile.is_zipfile(ZIP_PATH):\n",
        "        os.remove(ZIP_PATH)\n",
        "        st.error(\"âŒ Downloaded file is not a valid ZIP.\")\n",
        "        st.stop()\n",
        "\n",
        "    if not os.path.exists(EXTRACT_DIR):\n",
        "        with st.spinner(\"ðŸ“¦ Extracting outer ZIPâ€¦\"):\n",
        "            with zipfile.ZipFile(ZIP_PATH, \"r\") as zf:\n",
        "                zf.extractall(EXTRACT_DIR)\n",
        "\n",
        "    with st.spinner(\"ðŸ“‚ Extracting subject/chapter ZIPsâ€¦\"):\n",
        "        extract_all_zips(EXTRACT_DIR)\n",
        "\n",
        "    pdf_files = [\n",
        "        os.path.join(root, f)\n",
        "        for root, _, files in os.walk(EXTRACT_DIR)\n",
        "        for f in files if f.lower().endswith(\".pdf\")\n",
        "    ]\n",
        "    st.info(f\"ðŸ“„ PDFs found: {len(pdf_files)}\")\n",
        "    return EXTRACT_DIR, pdf_files\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# PDF UTILITIES\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def read_pdf(path: str) -> str:\n",
        "    try:\n",
        "        reader = PdfReader(path)\n",
        "        return \" \".join(p.extract_text() or \"\" for p in reader.pages)\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    text = re.sub(r\"(activity|let us|exercise|project|editor|reprint|copyright|isbn).*\", \" \", text, flags=re.I)\n",
        "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "\n",
        "@st.cache_data(show_spinner=\"ðŸ“– Reading PDFsâ€¦\")\n",
        "def load_all_texts():\n",
        "    texts, paths = [], []\n",
        "    for pdf in Path(EXTRACT_DIR).rglob(\"*.pdf\"):\n",
        "        t = clean_text(read_pdf(str(pdf)))\n",
        "        if len(t.split()) > 50:\n",
        "            texts.append(t)\n",
        "            paths.append(str(pdf.name))\n",
        "    return texts, paths\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# TOPICS â€” extracted from PDFs, cached in DB\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "# â”€â”€ Noise patterns to reject â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Bibliography entries: \"SHARMA, R. 2005. Society...\"\n",
        "_AUTHOR_ENTRY   = re.compile(r\"^[A-Z]{2,},\\s+[A-Z]\")\n",
        "# Any line with a year like \"1970.\" or \"(2005)\"\n",
        "_HAS_YEAR       = re.compile(r\"\b(19|20)\\d{2}\b\")\n",
        "# Publisher / place names\n",
        "_PUBLISHER      = re.compile(\n",
        "    r\"\b(press|publishing|publishers|edition|reprint|oxford|cambridge|mcgraw|\"\n",
        "    r\"pearson|routledge|sage|wiley|elsevier|springer|penguin|harper|norton|\"\n",
        "    r\"new york|new delhi|london|chicago|boston|mumbai|kolkata|chennai|\"\n",
        "    r\"pp\\.|vol\\.|ibid|op\\.cit|et al|isbn|doi|http|www\\.)\b\",\n",
        "    re.I\n",
        ")\n",
        "# Lines that start with noise words\n",
        "_NOISE_START    = re.compile(\n",
        "    r\"^(the|a|an|this|that|these|it|its|in|on|at|by|for|of|to|and|or|but|\"\n",
        "    r\"such|also|thus|hence|therefore|however|moreover|furthermore|although|\"\n",
        "    r\"figure|table|box|note|source|see|pg|pp|ref|ncert|reprint|let us|\"\n",
        "    r\"activity|exercise|project|chapter|unit|section)\b\",\n",
        "    re.I\n",
        ")\n",
        "# Must look like a proper heading: starts uppercase, only letters/spaces/hyphens\n",
        "_VALID_HEADING  = re.compile(r\"^[A-Z][a-zA-Z\\s\\-:']{3,55}$\")\n",
        "\n",
        "\n",
        "def extract_topics_from_text(text: str) -> list:\n",
        "    \"\"\"\n",
        "    Strict extractor â€” only picks real section/chapter headings from PDF text.\n",
        "\n",
        "    Rules:\n",
        "    - 2 to 6 words only\n",
        "    - Must start with uppercase\n",
        "    - Must be mostly title-cased (â‰¥70% words capitalised)\n",
        "    - No years, no author surname patterns, no publisher names\n",
        "    - No punctuation endings, no digits\n",
        "    - Rejects known noise start words\n",
        "    \"\"\"\n",
        "    lines = text.split(\"\\n\")\n",
        "    topics = set()\n",
        "\n",
        "    for raw_line in lines:\n",
        "        line = raw_line.strip()\n",
        "\n",
        "        # â”€â”€ Basic length filter â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        words = line.split()\n",
        "        if not (2 <= len(words) <= 6):\n",
        "            continue\n",
        "\n",
        "        # â”€â”€ Hard rejections â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        if _AUTHOR_ENTRY.match(line):      # \"SHARMA, R. 2005...\"\n",
        "            continue\n",
        "        if _HAS_YEAR.search(line):         # anything with 1970, 2005 etc\n",
        "            continue\n",
        "        if _PUBLISHER.search(line):        # publisher / place names\n",
        "            continue\n",
        "        if _NOISE_START.match(line):       # starts with filler word\n",
        "            continue\n",
        "        if not _VALID_HEADING.match(line): # must match heading pattern\n",
        "            continue\n",
        "\n",
        "        # â”€â”€ Endings filter â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        if line[-1] in \".,;:\":\n",
        "            continue\n",
        "        last_word = words[-1].lower()\n",
        "        if last_word in (\"and\", \"or\", \"the\", \"of\", \"a\", \"an\", \"in\", \"on\"):\n",
        "            continue\n",
        "\n",
        "        # â”€â”€ Digits filter â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        if any(c.isdigit() for c in line):\n",
        "            continue\n",
        "\n",
        "        # â”€â”€ Title-case check â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        # At least 70% of words must start with uppercase\n",
        "        titled = sum(1 for w in words if w and w[0].isupper())\n",
        "        if titled / len(words) < 0.70:\n",
        "            continue\n",
        "\n",
        "        topics.add(line)\n",
        "\n",
        "    return list(topics)\n",
        "\n",
        "\n",
        "def topics_in_db() -> bool:\n",
        "    \"\"\"Quick check â€” are any topics already saved in the DB?\"\"\"\n",
        "    cur = get_db().cursor()\n",
        "    cur.execute(\"SELECT COUNT(*) as n FROM topics\")\n",
        "    return cur.fetchone()[\"n\"] > 0\n",
        "\n",
        "\n",
        "@st.cache_data(show_spinner=False)\n",
        "def get_all_topics(subject_name: str = None) -> list:\n",
        "    \"\"\"\n",
        "    Fast path: DB already has topics â†’ pure SQL query, instant.\n",
        "    Slow path: DB empty â†’ extract from PDFs once, save, never slow again.\n",
        "    \"\"\"\n",
        "    cur = get_db().cursor()\n",
        "\n",
        "    # â”€â”€ Fast path: topics already in DB â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    if topics_in_db():\n",
        "        if subject_name:\n",
        "            subject_id = subject_name.lower().replace(\" \", \"_\")\n",
        "            cur.execute(\"\"\"\n",
        "                SELECT DISTINCT t.topic_text FROM topics t\n",
        "                WHERE t.source_pdf IN (\n",
        "                    SELECT DISTINCT pdf_filename FROM chapters\n",
        "                    WHERE subject_id = ?\n",
        "                )\n",
        "                ORDER BY t.topic_text\n",
        "            \"\"\", (subject_id,))\n",
        "        else:\n",
        "            cur.execute(\"\"\"\n",
        "                SELECT DISTINCT topic_text FROM topics\n",
        "                ORDER BY topic_text LIMIT 600\n",
        "            \"\"\")\n",
        "        return [r[\"topic_text\"] for r in cur.fetchall()]\n",
        "\n",
        "    # â”€â”€ Slow path: extract from PDFs (first time only) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    with st.spinner(\"ðŸ” Extracting topics from PDFs â€” first time only, won't repeat...\"):\n",
        "        texts, paths = load_all_texts()\n",
        "        for text, path in zip(texts, paths):\n",
        "            extracted = extract_topics_from_text(text)\n",
        "            save_topics_to_db(extracted, path)\n",
        "\n",
        "    # Now re-query with the fresh data\n",
        "    return get_all_topics(subject_name)\n",
        "\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# EMBEDDER\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "@st.cache_resource\n",
        "def load_embedder():\n",
        "    return SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "embedder = load_embedder()\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# BOOT SEQUENCE\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "_, pdf_files = download_and_extract()\n",
        "# PDF embeddings and topics load lazily:\n",
        "#   - topics: loaded when sidebar dropdown opens (DB query, instant if cached)\n",
        "#   - embeddings: loaded when first search runs (cached after that)\n",
        "\n",
        "# Session ID for anonymous history logging\n",
        "'''if \"session_id\" not in st.session_state:\n",
        "    st.session_state[\"session_id\"] = str(uuid.uuid4())'''\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# SIDEBAR\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "st.sidebar.header(\"ðŸŽ“ Select Options\")\n",
        "# Loaded from DB â€” adding a new subject/level in DB instantly shows here\n",
        "all_subjects     = get_subjects()\n",
        "selected_subject = st.sidebar.selectbox(\"Select Subject\", all_subjects)\n",
        "all_levels       = get_levels(selected_subject)\n",
        "selected_level   = st.sidebar.selectbox(\"Select Level\", all_levels)\n",
        "\n",
        "st.sidebar.markdown(\"---\")\n",
        "\n",
        "# â”€â”€ Input mode toggle â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "input_mode = st.sidebar.radio(\n",
        "    \"Search by\",\n",
        "    [\"ðŸ“ Type a query\", \"ðŸ“‹ Pick from topics\"],\n",
        "    help=\"Type your own question OR pick chapter names from the list\"\n",
        ")\n",
        "\n",
        "user_query = \"\"\n",
        "selected_topics = []\n",
        "\n",
        "if input_mode == \"ðŸ“ Type a query\":\n",
        "    user_query = st.sidebar.text_area(\n",
        "        \"Your question or keyword\",\n",
        "        placeholder=\"e.g. what is the caste system?\\nor: federalism in India\\nor: GDP and national income\",\n",
        "        height=100,\n",
        "        help=\"Type anything â€” a question, keyword, or concept\"\n",
        "    )\n",
        "    # derive topics list from query for YouTube/books (use query words)\n",
        "    selected_topics = [w for w in user_query.split() if len(w) > 3] if user_query else []\n",
        "\n",
        "else:\n",
        "    subject_topics = get_all_topics(selected_subject)\n",
        "    if not subject_topics:\n",
        "        st.sidebar.warning(\"No topics found â€” PDFs will be scanned on first search.\")\n",
        "        selected_topics = []\n",
        "    else:\n",
        "        st.sidebar.caption(f\"{len(subject_topics)} topics from {selected_subject} PDFs\")\n",
        "        selected_topics = st.sidebar.multiselect(\n",
        "            \"Select Topic(s)\",\n",
        "            subject_topics,\n",
        "            help=\"Topics extracted from NCERT PDF text\"\n",
        "        )\n",
        "        # Reset button â€” wipes DB cache and re-extracts from PDFs\n",
        "        if st.sidebar.button(\"ðŸ”„ Reset & Re-extract Topics\", help=\"Use if topics look noisy\"):\n",
        "            conn = get_db()\n",
        "            conn.execute(\"DELETE FROM topics\")\n",
        "            conn.commit()\n",
        "            st.cache_data.clear()\n",
        "            st.rerun()\n",
        "    # join selected topics into a query string for the embedder\n",
        "    user_query = \" \".join(selected_topics)\n",
        "\n",
        "# Final search query used everywhere\n",
        "search_query = user_query.strip()\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# CHAPTER RECOMMENDATIONS (embeddings)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def lookup_chapter(pdf_filename: str, zip_source: str = None) -> dict:\n",
        "    \"\"\"\n",
        "    Given a pdf_filename (and optionally zip_source for disambiguation),\n",
        "    return readable chapter details from the DB.\n",
        "    zip_source matters for Business Studies â€” Class 11 & 12 share identical filenames.\n",
        "    Falls back to raw filename if not found.\n",
        "    \"\"\"\n",
        "    cur = get_db().cursor()\n",
        "    if zip_source:\n",
        "        cur.execute(\"\"\"\n",
        "            SELECT subject_id, class, chapter_number, chapter_name, book_name, is_special\n",
        "            FROM chapters WHERE pdf_filename = ? AND zip_source = ?\n",
        "        \"\"\", (pdf_filename, zip_source))\n",
        "    else:\n",
        "        cur.execute(\"\"\"\n",
        "            SELECT subject_id, class, chapter_number, chapter_name, book_name, is_special\n",
        "            FROM chapters WHERE pdf_filename = ? LIMIT 1\n",
        "        \"\"\", (pdf_filename,))\n",
        "    row = cur.fetchone()\n",
        "    if row:\n",
        "        return dict(row)\n",
        "    return {\n",
        "        \"subject_id\": \"â€”\", \"class\": \"â€”\", \"chapter_number\": \"â€”\",\n",
        "        \"chapter_name\": pdf_filename, \"book_name\": \"â€”\", \"is_special\": 0\n",
        "    }\n",
        "\n",
        "\n",
        "@st.cache_resource(show_spinner=\"ðŸ“ Indexing PDF chapters (one-time)â€¦\")\n",
        "def get_pdf_embeddings():\n",
        "    \"\"\"\n",
        "    Encode ALL PDF texts ONCE and cache in memory.\n",
        "    Any query then just encodes a single string and does cosine similarity â€” instant.\n",
        "    \"\"\"\n",
        "    texts, paths = load_all_texts()\n",
        "    embs = embedder.encode(texts, convert_to_numpy=True, show_progress_bar=False)\n",
        "    return texts, paths, embs\n",
        "\n",
        "\n",
        "def get_chapter_recommendations(query: str, top_n=5):\n",
        "    \"\"\"\n",
        "    Search chapters using free text OR chapter name keywords.\n",
        "    e.g. \"what is the caste system\", \"GDP national income\", \"Federalism\"\n",
        "    PDF embeddings are pre-cached so each search is instant.\n",
        "    \"\"\"\n",
        "    if not query or not query.strip():\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    texts, paths, text_emb = get_pdf_embeddings()   # cached â€” no recompute\n",
        "    topic_emb = embedder.encode([query.strip()], convert_to_numpy=True)\n",
        "    sims      = cosine_similarity(topic_emb, text_emb).flatten()\n",
        "    top_idx   = sims.argsort()[-top_n:][::-1]\n",
        "\n",
        "    rows = []\n",
        "    for i in top_idx:\n",
        "        info = lookup_chapter(paths[i])\n",
        "        if info.get(\"is_special\"):\n",
        "            continue\n",
        "        rows.append({\n",
        "            \"Subject\":      info[\"subject_id\"].replace(\"_\", \" \").title(),\n",
        "            \"Class\":        f\"Class {info['class']}\",\n",
        "            \"Book\":         info[\"book_name\"],\n",
        "            \"Ch No.\":       info[\"chapter_number\"],\n",
        "            \"Chapter Name\": info[\"chapter_name\"],\n",
        "            \"Relevance\":    round(float(sims[i]), 4),\n",
        "        })\n",
        "    return pd.DataFrame(rows) if rows else pd.DataFrame()\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# BOOK & PAPER RECOMMENDATIONS (from DB)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def recommend_materials(subject: str, level: str, topics: tuple) -> pd.DataFrame:\n",
        "    if not topics:\n",
        "        return pd.DataFrame({\"Message\": [\"Please select at least one topic\"]})\n",
        "    rows = get_recommendations(subject, level)\n",
        "    if not rows:\n",
        "        return pd.DataFrame({\"Message\": [f\"No data found for {subject} / {level}\"]})\n",
        "    return pd.DataFrame([{\n",
        "        \"Type\":          r[\"level_type\"],\n",
        "        \"Title\":         r[\"title\"],\n",
        "        \"Author\":        r[\"author\"],\n",
        "        \"Why Read This\": r[\"why\"],\n",
        "        \"NCERT Chapter\": r[\"ncert_chapter_link\"] or \"â€”\",\n",
        "        \"Link\":          r[\"link\"],\n",
        "    } for r in rows])\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# YOUTUBE SEARCH URL BUILDER\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def build_youtube_url(subject: str, level: str, topics: list) -> str:\n",
        "    \"\"\"\n",
        "    Build a YouTube search URL from subject + level + selected topics.\n",
        "    Opens YouTube search results directly â€” no API key needed.\n",
        "    \"\"\"\n",
        "    import urllib.parse\n",
        "    # Compose a focused search query\n",
        "    topic_str = \" \".join(topics[:3]) if topics else \"\"\n",
        "    query = f\"NCERT {subject} {level} {topic_str} class 11 12\".strip()\n",
        "    encoded = urllib.parse.quote_plus(query)\n",
        "    return f\"https://www.youtube.com/results?search_query={encoded}\"\n",
        "\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# RENDER â€” Tabs layout\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "st.markdown(\"---\")\n",
        "\n",
        "tab1, tab2, tab3 = st.tabs([\n",
        "    \"ðŸ“„ Chapter Recommendations\",\n",
        "    \"ðŸ“š Books & Research Papers\",\n",
        "    \"â–¶ï¸ YouTube Videos\",\n",
        "])\n",
        "\n",
        "# â”€â”€ Tab 1: Chapter Recommendations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "with tab1:\n",
        "    st.markdown(\n",
        "        \"Finds the most relevant NCERT chapters based on your query. \"\n",
        "        \"Works with free text â€” *what is caste system*, *explain GDP*, anything.\"\n",
        "    )\n",
        "    if not search_query:\n",
        "        st.info(\"ðŸ‘ˆ Type a question or select chapters from the sidebar to search.\")\n",
        "    else:\n",
        "        st.markdown(f\"ðŸ”Ž **Searching for:** `{search_query}`\")\n",
        "        # Auto-search on every new query â€” no button needed\n",
        "        with st.spinner(\"Finding relevant chapters...\"):\n",
        "            df = get_chapter_recommendations(search_query)\n",
        "        if df.empty:\n",
        "            st.warning(\"No matching chapters found. Try different keywords.\")\n",
        "        else:\n",
        "            st.success(f\"âœ… Top {len(df)} chapters found!\")\n",
        "            st.dataframe(\n",
        "                df,\n",
        "                use_container_width=True,\n",
        "                hide_index=True,\n",
        "                column_config={\n",
        "                    \"Relevance\": st.column_config.ProgressColumn(\n",
        "                        \"Relevance\",\n",
        "                        min_value=0,\n",
        "                        max_value=1,\n",
        "                        format=\"%.4f\"\n",
        "                    )\n",
        "                }\n",
        "            )\n",
        "\n",
        "# â”€â”€ Tab 2: Books & Research Papers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "with tab2:\n",
        "    st.markdown(\n",
        "        f\"Showing **{selected_level}** level resources for **{selected_subject}** \"\n",
        "        f\"â€” NCERT textbooks â†’ reference books â†’ research papers.\"\n",
        "    )\n",
        "    if st.button(\"ðŸ“– Get Books & Papers\", type=\"primary\", key=\"book_btn\"):\n",
        "        '''log_user_search(\n",
        "            st.session_state[\"session_id\"],\n",
        "            selected_subject,\n",
        "            selected_level,\n",
        "            selected_topics if selected_topics else [search_query]\n",
        "        )'''\n",
        "        rows = get_recommendations(selected_subject, selected_level)\n",
        "        if not rows:\n",
        "            st.warning(f\"No recommendations found for {selected_subject} / {selected_level}\")\n",
        "        else:\n",
        "            for r in rows:\n",
        "                type_icons = {\"NCERT\": \"ðŸ“—\", \"Book\": \"ðŸ“˜\", \"Paper\": \"ðŸ“„\"}\n",
        "                icon = type_icons.get(r[\"level_type\"], \"ðŸ“Œ\")\n",
        "                with st.expander(f\"{icon} {r['title']} â€” *{r['author']}*\", expanded=False):\n",
        "                    st.markdown(f\"**Type:** {r['level_type']}\")\n",
        "                    if r[\"journal\"]:\n",
        "                        st.markdown(f\"**Journal:** {r['journal']}\")\n",
        "                    if r[\"ncert_chapter_link\"]:\n",
        "                        st.markdown(f\"**NCERT Chapter:** `{r['ncert_chapter_link']}`\")\n",
        "                    st.markdown(f\"ðŸ’¡ *{r['why']}*\")\n",
        "                    if r[\"link\"]:\n",
        "                        st.markdown(f\"[ðŸ”— Open Resource]({r['link']})\")\n",
        "\n",
        "# â”€â”€ Tab 3: YouTube Videos â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "with tab3:\n",
        "    st.markdown(\n",
        "        \"Clicking the button below will open a **YouTube search** in a new tab \"\n",
        "        \"with a query built from your selected subject, level, and topics.\"\n",
        "    )\n",
        "\n",
        "    # Show the query that will be used\n",
        "    yt_query_preview = f\"NCERT {selected_subject} {selected_level} {search_query} class 11 12\".strip()\n",
        "    st.markdown(f\"**Search query:** `{yt_query_preview}`\")\n",
        "\n",
        "    if not search_query:\n",
        "        st.info(\"ðŸ‘ˆ Type a query or select topics to refine the YouTube search.\")\n",
        "\n",
        "    # Always show the button â€” even without topics it gives subject+level results\n",
        "    yt_url = build_youtube_url(selected_subject, selected_level, [search_query])\n",
        "    st.link_button(\"â–¶ï¸ Search on YouTube\", yt_url, type=\"primary\")\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    # Also show quick-access searches for each level\n",
        "    st.markdown(\"**Or jump directly to a level:**\")\n",
        "    level_icons = {\"Beginner\": \"ðŸŒ±\", \"Intermediate\": \"ðŸ“š\", \"Advanced\": \"ðŸ”¬\"}\n",
        "    yt_levels = get_levels(selected_subject)\n",
        "    cols = st.columns(len(yt_levels))\n",
        "    for col, lvl in zip(cols, yt_levels):\n",
        "        with col:\n",
        "            icon = level_icons.get(lvl, \"â–¶ï¸\")\n",
        "            url  = build_youtube_url(selected_subject, lvl, [search_query])\n",
        "            st.link_button(f\"{icon} {lvl}\", url, use_container_width=True)\n",
        "\n",
        "'''# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# SIDEBAR â€” search history\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "with st.sidebar.expander(\"ðŸ—‚ï¸ View Search History\"):\n",
        "    cur = get_db().cursor()\n",
        "    cur.execute(\"\"\"\n",
        "        SELECT subject_id, level, selected_topics, searched_at\n",
        "        FROM user_history ORDER BY searched_at DESC LIMIT 20\n",
        "    \"\"\")\n",
        "    history = [dict(r) for r in cur.fetchall()]\n",
        "    if history:\n",
        "        st.dataframe(pd.DataFrame(history), use_container_width=True)\n",
        "    else:\n",
        "        st.info(\"No searches logged yet.\")'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfiwSXLO3A-5",
        "outputId": "f13b636a-9d02-4552-a5e2-340bb1153390",
        "cellView": "form"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "User History Removed"
      ],
      "metadata": {
        "id": "4UeVpAK2XTIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "%%writefile app.py\n",
        "import os\n",
        "import re\n",
        "import uuid\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "import streamlit as st\n",
        "import gdown\n",
        "import requests\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "from pypdf import PdfReader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# CONFIG\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "FILE_ID     = \"1iKxF0Db1YjySxbFVCfQhOJ6CXYVwdxbC\"\n",
        "ZIP_PATH    = \"ncert.zip\"\n",
        "EXTRACT_DIR = \"ncert_extracted\"\n",
        "DB_PATH     = \"/content/drive/MyDrive/ZenithIndia/AI Learning Assistant/ncert.db\"\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# PAGE CONFIG\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "st.set_page_config(page_title=\"NCERT Learning Assistant\", layout=\"wide\")\n",
        "st.title(\"ðŸ“˜ NCERT Learning Assistant\")\n",
        "st.write(\"Select topics to get chapter, book, and research paper recommendations for Class 11â€“12 students\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# MOUNT GOOGLE DRIVE\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    if not os.path.exists(\"/content/drive/MyDrive\"):\n",
        "        drive.mount(\"/content/drive\")\n",
        "except ImportError:\n",
        "    DB_PATH = \"ncert.db\"   # fallback for local runs\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# DATABASE â€” connect only, no table creation\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "@st.cache_resource\n",
        "def get_db():\n",
        "    if not os.path.exists(DB_PATH):\n",
        "        st.error(f\"âŒ Database not found at:\\n`{DB_PATH}`\\n\\nPlease run setup_db.ipynb first, or update DB_PATH at the top of app.py.\")\n",
        "        st.stop()\n",
        "    try:\n",
        "        conn = sqlite3.connect(DB_PATH, check_same_thread=False)\n",
        "        conn.row_factory = sqlite3.Row\n",
        "        # Quick sanity check â€” if file is corrupt/not a DB this will fail fast\n",
        "        conn.execute(\"SELECT 1\")\n",
        "        return conn\n",
        "    except sqlite3.DatabaseError as e:\n",
        "        st.error(f\"âŒ File exists but is not a valid SQLite database:\\n`{DB_PATH}`\\n\\nError: {e}\\n\\nPlease re-run setup_db.ipynb to recreate it.\")\n",
        "        st.stop()\n",
        "\n",
        "\n",
        "def get_subjects() -> list:\n",
        "    \"\"\"Load all subjects from DB â€” adding a new subject to DB instantly appears in app.\"\"\"\n",
        "    cur = get_db().cursor()\n",
        "    cur.execute(\"SELECT subject_name FROM subjects ORDER BY subject_name\")\n",
        "    return [row[\"subject_name\"] for row in cur.fetchall()]\n",
        "\n",
        "\n",
        "def get_levels(subject_name: str = None) -> list:\n",
        "    \"\"\"\n",
        "    Load distinct levels available for a subject from recommendations table.\n",
        "    Order: Beginner â†’ Intermediate â†’ Advanced.\n",
        "    Adding a new level to DB instantly appears in app.\n",
        "    \"\"\"\n",
        "    cur = get_db().cursor()\n",
        "    level_order = \"CASE level WHEN 'Beginner' THEN 1 WHEN 'Intermediate' THEN 2 WHEN 'Advanced' THEN 3 ELSE 4 END\"\n",
        "    if subject_name:\n",
        "        subject_id = subject_name.lower().replace(\" \", \"_\")\n",
        "        cur.execute(f\"\"\"\n",
        "            SELECT DISTINCT level FROM recommendations\n",
        "            WHERE subject_id = ?\n",
        "            ORDER BY {level_order}\n",
        "        \"\"\", (subject_id,))\n",
        "    else:\n",
        "        cur.execute(f\"\"\"\n",
        "            SELECT DISTINCT level FROM recommendations\n",
        "            ORDER BY {level_order}\n",
        "        \"\"\")\n",
        "    return [row[\"level\"] for row in cur.fetchall()]\n",
        "\n",
        "\n",
        "def get_recommendations(subject_name: str, level: str) -> list:\n",
        "    subject_id = subject_name.lower().replace(\" \", \"_\")\n",
        "    cur = get_db().cursor()\n",
        "    cur.execute(\"\"\"\n",
        "        SELECT title, author, level_type, why, link, journal, ncert_chapter_link\n",
        "        FROM recommendations\n",
        "        WHERE subject_id = ? AND level = ?\n",
        "        ORDER BY rec_id\n",
        "    \"\"\", (subject_id, level))\n",
        "    return [dict(row) for row in cur.fetchall()]\n",
        "\n",
        "\n",
        "def save_topics_to_db(topics: list, source_pdf: str) -> None:\n",
        "    conn = get_db()\n",
        "    conn.executemany(\n",
        "        \"INSERT OR IGNORE INTO topics (topic_text, source_pdf) VALUES (?, ?)\",\n",
        "        [(t, source_pdf) for t in topics]\n",
        "    )\n",
        "    conn.commit()\n",
        "\n",
        "\n",
        "def load_topics_from_db() -> list:\n",
        "    cur = get_db().cursor()\n",
        "    cur.execute(\"SELECT DISTINCT topic_text FROM topics ORDER BY topic_text LIMIT 500\")\n",
        "    return [row[\"topic_text\"] for row in cur.fetchall()]\n",
        "\n",
        "\n",
        "def load_topics_from_chapters(subject_id: str = None) -> list:\n",
        "    \"\"\"\n",
        "    Load clean topic list from the chapters table.\n",
        "    These are real NCERT chapter names â€” no noise.\n",
        "    Optionally filter by subject.\n",
        "    \"\"\"\n",
        "    cur = get_db().cursor()\n",
        "    if subject_id:\n",
        "        cur.execute(\"\"\"\n",
        "            SELECT DISTINCT chapter_name FROM chapters\n",
        "            WHERE is_special = 0 AND subject_id = ?\n",
        "            ORDER BY chapter_name\n",
        "        \"\"\", (subject_id,))\n",
        "    else:\n",
        "        cur.execute(\"\"\"\n",
        "            SELECT DISTINCT chapter_name FROM chapters\n",
        "            WHERE is_special = 0\n",
        "            ORDER BY chapter_name\n",
        "        \"\"\")\n",
        "    return [row[\"chapter_name\"] for row in cur.fetchall()]\n",
        "\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# DOWNLOAD & EXTRACT NCERT PDFs\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def extract_all_zips(folder: str) -> None:\n",
        "    found_new = True\n",
        "    while found_new:\n",
        "        found_new = False\n",
        "        for root, _, files in os.walk(folder):\n",
        "            for file in files:\n",
        "                if not file.lower().endswith(\".zip\"):\n",
        "                    continue\n",
        "                zip_path   = os.path.join(root, file)\n",
        "                extract_to = os.path.join(root, os.path.splitext(file)[0])\n",
        "                if os.path.exists(extract_to):\n",
        "                    continue\n",
        "                try:\n",
        "                    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "                        zf.extractall(extract_to)\n",
        "                    found_new = True\n",
        "                except zipfile.BadZipFile:\n",
        "                    st.warning(f\"âš ï¸ Skipping corrupt ZIP: {file}\")\n",
        "                except Exception as e:\n",
        "                    st.warning(f\"âš ï¸ Could not extract {file}: {e}\")\n",
        "\n",
        "\n",
        "@st.cache_data\n",
        "def download_and_extract():\n",
        "    if not os.path.exists(ZIP_PATH):\n",
        "        with st.spinner(\"â¬‡ï¸ Downloading NCERT datasetâ€¦\"):\n",
        "            downloaded = False\n",
        "            try:\n",
        "                gdown.download(id=FILE_ID, output=ZIP_PATH, quiet=False, fuzzy=True)\n",
        "                downloaded = os.path.exists(ZIP_PATH) and os.path.getsize(ZIP_PATH) > 10_000\n",
        "            except Exception as e:\n",
        "                st.warning(f\"gdown failed: {e}. Trying fallbackâ€¦\")\n",
        "\n",
        "            if not downloaded:\n",
        "                try:\n",
        "                    session  = requests.Session()\n",
        "                    URL      = \"https://drive.google.com/uc?export=download\"\n",
        "                    response = session.get(URL, params={\"id\": FILE_ID}, stream=True)\n",
        "                    token    = next(\n",
        "                        (v for k, v in response.cookies.items()\n",
        "                         if k.startswith(\"download_warning\")), None\n",
        "                    )\n",
        "                    if token:\n",
        "                        response = session.get(URL, params={\"id\": FILE_ID, \"confirm\": token}, stream=True)\n",
        "                    else:\n",
        "                        response = session.get(\n",
        "                            f\"https://drive.google.com/uc?id={FILE_ID}&export=download&confirm=t\",\n",
        "                            stream=True,\n",
        "                        )\n",
        "                    with open(ZIP_PATH, \"wb\") as f:\n",
        "                        for chunk in response.iter_content(chunk_size=32_768):\n",
        "                            if chunk:\n",
        "                                f.write(chunk)\n",
        "                    if not os.path.exists(ZIP_PATH) or os.path.getsize(ZIP_PATH) < 10_000:\n",
        "                        raise ValueError(\"Downloaded file too small.\")\n",
        "                except Exception as e2:\n",
        "                    st.error(f\"âŒ Both download methods failed: {e2}\")\n",
        "                    st.stop()\n",
        "\n",
        "    if not zipfile.is_zipfile(ZIP_PATH):\n",
        "        os.remove(ZIP_PATH)\n",
        "        st.error(\"âŒ Downloaded file is not a valid ZIP.\")\n",
        "        st.stop()\n",
        "\n",
        "    if not os.path.exists(EXTRACT_DIR):\n",
        "        with st.spinner(\"ðŸ“¦ Extracting outer ZIPâ€¦\"):\n",
        "            with zipfile.ZipFile(ZIP_PATH, \"r\") as zf:\n",
        "                zf.extractall(EXTRACT_DIR)\n",
        "\n",
        "    with st.spinner(\"ðŸ“‚ Extracting subject/chapter ZIPsâ€¦\"):\n",
        "        extract_all_zips(EXTRACT_DIR)\n",
        "\n",
        "    pdf_files = [\n",
        "        os.path.join(root, f)\n",
        "        for root, _, files in os.walk(EXTRACT_DIR)\n",
        "        for f in files if f.lower().endswith(\".pdf\")\n",
        "    ]\n",
        "    st.info(f\"ðŸ“„ PDFs found: {len(pdf_files)}\")\n",
        "    return EXTRACT_DIR, pdf_files\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# PDF UTILITIES\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def read_pdf(path: str) -> str:\n",
        "    try:\n",
        "        reader = PdfReader(path)\n",
        "        return \" \".join(p.extract_text() or \"\" for p in reader.pages)\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    text = re.sub(r\"(activity|let us|exercise|project|editor|reprint|copyright|isbn).*\", \" \", text, flags=re.I)\n",
        "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "\n",
        "@st.cache_data(show_spinner=\"ðŸ“– Reading PDFsâ€¦\")\n",
        "def load_all_texts():\n",
        "    texts, paths = [], []\n",
        "    for pdf in Path(EXTRACT_DIR).rglob(\"*.pdf\"):\n",
        "        t = clean_text(read_pdf(str(pdf)))\n",
        "        if len(t.split()) > 50:\n",
        "            texts.append(t)\n",
        "            paths.append(str(pdf.name))\n",
        "    return texts, paths\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# TOPICS â€” extracted from PDFs, cached in DB\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "# â”€â”€ Noise patterns to reject â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Bibliography entries: \"SHARMA, R. 2005. Society...\"\n",
        "_AUTHOR_ENTRY   = re.compile(r\"^[A-Z]{2,},\\s+[A-Z]\")\n",
        "# Any line with a year like \"1970.\" or \"(2005)\"\n",
        "_HAS_YEAR       = re.compile(r\"\b(19|20)\\d{2}\b\")\n",
        "# Publisher / place names\n",
        "_PUBLISHER      = re.compile(\n",
        "    r\"\b(press|publishing|publishers|edition|reprint|oxford|cambridge|mcgraw|\"\n",
        "    r\"pearson|routledge|sage|wiley|elsevier|springer|penguin|harper|norton|\"\n",
        "    r\"new york|new delhi|london|chicago|boston|mumbai|kolkata|chennai|\"\n",
        "    r\"pp\\.|vol\\.|ibid|op\\.cit|et al|isbn|doi|http|www\\.)\b\",\n",
        "    re.I\n",
        ")\n",
        "# Lines that start with noise words\n",
        "_NOISE_START    = re.compile(\n",
        "    r\"^(the|a|an|this|that|these|it|its|in|on|at|by|for|of|to|and|or|but|\"\n",
        "    r\"such|also|thus|hence|therefore|however|moreover|furthermore|although|\"\n",
        "    r\"figure|table|box|note|source|see|pg|pp|ref|ncert|reprint|let us|\"\n",
        "    r\"activity|exercise|project|chapter|unit|section)\b\",\n",
        "    re.I\n",
        ")\n",
        "# Must look like a proper heading: starts uppercase, only letters/spaces/hyphens\n",
        "_VALID_HEADING  = re.compile(r\"^[A-Z][a-zA-Z\\s\\-:']{3,55}$\")\n",
        "\n",
        "\n",
        "def extract_topics_from_text(text: str) -> list:\n",
        "    \"\"\"\n",
        "    Strict extractor â€” only picks real section/chapter headings from PDF text.\n",
        "\n",
        "    Rules:\n",
        "    - 2 to 6 words only\n",
        "    - Must start with uppercase\n",
        "    - Must be mostly title-cased (â‰¥70% words capitalised)\n",
        "    - No years, no author surname patterns, no publisher names\n",
        "    - No punctuation endings, no digits\n",
        "    - Rejects known noise start words\n",
        "    \"\"\"\n",
        "    lines = text.split(\"\\n\")\n",
        "    topics = set()\n",
        "\n",
        "    for raw_line in lines:\n",
        "        line = raw_line.strip()\n",
        "\n",
        "        # â”€â”€ Basic length filter â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        words = line.split()\n",
        "        if not (2 <= len(words) <= 6):\n",
        "            continue\n",
        "\n",
        "        # â”€â”€ Hard rejections â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        if _AUTHOR_ENTRY.match(line):      # \"SHARMA, R. 2005...\"\n",
        "            continue\n",
        "        if _HAS_YEAR.search(line):         # anything with 1970, 2005 etc\n",
        "            continue\n",
        "        if _PUBLISHER.search(line):        # publisher / place names\n",
        "            continue\n",
        "        if _NOISE_START.match(line):       # starts with filler word\n",
        "            continue\n",
        "        if not _VALID_HEADING.match(line): # must match heading pattern\n",
        "            continue\n",
        "\n",
        "        # â”€â”€ Endings filter â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        if line[-1] in \".,;:\":\n",
        "            continue\n",
        "        last_word = words[-1].lower()\n",
        "        if last_word in (\"and\", \"or\", \"the\", \"of\", \"a\", \"an\", \"in\", \"on\"):\n",
        "            continue\n",
        "\n",
        "        # â”€â”€ Digits filter â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        if any(c.isdigit() for c in line):\n",
        "            continue\n",
        "\n",
        "        # â”€â”€ Title-case check â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        # At least 70% of words must start with uppercase\n",
        "        titled = sum(1 for w in words if w and w[0].isupper())\n",
        "        if titled / len(words) < 0.70:\n",
        "            continue\n",
        "\n",
        "        topics.add(line)\n",
        "\n",
        "    return list(topics)\n",
        "\n",
        "\n",
        "def topics_in_db() -> bool:\n",
        "    \"\"\"Quick check â€” are any topics already saved in the DB?\"\"\"\n",
        "    cur = get_db().cursor()\n",
        "    cur.execute(\"SELECT COUNT(*) as n FROM topics\")\n",
        "    return cur.fetchone()[\"n\"] > 0\n",
        "\n",
        "\n",
        "@st.cache_data(show_spinner=False)\n",
        "def get_all_topics(subject_name: str = None) -> list:\n",
        "    \"\"\"\n",
        "    Fast path: DB already has topics â†’ pure SQL query, instant.\n",
        "    Slow path: DB empty â†’ extract from PDFs once, save, never slow again.\n",
        "    \"\"\"\n",
        "    cur = get_db().cursor()\n",
        "\n",
        "    # â”€â”€ Fast path: topics already in DB â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    if topics_in_db():\n",
        "        if subject_name:\n",
        "            subject_id = subject_name.lower().replace(\" \", \"_\")\n",
        "            cur.execute(\"\"\"\n",
        "                SELECT DISTINCT t.topic_text FROM topics t\n",
        "                WHERE t.source_pdf IN (\n",
        "                    SELECT DISTINCT pdf_filename FROM chapters\n",
        "                    WHERE subject_id = ?\n",
        "                )\n",
        "                ORDER BY t.topic_text\n",
        "            \"\"\", (subject_id,))\n",
        "        else:\n",
        "            cur.execute(\"\"\"\n",
        "                SELECT DISTINCT topic_text FROM topics\n",
        "                ORDER BY topic_text LIMIT 600\n",
        "            \"\"\")\n",
        "        return [r[\"topic_text\"] for r in cur.fetchall()]\n",
        "\n",
        "    # â”€â”€ Slow path: extract from PDFs (first time only) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    with st.spinner(\"ðŸ” Extracting topics from PDFs â€” first time only, won't repeat...\"):\n",
        "        texts, paths = load_all_texts()\n",
        "        for text, path in zip(texts, paths):\n",
        "            extracted = extract_topics_from_text(text)\n",
        "            save_topics_to_db(extracted, path)\n",
        "\n",
        "    # Now re-query with the fresh data\n",
        "    return get_all_topics(subject_name)\n",
        "\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# EMBEDDER\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "@st.cache_resource\n",
        "def load_embedder():\n",
        "    return SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "embedder = load_embedder()\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# BOOT SEQUENCE\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "_, pdf_files = download_and_extract()\n",
        "# PDF embeddings and topics load lazily:\n",
        "#   - topics: loaded when sidebar dropdown opens (DB query, instant if cached)\n",
        "#   - embeddings: loaded when first search runs (cached after that)\n",
        "\n",
        "# Session ID for anonymous history logging\n",
        "\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# SIDEBAR\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "st.sidebar.header(\"ðŸŽ“ Select Options\")\n",
        "# Loaded from DB â€” adding a new subject/level in DB instantly shows here\n",
        "all_subjects     = get_subjects()\n",
        "selected_subject = st.sidebar.selectbox(\"Select Subject\", all_subjects)\n",
        "all_levels       = get_levels(selected_subject)\n",
        "selected_level   = st.sidebar.selectbox(\"Select Level\", all_levels)\n",
        "\n",
        "st.sidebar.markdown(\"---\")\n",
        "\n",
        "# â”€â”€ Input mode toggle â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "input_mode = st.sidebar.radio(\n",
        "    \"Search by\",\n",
        "    [\"ðŸ“ Type a query\", \"ðŸ“‹ Pick from topics\"],\n",
        "    help=\"Type your own question OR pick chapter names from the list\"\n",
        ")\n",
        "\n",
        "user_query = \"\"\n",
        "selected_topics = []\n",
        "\n",
        "if input_mode == \"ðŸ“ Type a query\":\n",
        "    user_query = st.sidebar.text_area(\n",
        "        \"Your question or keyword\",\n",
        "        placeholder=\"e.g. what is the caste system?\\nor: federalism in India\\nor: GDP and national income\",\n",
        "        height=100,\n",
        "        help=\"Type anything â€” a question, keyword, or concept\"\n",
        "    )\n",
        "    # derive topics list from query for YouTube/books (use query words)\n",
        "    selected_topics = [w for w in user_query.split() if len(w) > 3] if user_query else []\n",
        "\n",
        "else:\n",
        "    subject_topics = get_all_topics(selected_subject)\n",
        "    if not subject_topics:\n",
        "        st.sidebar.warning(\"No topics found â€” PDFs will be scanned on first search.\")\n",
        "        selected_topics = []\n",
        "    else:\n",
        "        st.sidebar.caption(f\"{len(subject_topics)} topics from {selected_subject} PDFs\")\n",
        "        selected_topics = st.sidebar.multiselect(\n",
        "            \"Select Topic(s)\",\n",
        "            subject_topics,\n",
        "            help=\"Topics extracted from NCERT PDF text\"\n",
        "        )\n",
        "        # Reset button â€” wipes DB cache and re-extracts from PDFs\n",
        "        if st.sidebar.button(\"ðŸ”„ Reset & Re-extract Topics\", help=\"Use if topics look noisy\"):\n",
        "            conn = get_db()\n",
        "            conn.execute(\"DELETE FROM topics\")\n",
        "            conn.commit()\n",
        "            st.cache_data.clear()\n",
        "            st.rerun()\n",
        "    # join selected topics into a query string for the embedder\n",
        "    user_query = \" \".join(selected_topics)\n",
        "\n",
        "# Final search query used everywhere\n",
        "search_query = user_query.strip()\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# CHAPTER RECOMMENDATIONS (embeddings)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def lookup_chapter(pdf_filename: str, zip_source: str = None) -> dict:\n",
        "    \"\"\"\n",
        "    Given a pdf_filename (and optionally zip_source for disambiguation),\n",
        "    return readable chapter details from the DB.\n",
        "    zip_source matters for Business Studies â€” Class 11 & 12 share identical filenames.\n",
        "    Falls back to raw filename if not found.\n",
        "    \"\"\"\n",
        "    cur = get_db().cursor()\n",
        "    if zip_source:\n",
        "        cur.execute(\"\"\"\n",
        "            SELECT subject_id, class, chapter_number, chapter_name, book_name, is_special\n",
        "            FROM chapters WHERE pdf_filename = ? AND zip_source = ?\n",
        "        \"\"\", (pdf_filename, zip_source))\n",
        "    else:\n",
        "        cur.execute(\"\"\"\n",
        "            SELECT subject_id, class, chapter_number, chapter_name, book_name, is_special\n",
        "            FROM chapters WHERE pdf_filename = ? LIMIT 1\n",
        "        \"\"\", (pdf_filename,))\n",
        "    row = cur.fetchone()\n",
        "    if row:\n",
        "        return dict(row)\n",
        "    return {\n",
        "        \"subject_id\": \"â€”\", \"class\": \"â€”\", \"chapter_number\": \"â€”\",\n",
        "        \"chapter_name\": pdf_filename, \"book_name\": \"â€”\", \"is_special\": 0\n",
        "    }\n",
        "\n",
        "\n",
        "@st.cache_resource(show_spinner=\"ðŸ“ Indexing PDF chapters (one-time)â€¦\")\n",
        "def get_pdf_embeddings():\n",
        "    \"\"\"\n",
        "    Encode ALL PDF texts ONCE and cache in memory.\n",
        "    Any query then just encodes a single string and does cosine similarity â€” instant.\n",
        "    \"\"\"\n",
        "    texts, paths = load_all_texts()\n",
        "    embs = embedder.encode(texts, convert_to_numpy=True, show_progress_bar=False)\n",
        "    return texts, paths, embs\n",
        "\n",
        "\n",
        "def get_chapter_recommendations(query: str, top_n=5):\n",
        "    \"\"\"\n",
        "    Search chapters using free text OR chapter name keywords.\n",
        "    e.g. \"what is the caste system\", \"GDP national income\", \"Federalism\"\n",
        "    PDF embeddings are pre-cached so each search is instant.\n",
        "    \"\"\"\n",
        "    if not query or not query.strip():\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    texts, paths, text_emb = get_pdf_embeddings()   # cached â€” no recompute\n",
        "    topic_emb = embedder.encode([query.strip()], convert_to_numpy=True)\n",
        "    sims      = cosine_similarity(topic_emb, text_emb).flatten()\n",
        "    top_idx   = sims.argsort()[-top_n:][::-1]\n",
        "\n",
        "    rows = []\n",
        "    for i in top_idx:\n",
        "        info = lookup_chapter(paths[i])\n",
        "        if info.get(\"is_special\"):\n",
        "            continue\n",
        "        rows.append({\n",
        "            \"Subject\":      info[\"subject_id\"].replace(\"_\", \" \").title(),\n",
        "            \"Class\":        f\"Class {info['class']}\",\n",
        "            \"Book\":         info[\"book_name\"],\n",
        "            \"Ch No.\":       info[\"chapter_number\"],\n",
        "            \"Chapter Name\": info[\"chapter_name\"],\n",
        "            \"Relevance\":    round(float(sims[i]), 4),\n",
        "        })\n",
        "    return pd.DataFrame(rows) if rows else pd.DataFrame()\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# BOOK & PAPER RECOMMENDATIONS (from DB)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def recommend_materials(subject: str, level: str, topics: tuple) -> pd.DataFrame:\n",
        "    if not topics:\n",
        "        return pd.DataFrame({\"Message\": [\"Please select at least one topic\"]})\n",
        "    rows = get_recommendations(subject, level)\n",
        "    if not rows:\n",
        "        return pd.DataFrame({\"Message\": [f\"No data found for {subject} / {level}\"]})\n",
        "    return pd.DataFrame([{\n",
        "        \"Type\":          r[\"level_type\"],\n",
        "        \"Title\":         r[\"title\"],\n",
        "        \"Author\":        r[\"author\"],\n",
        "        \"Why Read This\": r[\"why\"],\n",
        "        \"NCERT Chapter\": r[\"ncert_chapter_link\"] or \"â€”\",\n",
        "        \"Link\":          r[\"link\"],\n",
        "    } for r in rows])\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# YOUTUBE SEARCH URL BUILDER\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def build_youtube_url(subject: str, level: str, topics: list) -> str:\n",
        "    \"\"\"\n",
        "    Build a YouTube search URL from subject + level + selected topics.\n",
        "    Opens YouTube search results directly â€” no API key needed.\n",
        "    \"\"\"\n",
        "    import urllib.parse\n",
        "    # Compose a focused search query\n",
        "    topic_str = \" \".join(topics[:3]) if topics else \"\"\n",
        "    query = f\"NCERT {subject} {level} {topic_str} class 11 12\".strip()\n",
        "    encoded = urllib.parse.quote_plus(query)\n",
        "    return f\"https://www.youtube.com/results?search_query={encoded}\"\n",
        "\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# RENDER â€” Tabs layout\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "st.markdown(\"---\")\n",
        "\n",
        "tab1, tab2, tab3 = st.tabs([\n",
        "    \"ðŸ“„ Chapter Recommendations\",\n",
        "    \"ðŸ“š Books & Research Papers\",\n",
        "    \"â–¶ï¸ YouTube Videos\",\n",
        "])\n",
        "\n",
        "# â”€â”€ Tab 1: Chapter Recommendations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "with tab1:\n",
        "    st.markdown(\n",
        "        \"Finds the most relevant NCERT chapters based on your query. \"\n",
        "        \"Works with free text â€” *what is caste system*, *explain GDP*, anything.\"\n",
        "    )\n",
        "    if not search_query:\n",
        "        st.info(\"ðŸ‘ˆ Type a question or select chapters from the sidebar to search.\")\n",
        "    else:\n",
        "        st.markdown(f\"ðŸ”Ž **Searching for:** `{search_query}`\")\n",
        "        # Auto-search on every new query â€” no button needed\n",
        "        with st.spinner(\"Finding relevant chapters...\"):\n",
        "            df = get_chapter_recommendations(search_query)\n",
        "        if df.empty:\n",
        "            st.warning(\"No matching chapters found. Try different keywords.\")\n",
        "        else:\n",
        "            st.success(f\"âœ… Top {len(df)} chapters found!\")\n",
        "            st.dataframe(\n",
        "                df,\n",
        "                use_container_width=True,\n",
        "                hide_index=True,\n",
        "                column_config={\n",
        "                    \"Relevance\": st.column_config.ProgressColumn(\n",
        "                        \"Relevance\",\n",
        "                        min_value=0,\n",
        "                        max_value=1,\n",
        "                        format=\"%.4f\"\n",
        "                    )\n",
        "                }\n",
        "            )\n",
        "\n",
        "# â”€â”€ Tab 2: Books & Research Papers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "with tab2:\n",
        "    st.markdown(\n",
        "        f\"Showing **{selected_level}** level resources for **{selected_subject}** \"\n",
        "        f\"â€” NCERT textbooks â†’ reference books â†’ research papers.\"\n",
        "    )\n",
        "    if st.button(\"ðŸ“– Get Books & Papers\", type=\"primary\", key=\"book_btn\"):\n",
        "\n",
        "        rows = get_recommendations(selected_subject, selected_level)\n",
        "        if not rows:\n",
        "            st.warning(f\"No recommendations found for {selected_subject} / {selected_level}\")\n",
        "        else:\n",
        "            for r in rows:\n",
        "                type_icons = {\"NCERT\": \"ðŸ“—\", \"Book\": \"ðŸ“˜\", \"Paper\": \"ðŸ“„\"}\n",
        "                icon = type_icons.get(r[\"level_type\"], \"ðŸ“Œ\")\n",
        "                with st.expander(f\"{icon} {r['title']} â€” *{r['author']}*\", expanded=False):\n",
        "                    st.markdown(f\"**Type:** {r['level_type']}\")\n",
        "                    if r[\"journal\"]:\n",
        "                        st.markdown(f\"**Journal:** {r['journal']}\")\n",
        "                    if r[\"ncert_chapter_link\"]:\n",
        "                        st.markdown(f\"**NCERT Chapter:** `{r['ncert_chapter_link']}`\")\n",
        "                    st.markdown(f\"ðŸ’¡ *{r['why']}*\")\n",
        "                    if r[\"link\"]:\n",
        "                        st.markdown(f\"[ðŸ”— Open Resource]({r['link']})\")\n",
        "\n",
        "# â”€â”€ Tab 3: YouTube Videos â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "with tab3:\n",
        "    st.markdown(\n",
        "        \"Clicking the button below will open a **YouTube search** in a new tab \"\n",
        "        \"with a query built from your selected subject, level, and topics.\"\n",
        "    )\n",
        "\n",
        "    # Show the query that will be used\n",
        "    yt_query_preview = f\"NCERT {selected_subject} {selected_level} {search_query} class 11 12\".strip()\n",
        "    st.markdown(f\"**Search query:** `{yt_query_preview}`\")\n",
        "\n",
        "    if not search_query:\n",
        "        st.info(\"ðŸ‘ˆ Type a query or select topics to refine the YouTube search.\")\n",
        "\n",
        "    # Always show the button â€” even without topics it gives subject+level results\n",
        "    yt_url = build_youtube_url(selected_subject, selected_level, [search_query])\n",
        "    st.link_button(\"â–¶ï¸ Search on YouTube\", yt_url, type=\"primary\")\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    # Also show quick-access searches for each level\n",
        "    st.markdown(\"**Or jump directly to a level:**\")\n",
        "    level_icons = {\"Beginner\": \"ðŸŒ±\", \"Intermediate\": \"ðŸ“š\", \"Advanced\": \"ðŸ”¬\"}\n",
        "    yt_levels = get_levels(selected_subject)\n",
        "    cols = st.columns(len(yt_levels))\n",
        "    for col, lvl in zip(cols, yt_levels):\n",
        "        with col:\n",
        "            icon = level_icons.get(lvl, \"â–¶ï¸\")\n",
        "            url  = build_youtube_url(selected_subject, lvl, [search_query])\n",
        "            st.link_button(f\"{icon} {lvl}\", url, use_container_width=True)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "dIOCXNSnWtjK",
        "outputId": "8f006b5e-9a2f-4fcb-8569-50ddb0ebf252"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ADDED EMBEDDINGS TO THE DB"
      ],
      "metadata": {
        "id": "jIgfe0btexh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "%%writefile app.py\n",
        "import os\n",
        "import re\n",
        "import uuid\n",
        "import hashlib\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import streamlit as st\n",
        "import gdown\n",
        "import requests\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "from pypdf import PdfReader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# CONFIG\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "FILE_ID    = \"1iKxF0Db1YjySxbFVCfQhOJ6CXYVwdxbC\"\n",
        "ZIP_PATH   = \"ncert.zip\"\n",
        "EXTRACT_DIR = \"ncert_extracted\"\n",
        "DB_PATH    = \"/content/drive/MyDrive/ZenithIndia/AI Learning Assistant/ncert.db\"\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# PAGE CONFIG\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "st.set_page_config(page_title=\"NCERT Learning Assistant\", layout=\"wide\")\n",
        "st.title(\"ðŸ“˜ NCERT Learning Assistant\")\n",
        "st.write(\"Select topics to get chapter, book, and research paper recommendations for Class 11â€“12 students\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# MOUNT GOOGLE DRIVE\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    if not os.path.exists(\"/content/drive/MyDrive\"):\n",
        "        drive.mount(\"/content/drive\")\n",
        "except ImportError:\n",
        "    DB_PATH = \"ncert.db\"   # fallback for local runs\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# DATABASE â€” connect only, no table creation\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "@st.cache_resource\n",
        "def get_db():\n",
        "    if not os.path.exists(DB_PATH):\n",
        "        st.error(\n",
        "            f\"âŒ Database not found at:\\n`{DB_PATH}`\\n\\n\"\n",
        "            \"Please run setup_db.ipynb first, or update DB_PATH at the top of app.py.\"\n",
        "        )\n",
        "        st.stop()\n",
        "    try:\n",
        "        conn = sqlite3.connect(DB_PATH, check_same_thread=False)\n",
        "        conn.row_factory = sqlite3.Row\n",
        "        # Sanity check â€” if file is corrupt/not a DB this will raise immediately\n",
        "        conn.execute(\"SELECT 1\")\n",
        "        # Ensure the embeddings table exists (safe to run every time â€” no-op if already there)\n",
        "        conn.execute(\"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS pdf_embeddings (\n",
        "                pdf_filename  TEXT PRIMARY KEY,\n",
        "                embedding     BLOB NOT NULL,\n",
        "                text_hash     TEXT NOT NULL,\n",
        "                created_at    TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
        "            )\n",
        "        \"\"\")\n",
        "        conn.commit()\n",
        "        return conn\n",
        "    except sqlite3.DatabaseError as e:\n",
        "        st.error(\n",
        "            f\"âŒ File exists but is not a valid SQLite database:\\n`{DB_PATH}`\\n\\n\"\n",
        "            f\"Error: {e}\\n\\nPlease re-run setup_db.ipynb to recreate it.\"\n",
        "        )\n",
        "        st.stop()\n",
        "\n",
        "\n",
        "# â”€â”€ Subject / Level helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def get_subjects() -> list:\n",
        "    cur = get_db().cursor()\n",
        "    cur.execute(\"SELECT subject_name FROM subjects ORDER BY subject_name\")\n",
        "    return [row[\"subject_name\"] for row in cur.fetchall()]\n",
        "\n",
        "\n",
        "def get_levels(subject_name: str = None) -> list:\n",
        "    cur = get_db().cursor()\n",
        "    level_order = (\n",
        "        \"CASE level WHEN 'Beginner' THEN 1 \"\n",
        "        \"WHEN 'Intermediate' THEN 2 WHEN 'Advanced' THEN 3 ELSE 4 END\"\n",
        "    )\n",
        "    if subject_name:\n",
        "        subject_id = subject_name.lower().replace(\" \", \"_\")\n",
        "        cur.execute(\n",
        "            f\"SELECT DISTINCT level FROM recommendations \"\n",
        "            f\"WHERE subject_id = ? ORDER BY {level_order}\",\n",
        "            (subject_id,)\n",
        "        )\n",
        "    else:\n",
        "        cur.execute(f\"SELECT DISTINCT level FROM recommendations ORDER BY {level_order}\")\n",
        "    return [row[\"level\"] for row in cur.fetchall()]\n",
        "\n",
        "\n",
        "def get_recommendations(subject_name: str, level: str) -> list:\n",
        "    subject_id = subject_name.lower().replace(\" \", \"_\")\n",
        "    cur = get_db().cursor()\n",
        "    cur.execute(\"\"\"\n",
        "        SELECT title, author, level_type, why, link, journal, ncert_chapter_link\n",
        "        FROM recommendations\n",
        "        WHERE subject_id = ? AND level = ?\n",
        "        ORDER BY rec_id\n",
        "    \"\"\", (subject_id, level))\n",
        "    return [dict(row) for row in cur.fetchall()]\n",
        "\n",
        "\n",
        "def save_topics_to_db(topics: list, source_pdf: str) -> None:\n",
        "    conn = get_db()\n",
        "    conn.executemany(\n",
        "        \"INSERT OR IGNORE INTO topics (topic_text, source_pdf) VALUES (?, ?)\",\n",
        "        [(t, source_pdf) for t in topics]\n",
        "    )\n",
        "    conn.commit()\n",
        "\n",
        "\n",
        "def load_topics_from_db() -> list:\n",
        "    cur = get_db().cursor()\n",
        "    cur.execute(\"SELECT DISTINCT topic_text FROM topics ORDER BY topic_text LIMIT 500\")\n",
        "    return [row[\"topic_text\"] for row in cur.fetchall()]\n",
        "\n",
        "\n",
        "def load_topics_from_chapters(subject_id: str = None) -> list:\n",
        "    cur = get_db().cursor()\n",
        "    if subject_id:\n",
        "        cur.execute(\"\"\"\n",
        "            SELECT DISTINCT chapter_name FROM chapters\n",
        "            WHERE is_special = 0 AND subject_id = ?\n",
        "            ORDER BY chapter_name\n",
        "        \"\"\", (subject_id,))\n",
        "    else:\n",
        "        cur.execute(\"\"\"\n",
        "            SELECT DISTINCT chapter_name FROM chapters\n",
        "            WHERE is_special = 0\n",
        "            ORDER BY chapter_name\n",
        "        \"\"\")\n",
        "    return [row[\"chapter_name\"] for row in cur.fetchall()]\n",
        "\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# DOWNLOAD & EXTRACT NCERT PDFs\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def extract_all_zips(folder: str) -> None:\n",
        "    found_new = True\n",
        "    while found_new:\n",
        "        found_new = False\n",
        "        for root, _, files in os.walk(folder):\n",
        "            for file in files:\n",
        "                if not file.lower().endswith(\".zip\"):\n",
        "                    continue\n",
        "                zip_path   = os.path.join(root, file)\n",
        "                extract_to = os.path.join(root, os.path.splitext(file)[0])\n",
        "                if os.path.exists(extract_to):\n",
        "                    continue\n",
        "                try:\n",
        "                    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "                        zf.extractall(extract_to)\n",
        "                    found_new = True\n",
        "                except zipfile.BadZipFile:\n",
        "                    st.warning(f\"âš ï¸ Skipping corrupt ZIP: {file}\")\n",
        "                except Exception as e:\n",
        "                    st.warning(f\"âš ï¸ Could not extract {file}: {e}\")\n",
        "\n",
        "\n",
        "@st.cache_data\n",
        "def download_and_extract():\n",
        "    if not os.path.exists(ZIP_PATH):\n",
        "        with st.spinner(\"â¬‡ï¸ Downloading NCERT datasetâ€¦\"):\n",
        "            downloaded = False\n",
        "            try:\n",
        "                gdown.download(id=FILE_ID, output=ZIP_PATH, quiet=False, fuzzy=True)\n",
        "                downloaded = os.path.exists(ZIP_PATH) and os.path.getsize(ZIP_PATH) > 10_000\n",
        "            except Exception as e:\n",
        "                st.warning(f\"gdown failed: {e}. Trying fallbackâ€¦\")\n",
        "\n",
        "            if not downloaded:\n",
        "                try:\n",
        "                    session  = requests.Session()\n",
        "                    URL      = \"https://drive.google.com/uc?export=download\"\n",
        "                    response = session.get(URL, params={\"id\": FILE_ID}, stream=True)\n",
        "                    token    = next(\n",
        "                        (v for k, v in response.cookies.items() if k.startswith(\"download_warning\")),\n",
        "                        None\n",
        "                    )\n",
        "                    if token:\n",
        "                        response = session.get(URL, params={\"id\": FILE_ID, \"confirm\": token}, stream=True)\n",
        "                    else:\n",
        "                        response = session.get(\n",
        "                            f\"https://drive.google.com/uc?id={FILE_ID}&export=download&confirm=t\",\n",
        "                            stream=True,\n",
        "                        )\n",
        "                    with open(ZIP_PATH, \"wb\") as f:\n",
        "                        for chunk in response.iter_content(chunk_size=32_768):\n",
        "                            if chunk:\n",
        "                                f.write(chunk)\n",
        "                    if not os.path.exists(ZIP_PATH) or os.path.getsize(ZIP_PATH) < 10_000:\n",
        "                        raise ValueError(\"Downloaded file too small.\")\n",
        "                except Exception as e2:\n",
        "                    st.error(f\"âŒ Both download methods failed: {e2}\")\n",
        "                    st.stop()\n",
        "\n",
        "        if not zipfile.is_zipfile(ZIP_PATH):\n",
        "            os.remove(ZIP_PATH)\n",
        "            st.error(\"âŒ Downloaded file is not a valid ZIP.\")\n",
        "            st.stop()\n",
        "\n",
        "    if not os.path.exists(EXTRACT_DIR):\n",
        "        with st.spinner(\"ðŸ“¦ Extracting outer ZIPâ€¦\"):\n",
        "            with zipfile.ZipFile(ZIP_PATH, \"r\") as zf:\n",
        "                zf.extractall(EXTRACT_DIR)\n",
        "        with st.spinner(\"ðŸ“‚ Extracting subject/chapter ZIPsâ€¦\"):\n",
        "            extract_all_zips(EXTRACT_DIR)\n",
        "\n",
        "    pdf_files = [\n",
        "        os.path.join(root, f)\n",
        "        for root, _, files in os.walk(EXTRACT_DIR)\n",
        "        for f in files\n",
        "        if f.lower().endswith(\".pdf\")\n",
        "    ]\n",
        "    st.info(f\"ðŸ“„ PDFs found: {len(pdf_files)}\")\n",
        "    return EXTRACT_DIR, pdf_files\n",
        "\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# PDF UTILITIES\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def read_pdf(path: str) -> str:\n",
        "    try:\n",
        "        reader = PdfReader(path)\n",
        "        return \" \".join(p.extract_text() or \"\" for p in reader.pages)\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    text = re.sub(\n",
        "        r\"(activity|let us|exercise|project|editor|reprint|copyright|isbn).*\",\n",
        "        \" \", text, flags=re.I\n",
        "    )\n",
        "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "\n",
        "@st.cache_data(show_spinner=\"ðŸ“– Reading PDFsâ€¦\")\n",
        "def load_all_texts():\n",
        "    texts, paths = [], []\n",
        "    for pdf in Path(EXTRACT_DIR).rglob(\"*.pdf\"):\n",
        "        t = clean_text(read_pdf(str(pdf)))\n",
        "        if len(t.split()) > 50:\n",
        "            texts.append(t)\n",
        "            paths.append(str(pdf.name))\n",
        "    return texts, paths\n",
        "\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# TOPICS â€” extracted from PDFs, cached in DB\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "_AUTHOR_ENTRY  = re.compile(r\"^[A-Z]{2,},\\s+[A-Z]\")\n",
        "_HAS_YEAR      = re.compile(r\"(19|20)\\d{2}\")\n",
        "_PUBLISHER     = re.compile(\n",
        "    r\"(press|publishing|publishers|edition|reprint|oxford|cambridge|mcgraw|\"\n",
        "    r\"pearson|routledge|sage|wiley|elsevier|springer|penguin|harper|norton|\"\n",
        "    r\"new york|new delhi|london|chicago|boston|mumbai|kolkata|chennai|\"\n",
        "    r\"pp\\.|vol\\.|ibid|op\\.cit|et al|isbn|doi|http|www\\.)\", re.I\n",
        ")\n",
        "_NOISE_START   = re.compile(\n",
        "    r\"^(the|a|an|this|that|these|it|its|in|on|at|by|for|of|to|and|or|but|\"\n",
        "    r\"such|also|thus|hence|therefore|however|moreover|furthermore|although|\"\n",
        "    r\"figure|table|box|note|source|see|pg|pp|ref|ncert|reprint|let us|\"\n",
        "    r\"activity|exercise|project|chapter|unit|section)\", re.I\n",
        ")\n",
        "_VALID_HEADING = re.compile(r\"^[A-Z][a-zA-Z\\s\\-:']{3,55}$\")\n",
        "\n",
        "\n",
        "def extract_topics_from_text(text: str) -> list:\n",
        "    lines  = text.split(\"\\n\")\n",
        "    topics = set()\n",
        "    for raw_line in lines:\n",
        "        line  = raw_line.strip()\n",
        "        words = line.split()\n",
        "        if not (2 <= len(words) <= 6):\n",
        "            continue\n",
        "        if _AUTHOR_ENTRY.match(line):\n",
        "            continue\n",
        "        if _HAS_YEAR.search(line):\n",
        "            continue\n",
        "        if _PUBLISHER.search(line):\n",
        "            continue\n",
        "        if _NOISE_START.match(line):\n",
        "            continue\n",
        "        if not _VALID_HEADING.match(line):\n",
        "            continue\n",
        "        if line[-1] in \".,;:\":\n",
        "            continue\n",
        "        last_word = words[-1].lower()\n",
        "        if last_word in (\"and\", \"or\", \"the\", \"of\", \"a\", \"an\", \"in\", \"on\"):\n",
        "            continue\n",
        "        if any(c.isdigit() for c in line):\n",
        "            continue\n",
        "        titled = sum(1 for w in words if w and w[0].isupper())\n",
        "        if titled / len(words) < 0.70:\n",
        "            continue\n",
        "        topics.add(line)\n",
        "    return list(topics)\n",
        "\n",
        "\n",
        "def topics_in_db() -> bool:\n",
        "    cur = get_db().cursor()\n",
        "    cur.execute(\"SELECT COUNT(*) as n FROM topics\")\n",
        "    return cur.fetchone()[\"n\"] > 0\n",
        "\n",
        "\n",
        "@st.cache_data(show_spinner=False)\n",
        "def get_all_topics(subject_name: str = None) -> list:\n",
        "    cur = get_db().cursor()\n",
        "\n",
        "    if topics_in_db():\n",
        "        if subject_name:\n",
        "            subject_id = subject_name.lower().replace(\" \", \"_\")\n",
        "            cur.execute(\"\"\"\n",
        "                SELECT DISTINCT t.topic_text FROM topics t\n",
        "                WHERE t.source_pdf IN (\n",
        "                    SELECT DISTINCT pdf_filename FROM chapters WHERE subject_id = ?\n",
        "                )\n",
        "                ORDER BY t.topic_text\n",
        "            \"\"\", (subject_id,))\n",
        "        else:\n",
        "            cur.execute(\"\"\"\n",
        "                SELECT DISTINCT topic_text FROM topics\n",
        "                ORDER BY topic_text LIMIT 600\n",
        "            \"\"\")\n",
        "        return [r[\"topic_text\"] for r in cur.fetchall()]\n",
        "\n",
        "    with st.spinner(\"ðŸ” Extracting topics from PDFs â€” first time only, won't repeat...\"):\n",
        "        texts, paths = load_all_texts()\n",
        "        for text, path in zip(texts, paths):\n",
        "            extracted = extract_topics_from_text(text)\n",
        "            save_topics_to_db(extracted, path)\n",
        "\n",
        "    return get_all_topics(subject_name)\n",
        "\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# EMBEDDER\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "@st.cache_resource\n",
        "def load_embedder():\n",
        "    return SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "embedder = load_embedder()\n",
        "\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# EMBEDDING HELPERS â€” DB backed\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def _text_hash(text: str) -> str:\n",
        "    \"\"\"MD5 of PDF text â€” used to detect stale cached embeddings.\"\"\"\n",
        "    return hashlib.md5(text.encode()).hexdigest()\n",
        "\n",
        "\n",
        "def _load_embedding_from_db(pdf_filename: str, text: str):\n",
        "    \"\"\"\n",
        "    Returns the cached numpy embedding if it exists AND the text hasn't changed.\n",
        "    Returns None if not cached or stale.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        cur = get_db().cursor()\n",
        "        cur.execute(\n",
        "            \"SELECT embedding, text_hash FROM pdf_embeddings WHERE pdf_filename = ?\",\n",
        "            (pdf_filename,)\n",
        "        )\n",
        "        row = cur.fetchone()\n",
        "        if row and row[\"text_hash\"] == _text_hash(text):\n",
        "            return np.frombuffer(row[\"embedding\"], dtype=np.float32)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "\n",
        "def _save_embedding_to_db(pdf_filename: str, text: str, embedding: np.ndarray) -> None:\n",
        "    \"\"\"Upsert a single embedding into the DB.\"\"\"\n",
        "    try:\n",
        "        get_db().execute(\"\"\"\n",
        "            INSERT OR REPLACE INTO pdf_embeddings (pdf_filename, embedding, text_hash)\n",
        "            VALUES (?, ?, ?)\n",
        "        \"\"\", (pdf_filename, embedding.astype(np.float32).tobytes(), _text_hash(text)))\n",
        "        get_db().commit()\n",
        "    except Exception as e:\n",
        "        st.warning(f\"âš ï¸ Could not save embedding for {pdf_filename}: {e}\")\n",
        "\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# BOOT SEQUENCE\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "_, pdf_files = download_and_extract()\n",
        "\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# CHAPTER RECOMMENDATIONS (embeddings)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def lookup_chapter(pdf_filename: str, zip_source: str = None) -> dict:\n",
        "    cur = get_db().cursor()\n",
        "    if zip_source:\n",
        "        cur.execute(\"\"\"\n",
        "            SELECT subject_id, class, chapter_number, chapter_name,\n",
        "                   book_name, is_special\n",
        "            FROM chapters\n",
        "            WHERE pdf_filename = ? AND zip_source = ?\n",
        "        \"\"\", (pdf_filename, zip_source))\n",
        "    else:\n",
        "        cur.execute(\"\"\"\n",
        "            SELECT subject_id, class, chapter_number, chapter_name,\n",
        "                   book_name, is_special\n",
        "            FROM chapters WHERE pdf_filename = ? LIMIT 1\n",
        "        \"\"\", (pdf_filename,))\n",
        "    row = cur.fetchone()\n",
        "    if row:\n",
        "        return dict(row)\n",
        "    return {\n",
        "        \"subject_id\": \"â€”\", \"class\": \"â€”\", \"chapter_number\": \"â€”\",\n",
        "        \"chapter_name\": pdf_filename, \"book_name\": \"â€”\", \"is_special\": 0\n",
        "    }\n",
        "\n",
        "\n",
        "@st.cache_resource(show_spinner=\"ðŸ“ Indexing PDF chapters (one-time)â€¦\")\n",
        "def get_pdf_embeddings():\n",
        "    \"\"\"\n",
        "    Smart embedding loader:\n",
        "      â€¢ Checks DB first for each PDF.\n",
        "      â€¢ Only encodes PDFs that are NEW or whose text has CHANGED.\n",
        "      â€¢ Saves new embeddings to DB so every subsequent run is instant.\n",
        "\n",
        "    After the first full run, this function is O(n) DB reads â€” no GPU/CPU encoding.\n",
        "    \"\"\"\n",
        "    texts, paths = load_all_texts()\n",
        "\n",
        "    embeddings       = [None] * len(texts)\n",
        "    to_encode_idx    = []\n",
        "    to_encode_texts  = []\n",
        "\n",
        "    # â”€â”€ Pass 1: load cached embeddings from DB â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    for i, (text, path) in enumerate(zip(texts, paths)):\n",
        "        cached = _load_embedding_from_db(path, text)\n",
        "        if cached is not None:\n",
        "            embeddings[i] = cached          # âœ… cache hit â€” no encoding needed\n",
        "        else:\n",
        "            to_encode_idx.append(i)         # âŒ missing or stale â€” queue for encoding\n",
        "            to_encode_texts.append(text)\n",
        "\n",
        "    # â”€â”€ Pass 2: encode only missing/stale PDFs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    if to_encode_texts:\n",
        "        status_msg = (\n",
        "            f\"ðŸ§  Encoding {len(to_encode_texts)} \"\n",
        "            f\"{'new/changed' if len(to_encode_texts) < len(texts) else ''} PDFsâ€¦\"\n",
        "        )\n",
        "        with st.spinner(status_msg):\n",
        "            new_embs = embedder.encode(\n",
        "                to_encode_texts,\n",
        "                convert_to_numpy=True,\n",
        "                show_progress_bar=False\n",
        "            )\n",
        "        # Save each new embedding to DB and fill the array\n",
        "        for idx, emb, text in zip(to_encode_idx, new_embs, to_encode_texts):\n",
        "            embeddings[idx] = emb.astype(np.float32)\n",
        "            _save_embedding_to_db(paths[idx], text, emb)\n",
        "\n",
        "    return texts, paths, np.array(embeddings, dtype=np.float32)\n",
        "\n",
        "\n",
        "def get_chapter_recommendations(query: str, top_n: int = 5) -> pd.DataFrame:\n",
        "    if not query or not query.strip():\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    texts, paths, text_emb = get_pdf_embeddings()   # instant after first run\n",
        "    topic_emb = embedder.encode([query.strip()], convert_to_numpy=True)\n",
        "    sims      = cosine_similarity(topic_emb, text_emb).flatten()\n",
        "    top_idx   = sims.argsort()[-top_n:][::-1]\n",
        "\n",
        "    rows = []\n",
        "    for i in top_idx:\n",
        "        info = lookup_chapter(paths[i])\n",
        "        if info.get(\"is_special\"):\n",
        "            continue\n",
        "        rows.append({\n",
        "            \"Subject\":      info[\"subject_id\"].replace(\"_\", \" \").title(),\n",
        "            \"Class\":        f\"Class {info['class']}\",\n",
        "            \"Book\":         info[\"book_name\"],\n",
        "            \"Ch No.\":       info[\"chapter_number\"],\n",
        "            \"Chapter Name\": info[\"chapter_name\"],\n",
        "            \"Relevance\":    round(float(sims[i]), 4),\n",
        "        })\n",
        "    return pd.DataFrame(rows) if rows else pd.DataFrame()\n",
        "\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# BOOK & PAPER RECOMMENDATIONS (from DB)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def recommend_materials(subject: str, level: str, topics: tuple) -> pd.DataFrame:\n",
        "    if not topics:\n",
        "        return pd.DataFrame({\"Message\": [\"Please select at least one topic\"]})\n",
        "    rows = get_recommendations(subject, level)\n",
        "    if not rows:\n",
        "        return pd.DataFrame({\"Message\": [f\"No data found for {subject} / {level}\"]})\n",
        "    return pd.DataFrame([{\n",
        "        \"Type\":          r[\"level_type\"],\n",
        "        \"Title\":         r[\"title\"],\n",
        "        \"Author\":        r[\"author\"],\n",
        "        \"Why Read This\": r[\"why\"],\n",
        "        \"NCERT Chapter\": r[\"ncert_chapter_link\"] or \"â€”\",\n",
        "        \"Link\":          r[\"link\"],\n",
        "    } for r in rows])\n",
        "\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# YOUTUBE SEARCH URL BUILDER\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def build_youtube_url(subject: str, level: str, topics: list) -> str:\n",
        "    import urllib.parse\n",
        "    topic_str = \" \".join(topics[:3]) if topics else \"\"\n",
        "    query     = f\"NCERT {subject} {level} {topic_str} class 11 12\".strip()\n",
        "    encoded   = urllib.parse.quote_plus(query)\n",
        "    return f\"https://www.youtube.com/results?search_query={encoded}\"\n",
        "\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# SIDEBAR\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "st.sidebar.header(\"ðŸŽ“ Select Options\")\n",
        "\n",
        "all_subjects     = get_subjects()\n",
        "selected_subject = st.sidebar.selectbox(\"Select Subject\", all_subjects)\n",
        "\n",
        "all_levels       = get_levels(selected_subject)\n",
        "selected_level   = st.sidebar.selectbox(\"Select Level\", all_levels)\n",
        "\n",
        "st.sidebar.markdown(\"---\")\n",
        "\n",
        "input_mode = st.sidebar.radio(\n",
        "    \"Search by\",\n",
        "    [\"ðŸ“ Type a query\", \"ðŸ“‹ Pick from topics\"],\n",
        "    help=\"Type your own question OR pick chapter names from the list\"\n",
        ")\n",
        "\n",
        "user_query      = \"\"\n",
        "selected_topics = []\n",
        "\n",
        "if input_mode == \"ðŸ“ Type a query\":\n",
        "    user_query = st.sidebar.text_area(\n",
        "        \"Your question or keyword\",\n",
        "        placeholder=(\n",
        "            \"e.g. what is the caste system?\\n\"\n",
        "            \"or: federalism in India\\n\"\n",
        "            \"or: GDP and national income\"\n",
        "        ),\n",
        "        height=100,\n",
        "        help=\"Type anything â€” a question, keyword, or concept\"\n",
        "    )\n",
        "    selected_topics = [w for w in user_query.split() if len(w) > 3] if user_query else []\n",
        "\n",
        "else:\n",
        "    subject_topics = get_all_topics(selected_subject)\n",
        "    if not subject_topics:\n",
        "        st.sidebar.warning(\"No topics found â€” PDFs will be scanned on first search.\")\n",
        "        selected_topics = []\n",
        "    else:\n",
        "        st.sidebar.caption(f\"{len(subject_topics)} topics from {selected_subject} PDFs\")\n",
        "        selected_topics = st.sidebar.multiselect(\n",
        "            \"Select Topic(s)\", subject_topics,\n",
        "            help=\"Topics extracted from NCERT PDF text\"\n",
        "        )\n",
        "\n",
        "user_query   = \" \".join(selected_topics)\n",
        "search_query = user_query.strip()\n",
        "\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# RENDER â€” Tabs layout\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "st.markdown(\"---\")\n",
        "tab1, tab2, tab3 = st.tabs([\n",
        "    \"ðŸ“„ Chapter Recommendations\",\n",
        "    \"ðŸ“š Books & Research Papers\",\n",
        "    \"â–¶ï¸ YouTube Videos\",\n",
        "])\n",
        "\n",
        "# â”€â”€ Tab 1: Chapter Recommendations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "with tab1:\n",
        "    st.markdown(\n",
        "        \"Finds the most relevant NCERT chapters based on your query. \"\n",
        "        \"Works with free text â€” *what is caste system*, *explain GDP*, anything.\"\n",
        "    )\n",
        "    if not search_query:\n",
        "        st.info(\"ðŸ‘ˆ Type a question or select chapters from the sidebar to search.\")\n",
        "    else:\n",
        "        st.markdown(f\"ðŸ”Ž **Searching for:** `{search_query}`\")\n",
        "        with st.spinner(\"Finding relevant chapters...\"):\n",
        "            df = get_chapter_recommendations(search_query)\n",
        "        if df.empty:\n",
        "            st.warning(\"No matching chapters found. Try different keywords.\")\n",
        "        else:\n",
        "            st.success(f\"âœ… Top {len(df)} chapters found!\")\n",
        "            st.dataframe(\n",
        "                df,\n",
        "                use_container_width=True,\n",
        "                hide_index=True,\n",
        "                column_config={\n",
        "                    \"Relevance\": st.column_config.ProgressColumn(\n",
        "                        \"Relevance\", min_value=0, max_value=1, format=\"%.4f\"\n",
        "                    )\n",
        "                }\n",
        "            )\n",
        "\n",
        "# â”€â”€ Tab 2: Books & Research Papers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "with tab2:\n",
        "    st.markdown(\n",
        "        f\"Showing **{selected_level}** level resources for **{selected_subject}** \"\n",
        "        f\"â€” NCERT textbooks â†’ reference books â†’ research papers.\"\n",
        "    )\n",
        "    if st.button(\"ðŸ“– Get Books & Papers\", type=\"primary\", key=\"book_btn\"):\n",
        "        rows = get_recommendations(selected_subject, selected_level)\n",
        "        if not rows:\n",
        "            st.warning(f\"No recommendations found for {selected_subject} / {selected_level}\")\n",
        "        else:\n",
        "            for r in rows:\n",
        "                type_icons = {\"NCERT\": \"ðŸ“—\", \"Book\": \"ðŸ“˜\", \"Paper\": \"ðŸ“„\"}\n",
        "                icon = type_icons.get(r[\"level_type\"], \"ðŸ“Œ\")\n",
        "                with st.expander(f\"{icon} {r['title']} â€” *{r['author']}*\", expanded=False):\n",
        "                    st.markdown(f\"**Type:** {r['level_type']}\")\n",
        "                    if r[\"journal\"]:\n",
        "                        st.markdown(f\"**Journal:** {r['journal']}\")\n",
        "                    if r[\"ncert_chapter_link\"]:\n",
        "                        st.markdown(f\"**NCERT Chapter:** `{r['ncert_chapter_link']}`\")\n",
        "                    st.markdown(f\"ðŸ’¡ *{r['why']}*\")\n",
        "                    if r[\"link\"]:\n",
        "                        st.markdown(f\"[ðŸ”— Open Resource]({r['link']})\")\n",
        "\n",
        "# â”€â”€ Tab 3: YouTube Videos â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "with tab3:\n",
        "    st.markdown(\n",
        "        \"Clicking the button below will open a **YouTube search** in a new tab \"\n",
        "        \"with a query built from your selected subject, level, and topics.\"\n",
        "    )\n",
        "    yt_query_preview = f\"NCERT {selected_subject} {selected_level} {search_query} class 11 12\".strip()\n",
        "    st.markdown(f\"**Search query:** `{yt_query_preview}`\")\n",
        "\n",
        "    if not search_query:\n",
        "        st.info(\"ðŸ‘ˆ Type a query or select topics to refine the YouTube search.\")\n",
        "\n",
        "    yt_url = build_youtube_url(selected_subject, selected_level, [search_query])\n",
        "    st.link_button(\"â–¶ï¸ Search on YouTube\", yt_url, type=\"primary\")\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    st.markdown(\"**Or jump directly to a level:**\")\n",
        "\n",
        "    level_icons = {\"Beginner\": \"ðŸŒ±\", \"Intermediate\": \"ðŸ“š\", \"Advanced\": \"ðŸ”¬\"}\n",
        "    yt_levels   = get_levels(selected_subject)\n",
        "    cols        = st.columns(len(yt_levels))\n",
        "    for col, lvl in zip(cols, yt_levels):\n",
        "        with col:\n",
        "            icon = level_icons.get(lvl, \"â–¶ï¸\")\n",
        "            url  = build_youtube_url(selected_subject, lvl, [search_query])\n",
        "            st.link_button(f\"{icon} {lvl}\", url, use_container_width=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "DofUDdoaewxY",
        "outputId": "90e1a294-ef1a-4eb0-d930-a8063543436c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "removed the reading pdfs, everything happens by fetching frilies from DB...all embeddings stored in DB"
      ],
      "metadata": {
        "id": "_mj3py8Evi-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import os\n",
        "import re\n",
        "import hashlib\n",
        "import urllib.parse\n",
        "\n",
        "import numpy as np\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# CONFIG\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "DB_PATH = \"/content/drive/MyDrive/ZenithIndia/AI Learning Assistant/ncert.db\"\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# PAGE CONFIG\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "st.set_page_config(page_title=\"NCERT Learning Assistant\", layout=\"wide\")\n",
        "st.title(\"ðŸ“˜ NCERT Learning Assistant\")\n",
        "st.write(\"Select topics to get chapter, book, and research paper recommendations for Class 11â€“12 students\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# MOUNT GOOGLE DRIVE\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    if not os.path.exists(\"/content/drive/MyDrive\"):\n",
        "        drive.mount(\"/content/drive\")\n",
        "except ImportError:\n",
        "    DB_PATH = \"ncert.db\"  # fallback for local runs\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# DATABASE\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "@st.cache_resource\n",
        "def get_db():\n",
        "    if not os.path.exists(DB_PATH):\n",
        "        st.error(\n",
        "            f\"âŒ Database not found at:\\n`{DB_PATH}`\\n\\n\"\n",
        "            \"Please run setup_db.ipynb first, or update DB_PATH at the top of app.py.\"\n",
        "        )\n",
        "        st.stop()\n",
        "    try:\n",
        "        conn = sqlite3.connect(DB_PATH, check_same_thread=False)\n",
        "        conn.row_factory = sqlite3.Row\n",
        "        conn.execute(\"SELECT 1\")  # sanity check\n",
        "        return conn\n",
        "    except sqlite3.DatabaseError as e:\n",
        "        st.error(\n",
        "            f\"âŒ File exists but is not a valid SQLite database:\\n`{DB_PATH}`\\n\\n\"\n",
        "            f\"Error: {e}\\n\\nPlease re-run setup_db.ipynb to recreate it.\"\n",
        "        )\n",
        "        st.stop()\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# DB HELPERS\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def get_subjects() -> list:\n",
        "    cur = get_db().cursor()\n",
        "    cur.execute(\"SELECT subject_name FROM subjects ORDER BY subject_name\")\n",
        "    return [row[\"subject_name\"] for row in cur.fetchall()]\n",
        "\n",
        "\n",
        "def get_levels(subject_name: str = None) -> list:\n",
        "    cur = get_db().cursor()\n",
        "    level_order = (\n",
        "        \"CASE level WHEN 'Beginner' THEN 1 \"\n",
        "        \"WHEN 'Intermediate' THEN 2 WHEN 'Advanced' THEN 3 ELSE 4 END\"\n",
        "    )\n",
        "    if subject_name:\n",
        "        subject_id = subject_name.lower().replace(\" \", \"_\")\n",
        "        cur.execute(\n",
        "            f\"SELECT DISTINCT level FROM recommendations \"\n",
        "            f\"WHERE subject_id = ? ORDER BY {level_order}\",\n",
        "            (subject_id,)\n",
        "        )\n",
        "    else:\n",
        "        cur.execute(f\"SELECT DISTINCT level FROM recommendations ORDER BY {level_order}\")\n",
        "    return [row[\"level\"] for row in cur.fetchall()]\n",
        "\n",
        "\n",
        "def get_recommendations(subject_name: str, level: str) -> list:\n",
        "    subject_id = subject_name.lower().replace(\" \", \"_\")\n",
        "    cur = get_db().cursor()\n",
        "    cur.execute(\"\"\"\n",
        "        SELECT title, author, level_type, why, link, journal, ncert_chapter_link\n",
        "        FROM recommendations\n",
        "        WHERE subject_id = ? AND level = ?\n",
        "        ORDER BY rec_id\n",
        "    \"\"\", (subject_id, level))\n",
        "    return [dict(row) for row in cur.fetchall()]\n",
        "\n",
        "\n",
        "def get_all_topics(subject_name: str = None) -> list:\n",
        "    cur = get_db().cursor()\n",
        "    if subject_name:\n",
        "        subject_id = subject_name.lower().replace(\" \", \"_\")\n",
        "        cur.execute(\"\"\"\n",
        "            SELECT DISTINCT t.topic_text FROM topics t\n",
        "            WHERE t.source_pdf IN (\n",
        "                SELECT DISTINCT pdf_filename FROM chapters WHERE subject_id = ?\n",
        "            )\n",
        "            ORDER BY t.topic_text\n",
        "        \"\"\", (subject_id,))\n",
        "    else:\n",
        "        cur.execute(\"SELECT DISTINCT topic_text FROM topics ORDER BY topic_text LIMIT 600\")\n",
        "    return [r[\"topic_text\"] for r in cur.fetchall()]\n",
        "\n",
        "\n",
        "def lookup_chapter(pdf_filename: str) -> dict:\n",
        "    cur = get_db().cursor()\n",
        "    cur.execute(\"\"\"\n",
        "        SELECT subject_id, class, chapter_number, chapter_name, book_name, is_special\n",
        "        FROM chapters WHERE pdf_filename = ? LIMIT 1\n",
        "    \"\"\", (pdf_filename,))\n",
        "    row = cur.fetchone()\n",
        "    if row:\n",
        "        return dict(row)\n",
        "    return {\n",
        "        \"subject_id\": \"â€”\", \"class\": \"â€”\", \"chapter_number\": \"â€”\",\n",
        "        \"chapter_name\": pdf_filename, \"book_name\": \"â€”\", \"is_special\": 0\n",
        "    }\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# EMBEDDER\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "@st.cache_resource(show_spinner=\"ðŸ¤– Loading embedding modelâ€¦\")\n",
        "def load_embedder():\n",
        "    return SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "embedder = load_embedder()\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# LOAD ALL EMBEDDINGS FROM DB (once per session)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "@st.cache_resource(show_spinner=\"âš¡ Loading embeddings from databaseâ€¦\")\n",
        "def load_embeddings_from_db():\n",
        "    \"\"\"\n",
        "    Reads ALL embeddings + filenames straight from DB into numpy arrays.\n",
        "    No PDF reading. No encoding. Pure DB â†’ RAM.\n",
        "    Runs once per session in ~1-2 seconds.\n",
        "    \"\"\"\n",
        "    cur = get_db().cursor()\n",
        "    cur.execute(\"SELECT pdf_filename, embedding FROM pdf_embeddings\")\n",
        "    rows = cur.fetchall()\n",
        "\n",
        "    paths      = []\n",
        "    embeddings = []\n",
        "    for row in rows:\n",
        "        emb = np.frombuffer(row[\"embedding\"], dtype=np.float32)\n",
        "        paths.append(row[\"pdf_filename\"])\n",
        "        embeddings.append(emb)\n",
        "\n",
        "    return paths, np.array(embeddings, dtype=np.float32)\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# CHAPTER RECOMMENDATIONS\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def get_chapter_recommendations(query: str, top_n: int = 5) -> pd.DataFrame:\n",
        "    if not query or not query.strip():\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    paths, text_emb = load_embeddings_from_db()   # instant â€” already in RAM\n",
        "    query_emb = embedder.encode([query.strip()], convert_to_numpy=True)\n",
        "    sims      = cosine_similarity(query_emb, text_emb).flatten()\n",
        "    top_idx   = sims.argsort()[-top_n:][::-1]\n",
        "\n",
        "    rows = []\n",
        "    for i in top_idx:\n",
        "        info = lookup_chapter(paths[i])\n",
        "        if info.get(\"is_special\"):\n",
        "            continue\n",
        "        rows.append({\n",
        "            \"Subject\":      info[\"subject_id\"].replace(\"_\", \" \").title(),\n",
        "            \"Class\":        f\"Class {info['class']}\",\n",
        "            \"Book\":         info[\"book_name\"],\n",
        "            \"Ch No.\":       info[\"chapter_number\"],\n",
        "            \"Chapter Name\": info[\"chapter_name\"],\n",
        "            \"Relevance\":    round(float(sims[i]), 4),\n",
        "        })\n",
        "    return pd.DataFrame(rows) if rows else pd.DataFrame()\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# YOUTUBE URL BUILDER\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def build_youtube_url(subject: str, level: str, topics: list) -> str:\n",
        "    topic_str = \" \".join(topics[:3]) if topics else \"\"\n",
        "    query     = f\"NCERT {subject} {level} {topic_str} class 11 12\".strip()\n",
        "    return f\"https://www.youtube.com/results?search_query={urllib.parse.quote_plus(query)}\"\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# SIDEBAR\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "st.sidebar.header(\"ðŸŽ“ Select Options\")\n",
        "\n",
        "all_subjects     = get_subjects()\n",
        "selected_subject = st.sidebar.selectbox(\"Select Subject\", all_subjects)\n",
        "\n",
        "all_levels       = get_levels(selected_subject)\n",
        "selected_level   = st.sidebar.selectbox(\"Select Level\", all_levels)\n",
        "\n",
        "st.sidebar.markdown(\"---\")\n",
        "\n",
        "input_mode = st.sidebar.radio(\n",
        "    \"Search by\",\n",
        "    [\"ðŸ“ Type a query\", \"ðŸ“‹ Pick from topics\"],\n",
        "    help=\"Type your own question OR pick chapter names from the list\"\n",
        ")\n",
        "\n",
        "user_query      = \"\"\n",
        "selected_topics = []\n",
        "\n",
        "if input_mode == \"ðŸ“ Type a query\":\n",
        "    user_query = st.sidebar.text_area(\n",
        "        \"Your question or keyword\",\n",
        "        placeholder=(\n",
        "            \"e.g. what is the caste system?\\n\"\n",
        "            \"or: federalism in India\\n\"\n",
        "            \"or: GDP and national income\"\n",
        "        ),\n",
        "        height=100,\n",
        "        help=\"Type anything â€” a question, keyword, or concept\"\n",
        "    )\n",
        "    selected_topics = [w for w in user_query.split() if len(w) > 3] if user_query else []\n",
        "\n",
        "else:\n",
        "    subject_topics = get_all_topics(selected_subject)\n",
        "    if not subject_topics:\n",
        "        st.sidebar.warning(\"No topics found in database.\")\n",
        "        selected_topics = []\n",
        "    else:\n",
        "        st.sidebar.caption(f\"{len(subject_topics)} topics available\")\n",
        "        selected_topics = st.sidebar.multiselect(\n",
        "            \"Select Topic(s)\", subject_topics,\n",
        "            help=\"Topics from NCERT chapters\"\n",
        "        )\n",
        "    user_query = \" \".join(selected_topics)\n",
        "\n",
        "search_query = user_query.strip()\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# TABS\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "st.markdown(\"---\")\n",
        "tab1, tab2, tab3 = st.tabs([\n",
        "    \"ðŸ“„ Chapter Recommendations\",\n",
        "    \"ðŸ“š Books & Research Papers\",\n",
        "    \"â–¶ï¸ YouTube Videos\",\n",
        "])\n",
        "\n",
        "# â”€â”€ Tab 1: Chapter Recommendations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "with tab1:\n",
        "    st.markdown(\n",
        "        \"Finds the most relevant NCERT chapters based on your query. \"\n",
        "        \"Works with free text â€” *what is caste system*, *explain GDP*, anything.\"\n",
        "    )\n",
        "    if not search_query:\n",
        "        st.info(\"ðŸ‘ˆ Type a question or select topics from the sidebar to search.\")\n",
        "    else:\n",
        "        st.markdown(f\"ðŸ”Ž **Searching for:** `{search_query}`\")\n",
        "        with st.spinner(\"Finding relevant chapters...\"):\n",
        "            df = get_chapter_recommendations(search_query)\n",
        "        if df.empty:\n",
        "            st.warning(\"No matching chapters found. Try different keywords.\")\n",
        "        else:\n",
        "            st.success(f\"âœ… Top {len(df)} chapters found!\")\n",
        "            st.dataframe(\n",
        "                df,\n",
        "                use_container_width=True,\n",
        "                hide_index=True,\n",
        "                column_config={\n",
        "                    \"Relevance\": st.column_config.ProgressColumn(\n",
        "                        \"Relevance\", min_value=0, max_value=1, format=\"%.4f\"\n",
        "                    )\n",
        "                }\n",
        "            )\n",
        "\n",
        "# â”€â”€ Tab 2: Books & Research Papers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "with tab2:\n",
        "    st.markdown(\n",
        "        f\"Showing **{selected_level}** level resources for **{selected_subject}** \"\n",
        "        f\"â€” NCERT textbooks â†’ reference books â†’ research papers.\"\n",
        "    )\n",
        "    if st.button(\"ðŸ“– Get Books & Papers\", type=\"primary\", key=\"book_btn\"):\n",
        "        rows = get_recommendations(selected_subject, selected_level)\n",
        "        if not rows:\n",
        "            st.warning(f\"No recommendations found for {selected_subject} / {selected_level}\")\n",
        "        else:\n",
        "            for r in rows:\n",
        "                type_icons = {\"NCERT\": \"ðŸ“—\", \"Book\": \"ðŸ“˜\", \"Paper\": \"ðŸ“„\"}\n",
        "                icon = type_icons.get(r[\"level_type\"], \"ðŸ“Œ\")\n",
        "                with st.expander(f\"{icon} {r['title']} â€” *{r['author']}*\", expanded=False):\n",
        "                    st.markdown(f\"**Type:** {r['level_type']}\")\n",
        "                    if r[\"journal\"]:\n",
        "                        st.markdown(f\"**Journal:** {r['journal']}\")\n",
        "                    if r[\"ncert_chapter_link\"]:\n",
        "                        st.markdown(f\"**NCERT Chapter:** `{r['ncert_chapter_link']}`\")\n",
        "                    st.markdown(f\"ðŸ’¡ *{r['why']}*\")\n",
        "                    if r[\"link\"]:\n",
        "                        st.markdown(f\"[ðŸ”— Open Resource]({r['link']})\")\n",
        "\n",
        "# â”€â”€ Tab 3: YouTube Videos â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "with tab3:\n",
        "    st.markdown(\n",
        "        \"Clicking the button below will open a **YouTube search** in a new tab \"\n",
        "        \"with a query built from your selected subject, level, and topics.\"\n",
        "    )\n",
        "    yt_query_preview = f\"NCERT {selected_subject} {selected_level} {search_query} class 11 12\".strip()\n",
        "    st.markdown(f\"**Search query:** `{yt_query_preview}`\")\n",
        "\n",
        "    if not search_query:\n",
        "        st.info(\"ðŸ‘ˆ Type a query or select topics to refine the YouTube search.\")\n",
        "\n",
        "    yt_url = build_youtube_url(selected_subject, selected_level, [search_query])\n",
        "    st.link_button(\"â–¶ï¸ Search on YouTube\", yt_url, type=\"primary\")\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    st.markdown(\"**Or jump directly to a level:**\")\n",
        "\n",
        "    level_icons = {\"Beginner\": \"ðŸŒ±\", \"Intermediate\": \"ðŸ“š\", \"Advanced\": \"ðŸ”¬\"}\n",
        "    yt_levels   = get_levels(selected_subject)\n",
        "    cols        = st.columns(len(yt_levels))\n",
        "    for col, lvl in zip(cols, yt_levels):\n",
        "        with col:\n",
        "            icon = level_icons.get(lvl, \"â–¶ï¸\")\n",
        "            url  = build_youtube_url(selected_subject, lvl, [search_query])\n",
        "            st.link_button(f\"{icon} {lvl}\", url, use_container_width=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsLjlfPjWtX0",
        "outputId": "8429541e-860e-4c39-c615-111ac1f8d186"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5Ox-jZ_HDKG",
        "outputId": "951f335c-cb79-435d-c9b2-79f46f3bb38f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.5.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n",
            "Downloading pyngrok-7.5.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import time\n",
        "from pyngrok import ngrok\n",
        "import torch\n",
        "\n",
        "# Verify GPU\n",
        "print(\"GPU available:\", torch.cuda.is_available())\n",
        "print(\"Device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
        "\n",
        "# Kill existing streamlit\n",
        "subprocess.run([\"pkill\", \"-f\", \"streamlit\"], capture_output=True)\n",
        "time.sleep(2)\n",
        "\n",
        "# Set ngrok token and kill old tunnels\n",
        "ngrok.set_auth_token(\"3A9Zz7jNmMsUvZ0XJkpn32mZ6mK_KJDJPs7MgmMTCd1kuiSo\")\n",
        "ngrok.kill()\n",
        "\n",
        "# Start Streamlit\n",
        "process = subprocess.Popen(\n",
        "    [\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\", \"--server.headless\", \"true\"]\n",
        ")\n",
        "\n",
        "print(\"Waiting for Streamlit to start...\")\n",
        "time.sleep(20)\n",
        "\n",
        "# Start tunnel\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"Open this URL:\", public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V65hcn1lGjXx",
        "outputId": "6667b19b-b8b5-4f07-8710-ac092fafb60b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU available: True\n",
            "Device: Tesla T4\n",
            "Waiting for Streamlit to start...\n",
            "Open this URL: NgrokTunnel: \"https://nemoricole-mercenarily-wendi.ngrok-free.dev\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3DaZWUOtx-h",
        "outputId": "9e0e5992-53d6-41c8-b82f-ffd8bbab54ab"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import numpy as np\n",
        "\n",
        "DB_PATH = \"/content/drive/MyDrive/ZenithIndia/AI Learning Assistant/ncert.db\"\n",
        "conn = sqlite3.connect(DB_PATH)\n",
        "conn.row_factory = sqlite3.Row\n",
        "\n",
        "# â”€â”€ 1. Count â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "cur = conn.cursor()\n",
        "cur.execute(\"SELECT COUNT(*) as total FROM pdf_embeddings\")\n",
        "total = cur.fetchone()[\"total\"]\n",
        "print(f\"âœ… Total embeddings stored: {total}\")\n",
        "\n",
        "# â”€â”€ 2. Sample 5 rows â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "cur.execute(\"\"\"\n",
        "    SELECT pdf_filename, text_hash, created_at,\n",
        "           LENGTH(embedding) as blob_size\n",
        "    FROM pdf_embeddings\n",
        "    ORDER BY created_at DESC\n",
        "    LIMIT 5\n",
        "\"\"\")\n",
        "rows = cur.fetchall()\n",
        "print(\"\\nðŸ“‹ Latest 5 embeddings:\")\n",
        "print(f\"{'Filename':<40} {'Hash':<10} {'Blob (bytes)':<14} {'Created At'}\")\n",
        "print(\"-\" * 90)\n",
        "for r in rows:\n",
        "    print(f\"{r['pdf_filename']:<40} {r['text_hash'][:8]:<10} {r['blob_size']:<14} {r['created_at']}\")\n",
        "\n",
        "# â”€â”€ 3. Decode one and check shape â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "cur.execute(\"SELECT pdf_filename, embedding FROM pdf_embeddings LIMIT 1\")\n",
        "row = cur.fetchone()\n",
        "if row:\n",
        "    emb = np.frombuffer(row[\"embedding\"], dtype=np.float32)\n",
        "    print(f\"\\nðŸ”¢ Sample embedding: '{row['pdf_filename']}'\")\n",
        "    print(f\"   Shape  : {emb.shape}\")\n",
        "    print(f\"   Dtype  : {emb.dtype}\")\n",
        "    print(f\"   Min    : {emb.min():.4f}\")\n",
        "    print(f\"   Max    : {emb.max():.4f}\")\n",
        "    print(f\"   Mean   : {emb.mean():.4f}\")\n",
        "    print(f\"   First 5 values: {emb[:5]}\")\n",
        "\n",
        "# â”€â”€ 4. Check for any nulls / corrupt rows â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "cur.execute(\"\"\"\n",
        "    SELECT COUNT(*) as bad\n",
        "    FROM pdf_embeddings\n",
        "    WHERE embedding IS NULL OR LENGTH(embedding) < 100\n",
        "\"\"\")\n",
        "bad = cur.fetchone()[\"bad\"]\n",
        "print(f\"\\n{'âœ… No corrupt rows found.' if bad == 0 else f'âš ï¸  {bad} corrupt/empty rows detected!'}\")\n",
        "\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "riZfxkKpjFwo",
        "outputId": "444736a8-0742-4a88-ea7b-0161a5872e17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Total embeddings stored: 84\n",
            "\n",
            "ðŸ“‹ Latest 5 embeddings:\n",
            "Filename                                 Hash       Blob (bytes)   Created At\n",
            "------------------------------------------------------------------------------------------\n",
            "lepy103.pdf                              a03e9584   1536           2026-02-27 05:18:08\n",
            "lepy107.pdf                              8c094e71   1536           2026-02-27 05:18:08\n",
            "lepy1gl.pdf                              798afa09   1536           2026-02-27 05:18:08\n",
            "lepy102.pdf                              58c1633e   1536           2026-02-27 05:18:08\n",
            "lepy1ps.pdf                              2eb9ef05   1536           2026-02-27 05:18:08\n",
            "\n",
            "ðŸ”¢ Sample embedding: 'keec103.pdf'\n",
            "   Shape  : (384,)\n",
            "   Dtype  : float32\n",
            "   Min    : -0.1250\n",
            "   Max    : 0.1578\n",
            "   Mean   : -0.0025\n",
            "   First 5 values: [-0.08271716 -0.04027534 -0.03839887 -0.04763067  0.03537634]\n",
            "\n",
            "âœ… No corrupt rows found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "\n",
        "DB_PATH = \"/content/drive/MyDrive/ZenithIndia/AI Learning Assistant/ncert.db\"\n",
        "conn = sqlite3.connect(DB_PATH)\n",
        "conn.row_factory = sqlite3.Row\n",
        "cur = conn.cursor()\n",
        "\n",
        "# â”€â”€ Check all 3 tables â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "cur.execute(\"SELECT COUNT(*) as n FROM pdf_embeddings\")\n",
        "emb_count = cur.fetchone()[\"n\"]\n",
        "\n",
        "cur.execute(\"SELECT COUNT(*) as n FROM topics\")\n",
        "topic_count = cur.fetchone()[\"n\"]\n",
        "\n",
        "cur.execute(\"SELECT COUNT(*) as n FROM chapters WHERE is_special = 0\")\n",
        "chapter_count = cur.fetchone()[\"n\"]\n",
        "\n",
        "print(f\"ðŸ“ Embeddings : {emb_count}\")\n",
        "print(f\"ðŸ“ Topics     : {topic_count}\")\n",
        "print(f\"ðŸ“š Chapters   : {chapter_count}\")\n",
        "\n",
        "if emb_count > 0 and topic_count > 0:\n",
        "    print(\"\\nâœ… DB is fully indexed â€” PDFs never needed again!\")\n",
        "else:\n",
        "    missing = []\n",
        "    if emb_count == 0: missing.append(\"embeddings\")\n",
        "    if topic_count == 0: missing.append(\"topics\")\n",
        "    print(f\"\\nâš ï¸ Still missing: {', '.join(missing)} â€” need one more full run\")\n",
        "\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "WqNXd66Gnayh",
        "outputId": "2c752c91-e038-4337-d88f-cefb6025074d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“ Embeddings : 84\n",
            "ðŸ“ Topics     : 273\n",
            "ðŸ“š Chapters   : 79\n",
            "\n",
            "âœ… DB is fully indexed â€” PDFs never needed again!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7osMaH1MG373"
      }
    }
  ]
}